{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqjSsKeM3fhL"
      },
      "source": [
        "# CS565-DS522 IoT Data Science Mini Project for K-EmoPhone dataset\n",
        "*This material is a joint work of TAs from IC Lab at KAIST, including Panyu Zhang, Soowon Kang, and Woohyeok Choi. This work is licensed under CC BY-SA 4.0.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQ3qZRo52Wi"
      },
      "source": [
        "## Instruction\n",
        "In this mini-project, we will build a model to predict users' self-reported stress using extracted features from K-EmoPhone dataset. This material mainly refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) conducting indepedent reproducibility experiments on K-EmoPhone dataset. In order to save time, we provide the extracted features from the raw data instead of starting from scratch. Besides, traditional machine learning model is used considering limited number of labels and multimodality issue in the in-the-wild K-EmoPhone dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbgf8BGx5MsB"
      },
      "source": [
        "## Guidance\n",
        "\n",
        "1. Before running the code, please first download the extracted features from the following [link](https://drive.google.com/file/d/1HcyFvzWEzO21osyP5E8VpVmHROX1ew7q/view?usp=sharing).\n",
        "\n",
        "2. Please change your runtime type to T4-GPU or other runtime types with GPU available since later we may use GPU for\n",
        "xgboost execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4izzUnJ6hp"
      },
      "source": [
        "Install latest version of xgboost > 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDlc8HW-Kt4",
        "outputId": "c6d197a5-7b27-4c57-f371-dc2f6c053009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "XnKkV_aTVZ_Z"
      },
      "outputs": [],
      "source": [
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import cloudpickle\n",
        "from datetime import datetime\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "import time\n",
        "from typing import Optional\n",
        "from contextlib import contextmanager\n",
        "\n",
        "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
        "\n",
        "RANDOM_STATE =42\n",
        "\n",
        "\n",
        "def log(msg: any):\n",
        "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aF5NVMMUzBb"
      },
      "source": [
        "## 1.Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5n57OnHIOM"
      },
      "source": [
        "### 1.1. Mount to Your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9KU0Kq4WZs",
        "outputId": "a9ea32a9-c6b4-4be2-c90f-df5028d7fe7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import drive\\n\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# not relevant for local execution\n",
        "'''\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9u_nJzcIKXF"
      },
      "source": [
        "### 1.2. Load Extracted Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg5JSHftIP_f",
        "outputId": "a770ce09-541d-4a64-f0d8-f368a7ce0ba1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "#PATH = '/content/drive/MyDrive/IoT_Data_Science/Project/Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "PATH = './Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "\n",
        "X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffYa1CuFIx7Q"
      },
      "source": [
        "X is the extracted features and the feature extraction process refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) and the immediate past time window is set as 15 minutes. y is the array of labels while groups is the user ids.\n",
        "\n",
        "Please note that here y is binarized using theoretical threshold (if ESM stress > 0, binarize as 1, else 0, ESM label scale [-3, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0jkopjUl2h"
      },
      "source": [
        "Since features are already extracted, we do not need to work on preprocessing and feature extraction again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLIYf29UYES"
      },
      "source": [
        "## 2.Feature Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOgjC9HXo5cP"
      },
      "source": [
        "There exist multiple types of features. Please try different combinations of features to see if there is any model performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "PE57GSJPVOWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#The following code is designed for reordering the data\n",
        "#################################################\n",
        "# Create a DataFrame with user_id and datetime\n",
        "\n",
        "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "\n",
        "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "\n",
        "# Sort the DataFrame by datetime\n",
        "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Update groups and datetimes\n",
        "groups = df_merged['user_id'].to_numpy()\n",
        "datetimes = df_merged['datetime'].to_numpy()\n",
        "y = df_merged['label'].to_numpy()\n",
        "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "\n",
        "\n",
        "#Divide the features into different categories\n",
        "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]\n",
        "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]\n",
        "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]\n",
        "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]\n",
        "\n",
        "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
        "\n",
        "#################################################################################\n",
        "#Below are the available features\n",
        "#Divide the time window features into sensor/ESM self-report features\n",
        "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]] #Current sensor features (value right before label)\n",
        "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]] #Current ESM features (value right before label)\n",
        "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]] #Immediate past sensor features (in past 15 minutes before label)\n",
        "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  #Immediate past ESM features\n",
        "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]] #Today epoch sensor features\n",
        "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]] #Today epoch ESM features\n",
        "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]] #Yesterday sensor features\n",
        "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] #Yesterday ESM features\n",
        "\n",
        "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]\n",
        "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]\n",
        "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]\n",
        "################################################################################\n",
        "\n",
        "#Prepare the final feature set\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_baseline  ],axis=1)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "VIqbJ_A8JGUS",
        "outputId": "59928fd1-9982-4580-c4a1-25f4afbb070c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ESM#LastLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2619 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ESM#LastLabel\n",
              "0               0.0\n",
              "1               1.0\n",
              "2               1.0\n",
              "3               0.0\n",
              "4               0.0\n",
              "...             ...\n",
              "2614            0.0\n",
              "2615            0.0\n",
              "2616            0.0\n",
              "2617            1.0\n",
              "2618            0.0\n",
              "\n",
              "[2619 rows x 1 columns]"
            ]
          },
          "execution_count": 37,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat_current_ESM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPIZll5fXQld"
      },
      "source": [
        "## 3.Model Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sE6PaldpCNU"
      },
      "source": [
        "Here is the revised XGBoost Classifier. We will use random eval_size percent of training set data as evaluation set for early stoppping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "cxqMVtSVXTfH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier, DMatrix\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from typing import Union\n",
        "\n",
        "#Function for revised xgboost classifier\n",
        "class EvXGBClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Enhanced XGBClassifier with built-in validation set approach for early stopping.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_size=None,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        random_state=None,\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the custom XGBoost Classifier.\n",
        "\n",
        "        Args:\n",
        "            eval_size (float): The proportion of the dataset to include in the evaluation split.\n",
        "            eval_metric (str): The evaluation metric used for model training.\n",
        "            early_stopping_rounds (int): The number of rounds to stop training if hold-out metric doesn't improve.\n",
        "            random_state (int): Seed for the random number generator for reproducibility.\n",
        "            **kwargs: Additional arguments to be passed to the underlying XGBClassifier.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.eval_size = eval_size\n",
        "        self.eval_metric = eval_metric\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        # Initialize the XGBClassifier with specified arguments and GPU acceleration.\n",
        "        self.model = XGBClassifier(\n",
        "            random_state=self.random_state,\n",
        "            eval_metric=self.eval_metric,\n",
        "            early_stopping_rounds=self.early_stopping_rounds,\n",
        "            tree_method = \"hist\", device = \"cuda\", #Use gpu for acceleration\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\" Returns the feature importances from the fitted model. \"\"\"\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "    @property\n",
        "    def feature_names_in_(self):\n",
        "        \"\"\" Returns the feature names from the input dataset used for fitting. \"\"\"\n",
        "        return self.model.feature_names_in_\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the XGBoost model with optional early stopping using a validation set.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Training features.\n",
        "            y (np.ndarray): Target values.\n",
        "        \"\"\"\n",
        "        if self.eval_size:\n",
        "            # Split data for early stopping evaluation if eval_size is specified.\n",
        "            X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "                X, y, test_size=self.eval_size, random_state=self.random_state)\n",
        "            # Fit the model with early stopping.\n",
        "            self.model.fit(\n",
        "                X_train_sub, y_train_sub,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "        else:\n",
        "            # Fit the model without early stopping.\n",
        "            self.model.fit(X, y, verbose=False)\n",
        "\n",
        "        # Store the best iteration number for predictions.\n",
        "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the classes for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_yate5pYRg"
      },
      "source": [
        "The following is defined functions for model training and model evaluation (cross-validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AoHTuB3qpsfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    print(message)  # Simple logging to stdout or enhance as needed\n",
        "\n",
        "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate the model for a single fold.\n",
        "    Args:\n",
        "        dir_result (str): Directory to store results.\n",
        "        fold_name (str): Name of the fold for identification.\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        C_cat, C_num (array): Lists of categorical and numeric feature names.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if normalize:\n",
        "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "            # Standard scaler only applied to numeric data\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        #Applying the LASSO feature selection method\n",
        "        if select:\n",
        "\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for i, s in enumerate(select):\n",
        "                C = np.asarray(X_train.columns)\n",
        "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
        "                C_sel = C[M]\n",
        "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
        "                C_num = C_num[np.isin(C_num, C_sel)]\n",
        "\n",
        "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "\n",
        "\n",
        "                X_train = pd.DataFrame(\n",
        "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "\n",
        "        if oversample:\n",
        "            # TODO: CHANGE TO ADASYN\n",
        "            #If there is any categorical data, apply SMOTE-NC, otherwise just SMOTE\n",
        "            if len(C_cat) > 0:\n",
        "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        #Deafult average method for roc_auc_score is macro\n",
        "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle =True, random_state = 42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
        "\n",
        "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "\n",
        "    return futures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOD1KJTup7il"
      },
      "source": [
        "Here, we define the feature selection method and classifier and execute the code. AUC-ROC is calculated as mean of macro AUC-ROC for all folds/users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqvkNsznrEDe",
        "outputId": "6d0b3b54-c64d-4d54-838c-e3dd6b6bdcb9"
      },
      "outputs": [
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[10], line 25\u001b[0m\n\u001b[1;32m     14\u001b[0m estimator \u001b[38;5;241m=\u001b[39m EvXGBClassifier(\n\u001b[1;32m     15\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[1;32m     16\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     21\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.01\u001b[39m,\n\u001b[1;32m     22\u001b[0m )\n\u001b[1;32m     24\u001b[0m \u001b[38;5;66;03m#Perform cross validation including model training and evaluation\u001b[39;00m\n\u001b[0;32m---> 25\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mperform_cross_validation\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgroups\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mSELECT_LASSO\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moversample\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     26\u001b[0m auc_values \u001b[38;5;241m=\u001b[39m [results[i]\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))]\n\u001b[1;32m     27\u001b[0m mean_auc \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(auc_values)\n",
            "Cell \u001b[0;32mIn[9], line 134\u001b[0m, in \u001b[0;36mperform_cross_validation\u001b[0;34m(X, y, groups, estimator, normalize, select, oversample, random_state)\u001b[0m\n\u001b[1;32m    131\u001b[0m     C_cat \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(cats))\n\u001b[1;32m    132\u001b[0m     C_num \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(\u001b[38;5;28msorted\u001b[39m(X\u001b[38;5;241m.\u001b[39mcolumns[\u001b[38;5;241m~\u001b[39mX\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39misin(C_cat)]))\n\u001b[0;32m--> 134\u001b[0m     job \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_fold\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpath_to_results\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mFold_\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43midx\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_cat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mC_num\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnormalize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mselect\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moversample\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    135\u001b[0m     futures\u001b[38;5;241m.\u001b[39mappend(job)\n\u001b[1;32m    137\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m futures\n",
            "Cell \u001b[0;32mIn[9], line 68\u001b[0m, in \u001b[0;36mtrain_fold\u001b[0;34m(dir_result, fold_name, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\u001b[0m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, s \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(select):\n\u001b[1;32m     67\u001b[0m     C \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39masarray(X_train\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[0;32m---> 68\u001b[0m     M \u001b[38;5;241m=\u001b[39m \u001b[43ms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_train\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget_support()\n\u001b[1;32m     69\u001b[0m     C_sel \u001b[38;5;241m=\u001b[39m C[M]\n\u001b[1;32m     70\u001b[0m     C_cat \u001b[38;5;241m=\u001b[39m C_cat[np\u001b[38;5;241m.\u001b[39misin(C_cat, C_sel)]\n",
            "File \u001b[0;32m~/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/sklearn/feature_selection/_from_model.py:388\u001b[0m, in \u001b[0;36mSelectFromModel.fit\u001b[0;34m(self, X, y, **fit_params)\u001b[0m\n\u001b[1;32m    385\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    386\u001b[0m         \u001b[38;5;66;03m# TODO(SLEP6): remove when metadata routing cannot be disabled.\u001b[39;00m\n\u001b[1;32m    387\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_ \u001b[38;5;241m=\u001b[39m clone(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator)\n\u001b[0;32m--> 388\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mestimator_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mfit_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    390\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfeature_names_in_\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m    391\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeature_names_in_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimator_\u001b[38;5;241m.\u001b[39mfeature_names_in_\n",
            "File \u001b[0;32m~/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/sklearn/base.py:1473\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[0;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1466\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[1;32m   1468\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[1;32m   1469\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[1;32m   1470\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[1;32m   1471\u001b[0m     )\n\u001b[1;32m   1472\u001b[0m ):\n\u001b[0;32m-> 1473\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[0;32m~/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/sklearn/linear_model/_logistic.py:1276\u001b[0m, in \u001b[0;36mLogisticRegression.fit\u001b[0;34m(self, X, y, sample_weight)\u001b[0m\n\u001b[1;32m   1270\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   1271\u001b[0m         warnings\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m   1272\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m > 1 does not have any effect when\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1273\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msolver\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is set to \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mliblinear\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. Got \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_jobs\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1274\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m = \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(effective_n_jobs(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_jobs))\n\u001b[1;32m   1275\u001b[0m         )\n\u001b[0;32m-> 1276\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcoef_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mintercept_, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43m_fit_liblinear\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1277\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1278\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1279\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1280\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit_intercept\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1281\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mintercept_scaling\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1282\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1283\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpenalty\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1284\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdual\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1285\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1286\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1287\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1288\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1289\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1291\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m solver \u001b[38;5;129;01min\u001b[39;00m [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msag\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msaga\u001b[39m\u001b[38;5;124m\"\u001b[39m]:\n",
            "File \u001b[0;32m~/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/sklearn/svm/_base.py:1215\u001b[0m, in \u001b[0;36m_fit_liblinear\u001b[0;34m(X, y, C, fit_intercept, intercept_scaling, class_weight, penalty, dual, verbose, max_iter, tol, random_state, multi_class, loss, epsilon, sample_weight)\u001b[0m\n\u001b[1;32m   1212\u001b[0m sample_weight \u001b[38;5;241m=\u001b[39m _check_sample_weight(sample_weight, X, dtype\u001b[38;5;241m=\u001b[39mnp\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[1;32m   1214\u001b[0m solver_type \u001b[38;5;241m=\u001b[39m _get_liblinear_solver_type(multi_class, penalty, loss, dual)\n\u001b[0;32m-> 1215\u001b[0m raw_coef_, n_iter_ \u001b[38;5;241m=\u001b[39m \u001b[43mliblinear\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_wrap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1216\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1217\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_ind\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1218\u001b[0m \u001b[43m    \u001b[49m\u001b[43msp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43missparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1219\u001b[0m \u001b[43m    \u001b[49m\u001b[43msolver_type\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1220\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtol\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1221\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1222\u001b[0m \u001b[43m    \u001b[49m\u001b[43mC\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1223\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1224\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1225\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrnd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrandint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miinfo\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mi\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1226\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepsilon\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1227\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1228\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1229\u001b[0m \u001b[38;5;66;03m# Regarding rnd.randint(..) in the above signature:\u001b[39;00m\n\u001b[1;32m   1230\u001b[0m \u001b[38;5;66;03m# seed for srand in range [0..INT_MAX); due to limitations in Numpy\u001b[39;00m\n\u001b[1;32m   1231\u001b[0m \u001b[38;5;66;03m# on 32-bit platforms, we can't get to the UINT_MAX limit that\u001b[39;00m\n\u001b[1;32m   1232\u001b[0m \u001b[38;5;66;03m# srand supports\u001b[39;00m\n\u001b[1;32m   1233\u001b[0m n_iter_max \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmax\u001b[39m(n_iter_)\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "#Featur Selection, you may want to change the feature selection methods\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005\n",
        ")\n",
        "#Classifier\n",
        "#There could exist more parameters. Please search in your defined parameter\n",
        "#space for model performance improvement\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01,\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EZcuiA56Ls"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuOqHMwFzWHv"
      },
      "source": [
        "## Assignment 1. Improve the model performance using different types of feature combinations. (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kR-jSPN0i52"
      },
      "source": [
        " Hint: Currently we are only using feat_baseline. You may want to try other feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wD6HOgC_zuiq"
      },
      "outputs": [],
      "source": [
        "#######You may need to go back to the feature preparation code and check#########\n",
        "feat_recent_ESM = pd.concat([feat_current_ESM,feat_ImmediatePast_ESM ], axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_recent_ESM, feat_time],axis=1)\n",
        "\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6745160499877482\n",
            "Training completed for Fold_1 with AUC: 0.5925673227871029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.5935339308578745\n",
            "Training completed for Fold_3 with AUC: 0.6148589930477686\n",
            "Training completed for Fold_4 with AUC: 0.6126336684382403\n",
            "0.6176219930237469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMEhtgwJz9IP"
      },
      "source": [
        "## Assignment 2. Please try different feature selection methods (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FCRh8eF0zud"
      },
      "source": [
        "Hint: Currently, we are using LASSO filter for feature selection. Please consider using embedded method as well(same model for both feature selection and model training). Besides, the threshold for LASSO filter may also affect the performance. **Sepcifically, there is a method called 'mean' which is using mean of feature importances of all features as threshold.** Please try both different feature selection methods and different thresholds for filtering features to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bwna8wpv3Dhj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "Training completed for Fold_1 with AUC: 0.6017932616833715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.6393451618803732\n",
            "Training completed for Fold_3 with AUC: 0.6202273491814307\n",
            "Training completed for Fold_4 with AUC: 0.6385029134917077\n",
            "0.6351244356056223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "xgb_selector = SelectFromModel(\n",
        "    estimator=XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
        "    threshold='mean' # Or a different threshold\n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_FJZurk3ulY"
      },
      "source": [
        "## Assignment 3. Please try using hyperopt for model hyperparameter tuning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQcs-te34PK"
      },
      "source": [
        "Hint: Please be aware that for revised xgboost classifier EvXGBClassifier, there exist other parameters other than default XGBClassifier parameters such as eval_size.\n",
        "\n",
        "For hyperparameter tuning, we will use 20% of training set as validation set to avoid data leakage.\n",
        "\n",
        "If it is too timeconsuming to run the code in colab, please run the code locally and consider using [ray tune](https://docs.ray.io/en/latest/tune/index.html) if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8PyEHuv4R-L",
        "outputId": "ee08935d-a307-409f-ba56-901261230535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.10trial/s, best loss: -0.6189693231281869]\n",
            "Best hyperparameters: {'colsample_bytree': 0.9012966810519976, 'gamma': 0.22247273381633864, 'learning_rate': 0.028542060596548048, 'max_depth': 7.0, 'min_child_weight': 8.0, 'n_estimators': 800.0, 'reg_alpha': 0.2608104132350774, 'reg_lambda': 0.2620754838361904, 'subsample': 0.8360036468008175}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            smote = SMOTENC(\n",
        "                categorical_features=[X_train_scaled.shape[1]-1],\n",
        "                random_state=int(params['random_state'])\n",
        "            )\n",
        "        else:\n",
        "            smote = SMOTE(random_state=int(params['random_state']))\n",
        "        X_train_os, y_train_os = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        selector = SelectFromModel(\n",
        "            LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                               random_state=int(params['random_state'])),\n",
        "            threshold='mean'\n",
        "        )\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = LogisticRegression(\n",
        "            random_state=int(params['random_state']),\n",
        "            max_iter=1000\n",
        "        )\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "space = {\n",
        "    'max_depth':          hp.quniform('max_depth', 3, 10, 1),  # Integer between 3 and 10\n",
        "    'min_child_weight':   hp.quniform('min_child_weight', 1, 10, 1), # Integer between 1 and 10\n",
        "    'subsample':          hp.uniform('subsample', 0.6, 1.0),  # Float between 0.6 and 1.0\n",
        "    'colsample_bytree':   hp.uniform('colsample_bytree', 0.6, 1.0), # Float between 0.6 and 1.0\n",
        "    'gamma':              hp.uniform('gamma', 0, 0.5),      # Float between 0 and 0.5\n",
        "    'learning_rate':      hp.loguniform('learning_rate', -5, 0), # Float on a log scale (0.0067 to 1)\n",
        "    'n_estimators':       hp.quniform('n_estimators', 100, 1000, 50), # Integer between 100 and 1000, steps of 50\n",
        "    'reg_lambda':         hp.uniform('reg_lambda', 0, 1),     # Float between 0 and 1 (L2 regularization)\n",
        "    'reg_alpha':          hp.uniform('reg_alpha', 0, 0.5),    # Float between 0 and 0.5 (L1 regularization)\n",
        "    'random_state':       42 # Keeping random_state fixed\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'EvXGBClassifier' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[5], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#Classifier\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m estimator \u001b[38;5;241m=\u001b[39m \u001b[43mEvXGBClassifier\u001b[49m(\n\u001b[1;32m      3\u001b[0m     colsample_bytree\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcolsample_bytree\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      4\u001b[0m     gamma\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mgamma\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      5\u001b[0m     learning_rate\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m      6\u001b[0m     max_depth\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      7\u001b[0m     min_child_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmin_child_weight\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      8\u001b[0m     n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mint\u001b[39m(best[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m'\u001b[39m]),\n\u001b[1;32m      9\u001b[0m     reg_alpha\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_alpha\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     10\u001b[0m     reg_lambda\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mreg_lambda\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     11\u001b[0m     subsample\u001b[38;5;241m=\u001b[39mbest[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msubsample\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[1;32m     12\u001b[0m     random_state\u001b[38;5;241m=\u001b[39mRANDOM_STATE,\n\u001b[1;32m     13\u001b[0m     eval_metric\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlogloss\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     14\u001b[0m     eval_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.2\u001b[39m,\n\u001b[1;32m     15\u001b[0m     early_stopping_rounds\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m,\n\u001b[1;32m     16\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mbinary:logistic\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;66;03m#Prediction instead of regression\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     verbosity\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m,\n\u001b[1;32m     18\u001b[0m     use_label_encoder\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m,  \u001b[38;5;66;03m# Avoid warning about label encoder\u001b[39;00m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     21\u001b[0m results \u001b[38;5;241m=\u001b[39m perform_cross_validation(X, y, groups, estimator, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, select\u001b[38;5;241m=\u001b[39m[xgb_selector], oversample\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m     22\u001b[0m auc_values \u001b[38;5;241m=\u001b[39m [results[i]\u001b[38;5;241m.\u001b[39mmetrics[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAUC\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(results))]\n",
            "\u001b[0;31mNameError\u001b[0m: name 'EvXGBClassifier' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#Classifier\n",
        "estimator = EvXGBClassifier(\n",
        "    colsample_bytree=best['colsample_bytree'],\n",
        "    gamma=best['gamma'],\n",
        "    learning_rate=best['learning_rate'],\n",
        "    max_depth=int(best['max_depth']),\n",
        "    min_child_weight=int(best['min_child_weight']),\n",
        "    n_estimators=int(best['n_estimators']),\n",
        "    reg_alpha=best['reg_alpha'],\n",
        "    reg_lambda=best['reg_lambda'],\n",
        "    subsample=best['subsample'],\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    use_label_encoder=False,  # Avoid warning about label encoder\n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPz966iD6yoH"
      },
      "source": [
        "## Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for **tabular data** to improve model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfbskLs7qmc"
      },
      "source": [
        "Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for **tabular data** and see if there is performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "class SklearnTabNet(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = TabNetClassifier(**kwargs)\n",
        "        self.fit_params = kwargs\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert DataFrame to NumPy if needed\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        y = self.label_encoder.fit_transform(y)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        preds = self.model.predict(X)\n",
        "        return self.label_encoder.inverse_transform(preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        return self.model.predict_proba(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbE_mod6TUIn"
      },
      "source": [
        "You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDEeP0pkBsn6"
      },
      "source": [
        "Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MZk2A2-I7nPd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82129 |  0:00:00s\n",
            "epoch 1  | loss: 0.69413 |  0:00:00s\n",
            "epoch 2  | loss: 0.68169 |  0:00:00s\n",
            "epoch 3  | loss: 0.67449 |  0:00:00s\n",
            "epoch 4  | loss: 0.66989 |  0:00:00s\n",
            "epoch 5  | loss: 0.66785 |  0:00:00s\n",
            "epoch 6  | loss: 0.66801 |  0:00:00s\n",
            "epoch 7  | loss: 0.6692  |  0:00:00s\n",
            "epoch 8  | loss: 0.67093 |  0:00:00s\n",
            "epoch 9  | loss: 0.66066 |  0:00:00s\n",
            "epoch 10 | loss: 0.66997 |  0:00:00s\n",
            "epoch 11 | loss: 0.66499 |  0:00:00s\n",
            "epoch 12 | loss: 0.6652  |  0:00:00s\n",
            "epoch 13 | loss: 0.66063 |  0:00:00s\n",
            "epoch 14 | loss: 0.66369 |  0:00:00s\n",
            "epoch 15 | loss: 0.66042 |  0:00:00s\n",
            "epoch 16 | loss: 0.66606 |  0:00:01s\n",
            "epoch 17 | loss: 0.6549  |  0:00:01s\n",
            "epoch 18 | loss: 0.66593 |  0:00:01s\n",
            "epoch 19 | loss: 0.66275 |  0:00:01s\n",
            "epoch 20 | loss: 0.66116 |  0:00:01s\n",
            "epoch 21 | loss: 0.67011 |  0:00:01s\n",
            "epoch 22 | loss: 0.66674 |  0:00:01s\n",
            "epoch 23 | loss: 0.66347 |  0:00:01s\n",
            "epoch 24 | loss: 0.66219 |  0:00:01s\n",
            "epoch 25 | loss: 0.66776 |  0:00:01s\n",
            "epoch 26 | loss: 0.66323 |  0:00:01s\n",
            "epoch 27 | loss: 0.66517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66195 |  0:00:01s\n",
            "epoch 29 | loss: 0.66681 |  0:00:01s\n",
            "epoch 30 | loss: 0.66132 |  0:00:01s\n",
            "epoch 31 | loss: 0.66383 |  0:00:01s\n",
            "epoch 32 | loss: 0.66425 |  0:00:02s\n",
            "epoch 33 | loss: 0.65932 |  0:00:02s\n",
            "epoch 34 | loss: 0.66514 |  0:00:02s\n",
            "epoch 35 | loss: 0.66176 |  0:00:02s\n",
            "epoch 36 | loss: 0.66962 |  0:00:02s\n",
            "epoch 37 | loss: 0.66413 |  0:00:02s\n",
            "epoch 38 | loss: 0.66864 |  0:00:02s\n",
            "epoch 39 | loss: 0.66556 |  0:00:02s\n",
            "epoch 40 | loss: 0.66459 |  0:00:02s\n",
            "epoch 41 | loss: 0.66431 |  0:00:02s\n",
            "epoch 42 | loss: 0.66639 |  0:00:02s\n",
            "epoch 43 | loss: 0.66566 |  0:00:02s\n",
            "epoch 44 | loss: 0.66461 |  0:00:02s\n",
            "epoch 45 | loss: 0.66274 |  0:00:02s\n",
            "epoch 46 | loss: 0.66234 |  0:00:02s\n",
            "epoch 47 | loss: 0.66805 |  0:00:02s\n",
            "epoch 48 | loss: 0.66645 |  0:00:02s\n",
            "epoch 49 | loss: 0.6657  |  0:00:03s\n",
            "epoch 50 | loss: 0.6629  |  0:00:03s\n",
            "epoch 51 | loss: 0.65916 |  0:00:03s\n",
            "epoch 52 | loss: 0.66155 |  0:00:03s\n",
            "epoch 53 | loss: 0.66205 |  0:00:03s\n",
            "epoch 54 | loss: 0.65815 |  0:00:03s\n",
            "epoch 55 | loss: 0.66152 |  0:00:03s\n",
            "epoch 56 | loss: 0.66028 |  0:00:03s\n",
            "epoch 57 | loss: 0.66463 |  0:00:03s\n",
            "epoch 58 | loss: 0.65953 |  0:00:03s\n",
            "epoch 59 | loss: 0.66682 |  0:00:03s\n",
            "epoch 60 | loss: 0.66321 |  0:00:03s\n",
            "epoch 61 | loss: 0.66373 |  0:00:04s\n",
            "epoch 62 | loss: 0.66355 |  0:00:04s\n",
            "epoch 63 | loss: 0.66078 |  0:00:04s\n",
            "epoch 64 | loss: 0.66516 |  0:00:04s\n",
            "epoch 65 | loss: 0.66152 |  0:00:04s\n",
            "epoch 66 | loss: 0.66482 |  0:00:04s\n",
            "epoch 67 | loss: 0.66327 |  0:00:04s\n",
            "epoch 68 | loss: 0.66354 |  0:00:04s\n",
            "epoch 69 | loss: 0.66134 |  0:00:04s\n",
            "epoch 70 | loss: 0.66747 |  0:00:04s\n",
            "epoch 71 | loss: 0.65962 |  0:00:04s\n",
            "epoch 72 | loss: 0.66135 |  0:00:04s\n",
            "epoch 73 | loss: 0.66312 |  0:00:04s\n",
            "epoch 74 | loss: 0.66194 |  0:00:04s\n",
            "epoch 75 | loss: 0.66819 |  0:00:04s\n",
            "epoch 76 | loss: 0.66151 |  0:00:04s\n",
            "epoch 77 | loss: 0.65716 |  0:00:04s\n",
            "epoch 78 | loss: 0.66099 |  0:00:04s\n",
            "epoch 79 | loss: 0.6632  |  0:00:05s\n",
            "epoch 80 | loss: 0.6629  |  0:00:05s\n",
            "epoch 81 | loss: 0.66266 |  0:00:05s\n",
            "epoch 82 | loss: 0.65956 |  0:00:05s\n",
            "epoch 83 | loss: 0.6576  |  0:00:05s\n",
            "epoch 84 | loss: 0.66103 |  0:00:05s\n",
            "epoch 85 | loss: 0.66449 |  0:00:05s\n",
            "epoch 86 | loss: 0.6628  |  0:00:05s\n",
            "epoch 87 | loss: 0.66488 |  0:00:05s\n",
            "epoch 88 | loss: 0.6673  |  0:00:05s\n",
            "epoch 89 | loss: 0.66309 |  0:00:05s\n",
            "epoch 90 | loss: 0.66404 |  0:00:05s\n",
            "epoch 91 | loss: 0.66472 |  0:00:05s\n",
            "epoch 92 | loss: 0.66572 |  0:00:05s\n",
            "epoch 93 | loss: 0.66462 |  0:00:05s\n",
            "epoch 94 | loss: 0.66543 |  0:00:05s\n",
            "epoch 95 | loss: 0.66423 |  0:00:05s\n",
            "epoch 96 | loss: 0.66119 |  0:00:05s\n",
            "epoch 97 | loss: 0.66243 |  0:00:06s\n",
            "epoch 98 | loss: 0.66079 |  0:00:06s\n",
            "epoch 99 | loss: 0.66241 |  0:00:06s\n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "epoch 0  | loss: 0.79829 |  0:00:00s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.66669 |  0:00:00s\n",
            "epoch 2  | loss: 0.66416 |  0:00:00s\n",
            "epoch 3  | loss: 0.65561 |  0:00:00s\n",
            "epoch 4  | loss: 0.65932 |  0:00:00s\n",
            "epoch 5  | loss: 0.66116 |  0:00:00s\n",
            "epoch 6  | loss: 0.65419 |  0:00:00s\n",
            "epoch 7  | loss: 0.65517 |  0:00:00s\n",
            "epoch 8  | loss: 0.66334 |  0:00:00s\n",
            "epoch 9  | loss: 0.65327 |  0:00:00s\n",
            "epoch 10 | loss: 0.65971 |  0:00:00s\n",
            "epoch 11 | loss: 0.659   |  0:00:00s\n",
            "epoch 12 | loss: 0.66051 |  0:00:00s\n",
            "epoch 13 | loss: 0.65711 |  0:00:00s\n",
            "epoch 14 | loss: 0.6592  |  0:00:00s\n",
            "epoch 15 | loss: 0.65609 |  0:00:00s\n",
            "epoch 16 | loss: 0.65753 |  0:00:00s\n",
            "epoch 17 | loss: 0.66103 |  0:00:01s\n",
            "epoch 18 | loss: 0.66029 |  0:00:01s\n",
            "epoch 19 | loss: 0.65484 |  0:00:01s\n",
            "epoch 20 | loss: 0.65865 |  0:00:01s\n",
            "epoch 21 | loss: 0.65416 |  0:00:01s\n",
            "epoch 22 | loss: 0.65564 |  0:00:01s\n",
            "epoch 23 | loss: 0.65826 |  0:00:01s\n",
            "epoch 24 | loss: 0.65388 |  0:00:01s\n",
            "epoch 25 | loss: 0.65951 |  0:00:01s\n",
            "epoch 26 | loss: 0.65773 |  0:00:01s\n",
            "epoch 27 | loss: 0.6569  |  0:00:01s\n",
            "epoch 28 | loss: 0.66149 |  0:00:01s\n",
            "epoch 29 | loss: 0.66262 |  0:00:01s\n",
            "epoch 30 | loss: 0.66118 |  0:00:01s\n",
            "epoch 31 | loss: 0.65915 |  0:00:01s\n",
            "epoch 32 | loss: 0.65585 |  0:00:01s\n",
            "epoch 33 | loss: 0.65769 |  0:00:01s\n",
            "epoch 34 | loss: 0.6549  |  0:00:01s\n",
            "epoch 35 | loss: 0.65304 |  0:00:01s\n",
            "epoch 36 | loss: 0.66283 |  0:00:01s\n",
            "epoch 37 | loss: 0.66036 |  0:00:01s\n",
            "epoch 38 | loss: 0.65727 |  0:00:01s\n",
            "epoch 39 | loss: 0.65843 |  0:00:01s\n",
            "epoch 40 | loss: 0.65764 |  0:00:02s\n",
            "epoch 41 | loss: 0.65725 |  0:00:02s\n",
            "epoch 42 | loss: 0.66286 |  0:00:02s\n",
            "epoch 43 | loss: 0.66094 |  0:00:02s\n",
            "epoch 44 | loss: 0.65994 |  0:00:02s\n",
            "epoch 45 | loss: 0.65736 |  0:00:02s\n",
            "epoch 46 | loss: 0.66132 |  0:00:02s\n",
            "epoch 47 | loss: 0.65876 |  0:00:02s\n",
            "epoch 48 | loss: 0.655   |  0:00:02s\n",
            "epoch 49 | loss: 0.6546  |  0:00:02s\n",
            "epoch 50 | loss: 0.6517  |  0:00:02s\n",
            "epoch 51 | loss: 0.65076 |  0:00:02s\n",
            "epoch 52 | loss: 0.66134 |  0:00:02s\n",
            "epoch 53 | loss: 0.65962 |  0:00:02s\n",
            "epoch 54 | loss: 0.65704 |  0:00:02s\n",
            "epoch 55 | loss: 0.65682 |  0:00:02s\n",
            "epoch 56 | loss: 0.65616 |  0:00:02s\n",
            "epoch 57 | loss: 0.66184 |  0:00:02s\n",
            "epoch 58 | loss: 0.66032 |  0:00:02s\n",
            "epoch 59 | loss: 0.6559  |  0:00:02s\n",
            "epoch 60 | loss: 0.65696 |  0:00:02s\n",
            "epoch 61 | loss: 0.65936 |  0:00:02s\n",
            "epoch 62 | loss: 0.65319 |  0:00:02s\n",
            "epoch 63 | loss: 0.65951 |  0:00:02s\n",
            "epoch 64 | loss: 0.6624  |  0:00:02s\n",
            "epoch 65 | loss: 0.65389 |  0:00:02s\n",
            "epoch 66 | loss: 0.65476 |  0:00:02s\n",
            "epoch 67 | loss: 0.6564  |  0:00:03s\n",
            "epoch 68 | loss: 0.6578  |  0:00:03s\n",
            "epoch 69 | loss: 0.65999 |  0:00:03s\n",
            "epoch 70 | loss: 0.66151 |  0:00:03s\n",
            "epoch 71 | loss: 0.65282 |  0:00:03s\n",
            "epoch 72 | loss: 0.65757 |  0:00:03s\n",
            "epoch 73 | loss: 0.65051 |  0:00:03s\n",
            "epoch 74 | loss: 0.65598 |  0:00:03s\n",
            "epoch 75 | loss: 0.65955 |  0:00:03s\n",
            "epoch 76 | loss: 0.65574 |  0:00:03s\n",
            "epoch 77 | loss: 0.66164 |  0:00:03s\n",
            "epoch 78 | loss: 0.65796 |  0:00:03s\n",
            "epoch 79 | loss: 0.65643 |  0:00:03s\n",
            "epoch 80 | loss: 0.65572 |  0:00:03s\n",
            "epoch 81 | loss: 0.65466 |  0:00:03s\n",
            "epoch 82 | loss: 0.65868 |  0:00:03s\n",
            "epoch 83 | loss: 0.66234 |  0:00:03s\n",
            "epoch 84 | loss: 0.65762 |  0:00:03s\n",
            "epoch 85 | loss: 0.65425 |  0:00:03s\n",
            "epoch 86 | loss: 0.65816 |  0:00:03s\n",
            "epoch 87 | loss: 0.65813 |  0:00:03s\n",
            "epoch 88 | loss: 0.66011 |  0:00:03s\n",
            "epoch 89 | loss: 0.65716 |  0:00:03s\n",
            "epoch 90 | loss: 0.65569 |  0:00:03s\n",
            "epoch 91 | loss: 0.6534  |  0:00:03s\n",
            "epoch 92 | loss: 0.65997 |  0:00:03s\n",
            "epoch 93 | loss: 0.65685 |  0:00:03s\n",
            "epoch 94 | loss: 0.65627 |  0:00:04s\n",
            "epoch 95 | loss: 0.65819 |  0:00:04s\n",
            "epoch 96 | loss: 0.65828 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.65309 |  0:00:04s\n",
            "epoch 99 | loss: 0.65704 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.74116 |  0:00:00s\n",
            "epoch 1  | loss: 0.70513 |  0:00:00s\n",
            "epoch 2  | loss: 0.69432 |  0:00:00s\n",
            "epoch 3  | loss: 0.67477 |  0:00:00s\n",
            "epoch 4  | loss: 0.67408 |  0:00:00s\n",
            "epoch 5  | loss: 0.66448 |  0:00:00s\n",
            "epoch 6  | loss: 0.66114 |  0:00:00s\n",
            "epoch 7  | loss: 0.67036 |  0:00:00s\n",
            "epoch 8  | loss: 0.66511 |  0:00:00s\n",
            "epoch 9  | loss: 0.66068 |  0:00:00s\n",
            "epoch 10 | loss: 0.66214 |  0:00:00s\n",
            "epoch 11 | loss: 0.6624  |  0:00:00s\n",
            "epoch 12 | loss: 0.66165 |  0:00:00s\n",
            "epoch 13 | loss: 0.66099 |  0:00:00s\n",
            "epoch 14 | loss: 0.66227 |  0:00:00s\n",
            "epoch 15 | loss: 0.66577 |  0:00:00s\n",
            "epoch 16 | loss: 0.6587  |  0:00:00s\n",
            "epoch 17 | loss: 0.66433 |  0:00:00s\n",
            "epoch 18 | loss: 0.6621  |  0:00:00s\n",
            "epoch 19 | loss: 0.6587  |  0:00:00s\n",
            "epoch 20 | loss: 0.65557 |  0:00:00s\n",
            "epoch 21 | loss: 0.65922 |  0:00:00s\n",
            "epoch 22 | loss: 0.66284 |  0:00:00s\n",
            "epoch 23 | loss: 0.65952 |  0:00:00s\n",
            "epoch 24 | loss: 0.66066 |  0:00:00s\n",
            "epoch 25 | loss: 0.65914 |  0:00:01s\n",
            "epoch 26 | loss: 0.65793 |  0:00:01s\n",
            "epoch 27 | loss: 0.66025 |  0:00:01s\n",
            "epoch 28 | loss: 0.66212 |  0:00:01s\n",
            "epoch 29 | loss: 0.66159 |  0:00:01s\n",
            "epoch 30 | loss: 0.65868 |  0:00:01s\n",
            "epoch 31 | loss: 0.66358 |  0:00:01s\n",
            "epoch 32 | loss: 0.65803 |  0:00:01s\n",
            "epoch 33 | loss: 0.65629 |  0:00:01s\n",
            "epoch 34 | loss: 0.658   |  0:00:01s\n",
            "epoch 35 | loss: 0.66108 |  0:00:01s\n",
            "epoch 36 | loss: 0.66254 |  0:00:01s\n",
            "epoch 37 | loss: 0.66275 |  0:00:01s\n",
            "epoch 38 | loss: 0.65842 |  0:00:01s\n",
            "epoch 39 | loss: 0.66139 |  0:00:01s\n",
            "epoch 40 | loss: 0.65955 |  0:00:01s\n",
            "epoch 41 | loss: 0.65772 |  0:00:01s\n",
            "epoch 42 | loss: 0.66374 |  0:00:01s\n",
            "epoch 43 | loss: 0.66061 |  0:00:01s\n",
            "epoch 44 | loss: 0.6623  |  0:00:01s\n",
            "epoch 45 | loss: 0.65957 |  0:00:01s\n",
            "epoch 46 | loss: 0.66121 |  0:00:01s\n",
            "epoch 47 | loss: 0.66013 |  0:00:01s\n",
            "epoch 48 | loss: 0.66283 |  0:00:01s\n",
            "epoch 49 | loss: 0.66251 |  0:00:02s\n",
            "epoch 50 | loss: 0.65744 |  0:00:02s\n",
            "epoch 51 | loss: 0.66202 |  0:00:02s\n",
            "epoch 52 | loss: 0.65722 |  0:00:02s\n",
            "epoch 53 | loss: 0.66317 |  0:00:02s\n",
            "epoch 54 | loss: 0.65923 |  0:00:02s\n",
            "epoch 55 | loss: 0.65988 |  0:00:02s\n",
            "epoch 56 | loss: 0.66293 |  0:00:02s\n",
            "epoch 57 | loss: 0.66161 |  0:00:02s\n",
            "epoch 58 | loss: 0.65706 |  0:00:02s\n",
            "epoch 59 | loss: 0.66115 |  0:00:02s\n",
            "epoch 60 | loss: 0.65831 |  0:00:02s\n",
            "epoch 61 | loss: 0.65972 |  0:00:02s\n",
            "epoch 62 | loss: 0.66007 |  0:00:02s\n",
            "epoch 63 | loss: 0.65941 |  0:00:02s\n",
            "epoch 64 | loss: 0.65281 |  0:00:02s\n",
            "epoch 65 | loss: 0.65539 |  0:00:02s\n",
            "epoch 66 | loss: 0.66362 |  0:00:02s\n",
            "epoch 67 | loss: 0.66359 |  0:00:02s\n",
            "epoch 68 | loss: 0.66279 |  0:00:02s\n",
            "epoch 69 | loss: 0.66138 |  0:00:02s\n",
            "epoch 70 | loss: 0.66039 |  0:00:02s\n",
            "epoch 71 | loss: 0.65481 |  0:00:02s\n",
            "epoch 72 | loss: 0.65951 |  0:00:03s\n",
            "epoch 73 | loss: 0.65521 |  0:00:03s\n",
            "epoch 74 | loss: 0.66144 |  0:00:03s\n",
            "epoch 75 | loss: 0.65836 |  0:00:03s\n",
            "epoch 76 | loss: 0.66008 |  0:00:03s\n",
            "epoch 77 | loss: 0.66101 |  0:00:03s\n",
            "epoch 78 | loss: 0.65836 |  0:00:03s\n",
            "epoch 79 | loss: 0.65731 |  0:00:03s\n",
            "epoch 80 | loss: 0.65836 |  0:00:03s\n",
            "epoch 81 | loss: 0.65784 |  0:00:03s\n",
            "epoch 82 | loss: 0.66167 |  0:00:03s\n",
            "epoch 83 | loss: 0.66068 |  0:00:03s\n",
            "epoch 84 | loss: 0.66122 |  0:00:03s\n",
            "epoch 85 | loss: 0.65947 |  0:00:03s\n",
            "epoch 86 | loss: 0.65888 |  0:00:03s\n",
            "epoch 87 | loss: 0.65789 |  0:00:03s\n",
            "epoch 88 | loss: 0.65418 |  0:00:03s\n",
            "epoch 89 | loss: 0.65439 |  0:00:03s\n",
            "epoch 90 | loss: 0.65631 |  0:00:03s\n",
            "epoch 91 | loss: 0.66406 |  0:00:03s\n",
            "epoch 92 | loss: 0.65971 |  0:00:03s\n",
            "epoch 93 | loss: 0.66105 |  0:00:03s\n",
            "epoch 94 | loss: 0.66296 |  0:00:03s\n",
            "epoch 95 | loss: 0.66138 |  0:00:03s\n",
            "epoch 96 | loss: 0.65509 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.66348 |  0:00:04s\n",
            "epoch 99 | loss: 0.65617 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.632787634900311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s\n",
            "epoch 1  | loss: 0.67173 |  0:00:00s\n",
            "epoch 2  | loss: 0.65725 |  0:00:00s\n",
            "epoch 3  | loss: 0.65813 |  0:00:00s\n",
            "epoch 4  | loss: 0.65162 |  0:00:00s\n",
            "epoch 5  | loss: 0.65938 |  0:00:00s\n",
            "epoch 6  | loss: 0.65699 |  0:00:00s\n",
            "epoch 7  | loss: 0.65609 |  0:00:00s\n",
            "epoch 8  | loss: 0.65957 |  0:00:00s\n",
            "epoch 9  | loss: 0.65455 |  0:00:00s\n",
            "epoch 10 | loss: 0.65117 |  0:00:00s\n",
            "epoch 11 | loss: 0.64928 |  0:00:00s\n",
            "epoch 12 | loss: 0.6484  |  0:00:00s\n",
            "epoch 13 | loss: 0.65202 |  0:00:00s\n",
            "epoch 14 | loss: 0.65083 |  0:00:00s\n",
            "epoch 15 | loss: 0.65236 |  0:00:00s\n",
            "epoch 16 | loss: 0.65407 |  0:00:00s\n",
            "epoch 17 | loss: 0.65722 |  0:00:00s\n",
            "epoch 18 | loss: 0.65453 |  0:00:00s\n",
            "epoch 19 | loss: 0.65006 |  0:00:00s\n",
            "epoch 20 | loss: 0.65286 |  0:00:00s\n",
            "epoch 21 | loss: 0.64635 |  0:00:00s\n",
            "epoch 22 | loss: 0.65601 |  0:00:00s\n",
            "epoch 23 | loss: 0.65498 |  0:00:01s\n",
            "epoch 24 | loss: 0.65045 |  0:00:01s\n",
            "epoch 25 | loss: 0.64957 |  0:00:01s\n",
            "epoch 26 | loss: 0.64914 |  0:00:01s\n",
            "epoch 27 | loss: 0.64831 |  0:00:01s\n",
            "epoch 28 | loss: 0.65263 |  0:00:01s\n",
            "epoch 29 | loss: 0.65653 |  0:00:01s\n",
            "epoch 30 | loss: 0.6524  |  0:00:01s\n",
            "epoch 31 | loss: 0.64977 |  0:00:01s\n",
            "epoch 32 | loss: 0.6519  |  0:00:01s\n",
            "epoch 33 | loss: 0.65507 |  0:00:01s\n",
            "epoch 34 | loss: 0.65584 |  0:00:01s\n",
            "epoch 35 | loss: 0.64758 |  0:00:01s\n",
            "epoch 36 | loss: 0.64753 |  0:00:01s\n",
            "epoch 37 | loss: 0.6508  |  0:00:01s\n",
            "epoch 38 | loss: 0.65147 |  0:00:01s\n",
            "epoch 39 | loss: 0.65669 |  0:00:01s\n",
            "epoch 40 | loss: 0.65063 |  0:00:01s\n",
            "epoch 41 | loss: 0.65125 |  0:00:01s\n",
            "epoch 42 | loss: 0.65242 |  0:00:01s\n",
            "epoch 43 | loss: 0.65139 |  0:00:01s\n",
            "epoch 44 | loss: 0.64979 |  0:00:01s\n",
            "epoch 45 | loss: 0.65282 |  0:00:01s\n",
            "epoch 46 | loss: 0.64992 |  0:00:02s\n",
            "epoch 47 | loss: 0.65172 |  0:00:02s\n",
            "epoch 48 | loss: 0.65339 |  0:00:02s\n",
            "epoch 49 | loss: 0.65177 |  0:00:02s\n",
            "epoch 50 | loss: 0.6483  |  0:00:02s\n",
            "epoch 51 | loss: 0.65488 |  0:00:02s\n",
            "epoch 52 | loss: 0.65112 |  0:00:02s\n",
            "epoch 53 | loss: 0.64882 |  0:00:02s\n",
            "epoch 54 | loss: 0.65205 |  0:00:02s\n",
            "epoch 55 | loss: 0.65058 |  0:00:02s\n",
            "epoch 56 | loss: 0.65775 |  0:00:02s\n",
            "epoch 57 | loss: 0.65229 |  0:00:02s\n",
            "epoch 58 | loss: 0.65413 |  0:00:02s\n",
            "epoch 59 | loss: 0.65194 |  0:00:02s\n",
            "epoch 60 | loss: 0.65233 |  0:00:02s\n",
            "epoch 61 | loss: 0.65317 |  0:00:02s\n",
            "epoch 62 | loss: 0.65004 |  0:00:02s\n",
            "epoch 63 | loss: 0.65206 |  0:00:02s\n",
            "epoch 64 | loss: 0.65329 |  0:00:02s\n",
            "epoch 65 | loss: 0.65854 |  0:00:02s\n",
            "epoch 66 | loss: 0.64967 |  0:00:02s\n",
            "epoch 67 | loss: 0.65222 |  0:00:02s\n",
            "epoch 68 | loss: 0.65413 |  0:00:02s\n",
            "epoch 69 | loss: 0.64616 |  0:00:03s\n",
            "epoch 70 | loss: 0.65072 |  0:00:03s\n",
            "epoch 71 | loss: 0.65243 |  0:00:03s\n",
            "epoch 72 | loss: 0.65491 |  0:00:03s\n",
            "epoch 73 | loss: 0.64967 |  0:00:03s\n",
            "epoch 74 | loss: 0.65569 |  0:00:03s\n",
            "epoch 75 | loss: 0.64423 |  0:00:03s\n",
            "epoch 76 | loss: 0.64815 |  0:00:03s\n",
            "epoch 77 | loss: 0.64981 |  0:00:03s\n",
            "epoch 78 | loss: 0.65015 |  0:00:03s\n",
            "epoch 79 | loss: 0.64704 |  0:00:03s\n",
            "epoch 80 | loss: 0.65555 |  0:00:03s\n",
            "epoch 81 | loss: 0.65336 |  0:00:03s\n",
            "epoch 82 | loss: 0.64887 |  0:00:03s\n",
            "epoch 83 | loss: 0.65124 |  0:00:03s\n",
            "epoch 84 | loss: 0.64734 |  0:00:03s\n",
            "epoch 85 | loss: 0.64845 |  0:00:03s\n",
            "epoch 86 | loss: 0.65064 |  0:00:03s\n",
            "epoch 87 | loss: 0.6533  |  0:00:03s\n",
            "epoch 88 | loss: 0.65552 |  0:00:04s\n",
            "epoch 89 | loss: 0.64917 |  0:00:04s\n",
            "epoch 90 | loss: 0.6497  |  0:00:04s\n",
            "epoch 91 | loss: 0.64727 |  0:00:04s\n",
            "epoch 92 | loss: 0.65358 |  0:00:04s\n",
            "epoch 93 | loss: 0.65438 |  0:00:04s\n",
            "epoch 94 | loss: 0.64729 |  0:00:04s\n",
            "epoch 95 | loss: 0.65835 |  0:00:04s\n",
            "epoch 96 | loss: 0.65608 |  0:00:04s\n",
            "epoch 97 | loss: 0.64505 |  0:00:04s\n",
            "epoch 98 | loss: 0.65118 |  0:00:04s\n",
            "epoch 99 | loss: 0.6487  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 1.16024 |  0:00:00s\n",
            "epoch 1  | loss: 0.71131 |  0:00:00s\n",
            "epoch 2  | loss: 0.70045 |  0:00:00s\n",
            "epoch 3  | loss: 0.69166 |  0:00:00s\n",
            "epoch 4  | loss: 0.67844 |  0:00:00s\n",
            "epoch 5  | loss: 0.67201 |  0:00:00s\n",
            "epoch 6  | loss: 0.66742 |  0:00:00s\n",
            "epoch 7  | loss: 0.65627 |  0:00:00s\n",
            "epoch 8  | loss: 0.65846 |  0:00:00s\n",
            "epoch 9  | loss: 0.66    |  0:00:00s\n",
            "epoch 10 | loss: 0.66268 |  0:00:00s\n",
            "epoch 11 | loss: 0.65486 |  0:00:00s\n",
            "epoch 12 | loss: 0.66073 |  0:00:00s\n",
            "epoch 13 | loss: 0.66184 |  0:00:00s\n",
            "epoch 14 | loss: 0.66056 |  0:00:00s\n",
            "epoch 15 | loss: 0.65274 |  0:00:00s\n",
            "epoch 16 | loss: 0.66015 |  0:00:00s\n",
            "epoch 17 | loss: 0.66601 |  0:00:00s\n",
            "epoch 18 | loss: 0.66265 |  0:00:00s\n",
            "epoch 19 | loss: 0.65902 |  0:00:00s\n",
            "epoch 20 | loss: 0.65729 |  0:00:00s\n",
            "epoch 21 | loss: 0.65535 |  0:00:01s\n",
            "epoch 22 | loss: 0.64845 |  0:00:01s\n",
            "epoch 23 | loss: 0.65655 |  0:00:01s\n",
            "epoch 24 | loss: 0.65627 |  0:00:01s\n",
            "epoch 25 | loss: 0.65279 |  0:00:01s\n",
            "epoch 26 | loss: 0.65325 |  0:00:01s\n",
            "epoch 27 | loss: 0.65968 |  0:00:01s\n",
            "epoch 28 | loss: 0.65152 |  0:00:01s\n",
            "epoch 29 | loss: 0.65759 |  0:00:01s\n",
            "epoch 30 | loss: 0.65344 |  0:00:01s\n",
            "epoch 31 | loss: 0.65118 |  0:00:01s\n",
            "epoch 32 | loss: 0.65427 |  0:00:01s\n",
            "epoch 33 | loss: 0.65642 |  0:00:01s\n",
            "epoch 34 | loss: 0.65423 |  0:00:01s\n",
            "epoch 35 | loss: 0.65555 |  0:00:01s\n",
            "epoch 36 | loss: 0.65816 |  0:00:01s\n",
            "epoch 37 | loss: 0.65569 |  0:00:01s\n",
            "epoch 38 | loss: 0.6559  |  0:00:01s\n",
            "epoch 39 | loss: 0.65315 |  0:00:01s\n",
            "epoch 40 | loss: 0.65212 |  0:00:02s\n",
            "epoch 41 | loss: 0.65438 |  0:00:02s\n",
            "epoch 42 | loss: 0.65336 |  0:00:02s\n",
            "epoch 43 | loss: 0.65686 |  0:00:02s\n",
            "epoch 44 | loss: 0.65681 |  0:00:02s\n",
            "epoch 45 | loss: 0.65714 |  0:00:02s\n",
            "epoch 46 | loss: 0.65902 |  0:00:02s\n",
            "epoch 47 | loss: 0.64796 |  0:00:02s\n",
            "epoch 48 | loss: 0.65183 |  0:00:02s\n",
            "epoch 49 | loss: 0.65885 |  0:00:02s\n",
            "epoch 50 | loss: 0.65517 |  0:00:02s\n",
            "epoch 51 | loss: 0.65815 |  0:00:02s\n",
            "epoch 52 | loss: 0.65451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65089 |  0:00:02s\n",
            "epoch 54 | loss: 0.6525  |  0:00:02s\n",
            "epoch 55 | loss: 0.6526  |  0:00:02s\n",
            "epoch 56 | loss: 0.6611  |  0:00:02s\n",
            "epoch 57 | loss: 0.65559 |  0:00:02s\n",
            "epoch 58 | loss: 0.65667 |  0:00:02s\n",
            "epoch 59 | loss: 0.65166 |  0:00:02s\n",
            "epoch 60 | loss: 0.6541  |  0:00:02s\n",
            "epoch 61 | loss: 0.64848 |  0:00:02s\n",
            "epoch 62 | loss: 0.65481 |  0:00:03s\n",
            "epoch 63 | loss: 0.65333 |  0:00:03s\n",
            "epoch 64 | loss: 0.65386 |  0:00:03s\n",
            "epoch 65 | loss: 0.64945 |  0:00:03s\n",
            "epoch 66 | loss: 0.64804 |  0:00:03s\n",
            "epoch 67 | loss: 0.65154 |  0:00:03s\n",
            "epoch 68 | loss: 0.65439 |  0:00:03s\n",
            "epoch 69 | loss: 0.65606 |  0:00:03s\n",
            "epoch 70 | loss: 0.65102 |  0:00:03s\n",
            "epoch 71 | loss: 0.65272 |  0:00:03s\n",
            "epoch 72 | loss: 0.65625 |  0:00:03s\n",
            "epoch 73 | loss: 0.65476 |  0:00:03s\n",
            "epoch 74 | loss: 0.64848 |  0:00:03s\n",
            "epoch 75 | loss: 0.65245 |  0:00:03s\n",
            "epoch 76 | loss: 0.65572 |  0:00:03s\n",
            "epoch 77 | loss: 0.66178 |  0:00:03s\n",
            "epoch 78 | loss: 0.65548 |  0:00:03s\n",
            "epoch 79 | loss: 0.65366 |  0:00:03s\n",
            "epoch 80 | loss: 0.66175 |  0:00:03s\n",
            "epoch 81 | loss: 0.65087 |  0:00:03s\n",
            "epoch 82 | loss: 0.65403 |  0:00:03s\n",
            "epoch 83 | loss: 0.654   |  0:00:03s\n",
            "epoch 84 | loss: 0.65244 |  0:00:03s\n",
            "epoch 85 | loss: 0.66005 |  0:00:04s\n",
            "epoch 86 | loss: 0.65493 |  0:00:04s\n",
            "epoch 87 | loss: 0.64949 |  0:00:04s\n",
            "epoch 88 | loss: 0.64943 |  0:00:04s\n",
            "epoch 89 | loss: 0.65977 |  0:00:04s\n",
            "epoch 90 | loss: 0.64923 |  0:00:04s\n",
            "epoch 91 | loss: 0.6526  |  0:00:04s\n",
            "epoch 92 | loss: 0.65189 |  0:00:04s\n",
            "epoch 93 | loss: 0.65097 |  0:00:04s\n",
            "epoch 94 | loss: 0.65219 |  0:00:04s\n",
            "epoch 95 | loss: 0.64573 |  0:00:04s\n",
            "epoch 96 | loss: 0.65529 |  0:00:04s\n",
            "epoch 97 | loss: 0.65213 |  0:00:04s\n",
            "epoch 98 | loss: 0.65517 |  0:00:04s\n",
            "epoch 99 | loss: 0.65794 |  0:00:04s\n",
            "Training completed for Fold_4 with AUC: 0.634548889031184\n",
            "0.6385634246774857\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#######Your code for deep learning model#########\n",
        "\n",
        "estimator = SklearnTabNet(\n",
        "    n_d=8,\n",
        "    n_a=8,\n",
        "    n_steps=3,\n",
        "    gamma=1.3,\n",
        "    cat_idxs=[X.columns.get_loc(c) for c in cats],\n",
        "    cat_dims=[2] * len(cats),  # Assuming binary categorical features\n",
        "    cat_emb_dim=1,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params=dict(step_size=50, gamma=0.9),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    verbose=0\n",
        ")\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: DELETE\n",
        "# Best hyperparameters for TabNet: {'cat_emb_dim': 4.0, 'gamma': 1.9625566327022135, 'lr': 0.028004677059855897, 'n_a': 12.0, 'n_d': 13.0, 'n_steps': 3.0}\n",
        "#Mean AUC after hyperparameter tuning: 0.6385634246774857"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZdspTz_G2D2"
      },
      "source": [
        "## Assignment 5. Please try combining all the above methods to push the model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftBi462fHGQn"
      },
      "source": [
        "Hint: Methods other than the above methods are also okay to use to improve model performance.\n",
        "\n",
        "Please avoid data leakage when conducting hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: CHECK SLIDES AND CHANGE FEATURE SELECTION\n",
        "# TODO: PROB BEST TO GO BACK TO THE INITIAL FEATURE SET WITH TWO FEATURES"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\", category=UserWarning, module='pytorch_tabnet')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection\n",
        "feat_recent_ESM = pd.concat([feat_current_ESM,feat_ImmediatePast_ESM ], axis=1)\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_recent_ESM, feat_baseline],axis=1)\n",
        "\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DEFINE FEATURE SELECTION METHOD and CLASSIFIER\n",
        "\n",
        "RANDOM_STATE = 42\n",
        "# classifier\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    use_label_encoder=False,  # Avoid warning about label encoder\n",
        ")\n",
        "\n",
        "# feature selection\n",
        "feature_selector = SelectFromModel(\n",
        "    estimator=estimator,\n",
        "    threshold='mean' # Or a different threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/100 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [34:13<00:00, 20.53s/trial, best loss: -0.6965046018176555]\n",
            "Best hyperparameters: {'colsample_bytree': 0.5032200778906365, 'early_stopping_rounds': 1, 'eval_metric': 0, 'eval_size': 2, 'gamma': 0.20811940534878576, 'learning_rate': 0.015023850109495487, 'max_depth': 8.0, 'min_child_weight': 9.0, 'n_estimators': 850.0, 'reg_alpha': 0.0029794289174290547, 'reg_lambda': 0.0498844150774424, 'scale_pos_weight': 1.3877883631383723, 'subsample': 0.7865463010975854}\n"
          ]
        }
      ],
      "source": [
        "# hyperparameter tuning\n",
        "from hyperopt import fmin, tpe, hp, Trials, STATUS_OK\n",
        "from hyperopt.pyll.base import scope\n",
        "\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "    estimator = EvXGBClassifier(\n",
        "        early_stopping_rounds=int(params['early_stopping_rounds']),\n",
        "        eval_metric=params['eval_metric'],\n",
        "        eval_size=params['eval_size'],\n",
        "        random_state=int(params['random_state']),\n",
        "        n_estimators=int(params['n_estimators']),\n",
        "        learning_rate=params['learning_rate'],\n",
        "        max_depth=int(params['max_depth']),\n",
        "        min_child_weight=int(params['min_child_weight']),\n",
        "        gamma=params['gamma'],\n",
        "        subsample=params['subsample'],\n",
        "        colsample_bytree=params['colsample_bytree'],\n",
        "        reg_alpha=params['reg_alpha'],\n",
        "        reg_lambda=params['reg_lambda'],\n",
        "        scale_pos_weight=params['scale_pos_weight'],\n",
        "        use_label_encoder=params['use_label_encoder']\n",
        "    )\n",
        "\n",
        "        \n",
        "    # embedded\n",
        "    feature_selector = SelectFromModel(\n",
        "        estimator=estimator,\n",
        "        threshold='mean' # Or a different threshold\n",
        "    )\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            smote = SMOTENC(\n",
        "                categorical_features=[X_train_scaled.shape[1]-1],\n",
        "                random_state=int(params['random_state'])\n",
        "            )\n",
        "        else:\n",
        "            smote = SMOTE(random_state=int(params['random_state']))\n",
        "        X_train_os, y_train_os = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        selector = feature_selector\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = estimator\n",
        "\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "\n",
        "space = {\n",
        "    'early_stopping_rounds': hp.choice('early_stopping_rounds', [10, 30, 50, 100]),\n",
        "    'eval_metric': hp.choice('eval_metric', ['rmse', 'mae', 'mape', 'logloss']),\n",
        "    'eval_size': hp.choice('eval_size', [0.1, 0.2, 0.25, 0.3]),\n",
        "    'random_state': 42  ,\n",
        "    'n_estimators': scope.int(hp.quniform('n_estimators', 100, 1000, 50)),\n",
        "    'learning_rate': hp.loguniform('learning_rate', -4.6, -0.5),  # ~0.01 to 0.6\n",
        "    'max_depth': scope.int(hp.quniform('max_depth', 3, 10, 1)),\n",
        "    'min_child_weight': scope.int(hp.quniform('min_child_weight', 1, 10, 1)),\n",
        "    'gamma': hp.uniform('gamma', 0, 0.5),\n",
        "    'subsample': hp.uniform('subsample', 0.5, 1.0),\n",
        "    'colsample_bytree': hp.uniform('colsample_bytree', 0.5, 1.0),\n",
        "    'reg_alpha': hp.loguniform('reg_alpha', -8, 2),     # 1e-8 to 100\n",
        "    'reg_lambda': hp.loguniform('reg_lambda', -8, 2),   # 1e-8 to 100\n",
        "    'scale_pos_weight': hp.uniform('scale_pos_weight', 1, 10),\n",
        "    'use_label_encoder': False\n",
        "}\n",
        "\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best hyperparameters: {'early_stopping_rounds': 2, 'eval_metric': 0, 'eval_size': 1}\n",
        "# Best hyperparameters: {'colsample_bytree': 0.5032200778906365, 'early_stopping_rounds': 1, 'eval_metric': 0, 'eval_size': 2, 'gamma': 0.20811940534878576, 'learning_rate': 0.015023850109495487, 'max_depth': 8.0, 'min_child_weight': 9.0, 'n_estimators': 850.0, 'reg_alpha': 0.0029794289174290547, 'reg_lambda': 0.0498844150774424, 'scale_pos_weight': 1.3877883631383723, 'subsample': 0.7865463010975854}\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6547414849301642\n",
            "Training completed for Fold_1 with AUC: 0.5607897596908586\n",
            "Training completed for Fold_2 with AUC: 0.5628772635814889\n",
            "Training completed for Fold_3 with AUC: 0.5625\n",
            "Training completed for Fold_4 with AUC: 0.5987065377473266\n",
            "0.5879230091899676\n"
          ]
        }
      ],
      "source": [
        "estimator = EvXGBClassifier(\n",
        "    early_stopping_rounds=30,\n",
        "    eval_metric='rmse',\n",
        "    eval_size=0.25,\n",
        "    random_state=42,\n",
        "    n_estimators=best['n_estimators'],\n",
        "    learning_rate=best['learning_rate'],\n",
        "    max_depth=int(best['max_depth']),\n",
        "    min_child_weight=int(best['min_child_weight']),\n",
        "    gamma=best['gamma'],\n",
        "    subsample=best['subsample'],\n",
        "    colsample_bytree=best['colsample_bytree'],\n",
        "    reg_alpha=best['reg_alpha'],\n",
        "    reg_lambda=best['reg_lambda'],\n",
        "    scale_pos_weight=best['scale_pos_weight'],\n",
        "    use_label_encoder=False\n",
        ")\n",
        "\n",
        "feature_selector = SelectFromModel(\n",
        "    estimator=estimator,\n",
        "    threshold='mean'  # Or a different threshold\n",
        ")\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[feature_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {},
      "outputs": [],
      "source": [
        "# set best parameter values\n",
        "estimator.set_params(\n",
        "    early_stopping_rounds=  50,\n",
        "    eval_metric= 'rmse',\n",
        "    eval_size= 0.2,\n",
        "    random_state= 42\n",
        ")\n",
        "\n",
        "feature_selector = SelectFromModel(\n",
        "    estimator=estimator,\n",
        "    threshold='mean'  # Or a different threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:48] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:48] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:51] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:51] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25-06-05 21:54:53] Training completed for Fold_0 with AUC: 0.6016417544719431\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:54] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:54] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:56] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:56] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25-06-05 21:54:58] Training completed for Fold_1 with AUC: 0.5930443183190436\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:59] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:54:59] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:01] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:01] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25-06-05 21:55:02] Training completed for Fold_2 with AUC: 0.5406255716114872\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:02] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:02] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:05] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:05] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25-06-05 21:55:06] Training completed for Fold_3 with AUC: 0.5619673693653285\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:06] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:06] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:08] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [21:55:08] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[25-06-05 21:55:10] Training completed for Fold_4 with AUC: 0.6212460779919319\n",
            "0.5837050183519469\n"
          ]
        }
      ],
      "source": [
        "# Run cross-validation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[feature_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(y)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "intro-machine-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

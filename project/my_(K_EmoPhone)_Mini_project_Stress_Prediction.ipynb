{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqjSsKeM3fhL"
      },
      "source": [
        "# CS565-DS522 IoT Data Science Mini Project for K-EmoPhone dataset\n",
        "*This material is a joint work of TAs from IC Lab at KAIST, including Panyu Zhang, Soowon Kang, and Woohyeok Choi. This work is licensed under CC BY-SA 4.0.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQ3qZRo52Wi"
      },
      "source": [
        "## Instruction\n",
        "In this mini-project, we will build a model to predict users' self-reported stress using extracted features from K-EmoPhone dataset. This material mainly refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) conducting indepedent reproducibility experiments on K-EmoPhone dataset. In order to save time, we provide the extracted features from the raw data instead of starting from scratch. Besides, traditional machine learning model is used considering limited number of labels and multimodality issue in the in-the-wild K-EmoPhone dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbgf8BGx5MsB"
      },
      "source": [
        "## Guidance\n",
        "\n",
        "1. Before running the code, please first download the extracted features from the following [link](https://drive.google.com/file/d/1HcyFvzWEzO21osyP5E8VpVmHROX1ew7q/view?usp=sharing).\n",
        "\n",
        "2. Please change your runtime type to T4-GPU or other runtime types with GPU available since later we may use GPU for\n",
        "xgboost execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4izzUnJ6hp"
      },
      "source": [
        "Install latest version of xgboost > 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDlc8HW-Kt4",
        "outputId": "c6d197a5-7b27-4c57-f371-dc2f6c053009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XnKkV_aTVZ_Z"
      },
      "outputs": [],
      "source": [
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import cloudpickle\n",
        "from datetime import datetime\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "import time\n",
        "from typing import Optional\n",
        "\n",
        "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
        "\n",
        "RANDOM_STATE =42\n",
        "\n",
        "\n",
        "def log(msg: any):\n",
        "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ignore warnings\n",
        "import warnings\n",
        "warnings.filterwarnings(\"ignore\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aF5NVMMUzBb"
      },
      "source": [
        "## 1.Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5n57OnHIOM"
      },
      "source": [
        "### 1.1. Mount to Your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9KU0Kq4WZs",
        "outputId": "a9ea32a9-c6b4-4be2-c90f-df5028d7fe7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import drive\\n\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "execution_count": 4,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# not relevant for local execution\n",
        "'''\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9u_nJzcIKXF"
      },
      "source": [
        "### 1.2. Load Extracted Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg5JSHftIP_f",
        "outputId": "a770ce09-541d-4a64-f0d8-f368a7ce0ba1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "#PATH = '/content/drive/MyDrive/IoT_Data_Science/Project/Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "PATH = './Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "\n",
        "X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffYa1CuFIx7Q"
      },
      "source": [
        "X is the extracted features and the feature extraction process refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) and the immediate past time window is set as 15 minutes. y is the array of labels while groups is the user ids.\n",
        "\n",
        "Please note that here y is binarized using theoretical threshold (if ESM stress > 0, binarize as 1, else 0, ESM label scale [-3, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0jkopjUl2h"
      },
      "source": [
        "Since features are already extracted, we do not need to work on preprocessing and feature extraction again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLIYf29UYES"
      },
      "source": [
        "## 2.Feature Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOgjC9HXo5cP"
      },
      "source": [
        "There exist multiple types of features. Please try different combinations of features to see if there is any model performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "PE57GSJPVOWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#The following code is designed for reordering the data\n",
        "#################################################\n",
        "# Create a DataFrame with user_id and datetime\n",
        "\n",
        "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "\n",
        "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "\n",
        "# Sort the DataFrame by datetime\n",
        "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Update groups and datetimes\n",
        "groups = df_merged['user_id'].to_numpy()\n",
        "datetimes = df_merged['datetime'].to_numpy()\n",
        "y = df_merged['label'].to_numpy()\n",
        "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "\n",
        "\n",
        "#Divide the features into different categories\n",
        "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]\n",
        "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]\n",
        "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]\n",
        "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]\n",
        "\n",
        "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
        "\n",
        "#################################################################################\n",
        "#Below are the available features\n",
        "#Divide the time window features into sensor/ESM self-report features\n",
        "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]] #Current sensor features (value right before label)\n",
        "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]] #Current ESM features (value right before label)\n",
        "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]] #Immediate past sensor features (in past 15 minutes before label)\n",
        "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  #Immediate past ESM features\n",
        "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]] #Today epoch sensor features\n",
        "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]] #Today epoch ESM features\n",
        "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]] #Yesterday sensor features\n",
        "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] #Yesterday ESM features\n",
        "\n",
        "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]\n",
        "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]\n",
        "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]\n",
        "################################################################################\n",
        "\n",
        "#Prepare the final feature set\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_baseline  ],axis=1)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "VIqbJ_A8JGUS",
        "outputId": "59928fd1-9982-4580-c4a1-25f4afbb070c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ESM#LastLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2619 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ESM#LastLabel\n",
              "0               0.0\n",
              "1               1.0\n",
              "2               1.0\n",
              "3               0.0\n",
              "4               0.0\n",
              "...             ...\n",
              "2614            0.0\n",
              "2615            0.0\n",
              "2616            0.0\n",
              "2617            1.0\n",
              "2618            0.0\n",
              "\n",
              "[2619 rows x 1 columns]"
            ]
          },
          "execution_count": 7,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat_current_ESM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPIZll5fXQld"
      },
      "source": [
        "## 3.Model Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sE6PaldpCNU"
      },
      "source": [
        "Here is the revised XGBoost Classifier. We will use random eval_size percent of training set data as evaluation set for early stoppping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "cxqMVtSVXTfH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier \n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import  train_test_split\n",
        "from typing import Union\n",
        "\n",
        "#Function for revised xgboost classifier\n",
        "class EvXGBClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Enhanced XGBClassifier with built-in validation set approach for early stopping.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_size=None,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        random_state=None,\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the custom XGBoost Classifier.\n",
        "\n",
        "        Args:\n",
        "            eval_size (float): The proportion of the dataset to include in the evaluation split.\n",
        "            eval_metric (str): The evaluation metric used for model training.\n",
        "            early_stopping_rounds (int): The number of rounds to stop training if hold-out metric doesn't improve.\n",
        "            random_state (int): Seed for the random number generator for reproducibility.\n",
        "            **kwargs: Additional arguments to be passed to the underlying XGBClassifier.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.eval_size = eval_size\n",
        "        self.eval_metric = eval_metric\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        # Initialize the XGBClassifier with specified arguments and GPU acceleration.\n",
        "        self.model = XGBClassifier(\n",
        "            random_state=self.random_state,\n",
        "            eval_metric=self.eval_metric,\n",
        "            early_stopping_rounds=self.early_stopping_rounds,\n",
        "            tree_method = \"hist\", device = \"cuda\", #Use gpu for acceleration\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\" Returns the feature importances from the fitted model. \"\"\"\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "    @property\n",
        "    def feature_names_in_(self):\n",
        "        \"\"\" Returns the feature names from the input dataset used for fitting. \"\"\"\n",
        "        return self.model.feature_names_in_\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the XGBoost model with optional early stopping using a validation set.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Training features.\n",
        "            y (np.ndarray): Target values.\n",
        "        \"\"\"\n",
        "        if self.eval_size:\n",
        "            # Split data for early stopping evaluation if eval_size is specified.\n",
        "            X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "                X, y, test_size=self.eval_size, random_state=self.random_state)\n",
        "            # Fit the model with early stopping.\n",
        "            self.model.fit(\n",
        "                X_train_sub, y_train_sub,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "        else:\n",
        "            # Fit the model without early stopping.\n",
        "            self.model.fit(X, y, verbose=False)\n",
        "\n",
        "        # Store the best iteration number for predictions.\n",
        "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the classes for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_yate5pYRg"
      },
      "source": [
        "The following is defined functions for model training and model evaluation (cross-validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "AoHTuB3qpsfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from dataclasses import dataclass\n",
        "from sklearn.preprocessing import OrdinalEncoder\n",
        "from imblearn.over_sampling import ADASYN\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    print(message)  # Simple logging to stdout or enhance as needed\n",
        "\n",
        "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate the model for a single fold.\n",
        "    Args:\n",
        "        dir_result (str): Directory to store results.\n",
        "        fold_name (str): Name of the fold for identification.\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        C_cat, C_num (array): Lists of categorical and numeric feature names.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if normalize:\n",
        "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "            # Standard scaler only applied to numeric data\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        if select:\n",
        "\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for i, s in enumerate(select):\n",
        "                C = np.asarray(X_train.columns)\n",
        "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
        "                C_sel = C[M]\n",
        "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
        "                C_num = C_num[np.isin(C_num, C_sel)]\n",
        "\n",
        "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "\n",
        "\n",
        "                X_train = pd.DataFrame(\n",
        "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "\n",
        "        if oversample:\n",
        "            # Encode categorical features if any\n",
        "            if len(C_cat) > 0:\n",
        "                encoder = OrdinalEncoder()\n",
        "                X_train[C_cat] = encoder.fit_transform(X_train[C_cat])\n",
        "\n",
        "            # Changed smote to ADASYN for better handling of imbalanced datasets\n",
        "            sampler = ADASYN(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        #Deafult average method for roc_auc_score is macro\n",
        "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle =True, random_state = 42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
        "\n",
        "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "    return futures\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOD1KJTup7il"
      },
      "source": [
        "Here, we define the feature selection method and classifier and execute the code. AUC-ROC is calculated as mean of macro AUC-ROC for all folds/users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqvkNsznrEDe",
        "outputId": "6d0b3b54-c64d-4d54-838c-e3dd6b6bdcb9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6085028179367802\n",
            "Training completed for Fold_1 with AUC: 0.566392947711629\n",
            "Training completed for Fold_2 with AUC: 0.5536583135174685\n",
            "Training completed for Fold_3 with AUC: 0.5370038125140166\n",
            "Training completed for Fold_4 with AUC: 0.6069667669846961\n",
            "0.5745049317329182\n"
          ]
        }
      ],
      "source": [
        "#Featur Selection, you may want to change the feature selection methods\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005\n",
        ")\n",
        "#Classifier\n",
        "#There could exist more parameters. Please search in your defined parameter\n",
        "#space for model performance improvement\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01,\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "BASELINE_SCORE = mean_auc\n",
        "previous_mean_auc = mean_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EZcuiA56Ls"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuOqHMwFzWHv"
      },
      "source": [
        "## Assignment 1. Improve the model performance using different types of feature combinations. (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kR-jSPN0i52"
      },
      "source": [
        " Hint: Currently we are only using feat_baseline. You may want to try other feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "id": "wD6HOgC_zuiq"
      },
      "outputs": [],
      "source": [
        "# Selecting features\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "feat_final = pd.concat([feat_baseline, feat_current_ESM,feat_today_ESM ], axis=1) \n",
        "\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6515069835824552\n",
            "Training completed for Fold_1 with AUC: 0.5798454292959787\n",
            "Training completed for Fold_2 with AUC: 0.5782970550576185\n",
            "Training completed for Fold_3 with AUC: 0.5772734918143081\n",
            "Training completed for Fold_4 with AUC: 0.6347569955817378\n",
            "0.6043359910664197\n",
            "Difference from baseline: 0.0298\n",
            "Difference from previous mean AUC: 0.0298\n"
          ]
        }
      ],
      "source": [
        "# Run model training and evaluation again with the selected features\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "\n",
        "print(f\"Difference from previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMEhtgwJz9IP"
      },
      "source": [
        "## Assignment 2. Please try different feature selection methods (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FCRh8eF0zud"
      },
      "source": [
        "Hint: Currently, we are using LASSO filter for feature selection. Please consider using embedded method as well(same model for both feature selection and model training). Besides, the threshold for LASSO filter may also affect the performance. **Sepcifically, there is a method called 'mean' which is using mean of feature importances of all features as threshold.** Please try both different feature selection methods and different thresholds for filtering features to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bwna8wpv3Dhj"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6511149228130361\n",
            "Training completed for Fold_1 with AUC: 0.6187658495350803\n",
            "Training completed for Fold_2 with AUC: 0.5821382842509603\n",
            "Training completed for Fold_3 with AUC: 0.5701390446288406\n",
            "Training completed for Fold_4 with AUC: 0.614706729845681\n",
            "0.6073729662147196\n",
            "Difference from baseline: 0.0329\n",
            "Difference from previous mean AUC: 0.0030\n"
          ]
        }
      ],
      "source": [
        "# Trying xgboost feature selection\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "xgb_selector = SelectFromModel(XGBClassifier(n_estimators=100, random_state=RANDOM_STATE))\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Difference from previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_FJZurk3ulY"
      },
      "source": [
        "## Assignment 3. Please try using hyperopt for model hyperparameter tuning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQcs-te34PK"
      },
      "source": [
        "Hint: Please be aware that for revised xgboost classifier EvXGBClassifier, there exist other parameters other than default XGBClassifier parameters such as eval_size.\n",
        "\n",
        "For hyperparameter tuning, we will use 20% of training set as validation set to avoid data leakage.\n",
        "\n",
        "If it is too timeconsuming to run the code in colab, please run the code locally and consider using [ray tune](https://docs.ray.io/en/latest/tune/index.html) if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8PyEHuv4R-L",
        "outputId": "ee08935d-a307-409f-ba56-901261230535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 50/50 [22:55<00:00, 27.51s/trial, best loss: -0.6926022284511587]\n",
            "Best hyperparameters: {'colsample_bytree': 0.7761731107644282, 'gamma': 0.38565216552801934, 'learning_rate': 0.007015335191520455, 'max_depth': 6.0, 'min_child_weight': 8.0, 'n_estimators': 450.0, 'reg_alpha': 0.06506803991327365, 'reg_lambda': 0.8834960461115632, 'subsample': 0.8805329501115369}\n"
          ]
        }
      ],
      "source": [
        "# Hyperparameter tuning using Hyperopt\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            encoder = OrdinalEncoder()\n",
        "            X_train_scaled[:, -1] = encoder.fit_transform(X_train_scaled[:, -1].reshape(-1, 1)).ravel()\n",
        "\n",
        "        adasyn = ADASYN(random_state=int(params['random_state']))\n",
        "\n",
        "        X_train_os, y_train_os = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        # embedded feature selection\n",
        "        estimator = EvXGBClassifier(\n",
        "            max_depth=int(params['max_depth']),\n",
        "            min_child_weight=int(params['min_child_weight']),\n",
        "            subsample=params['subsample'],\n",
        "            colsample_bytree=params['colsample_bytree'],\n",
        "            gamma=params['gamma'],\n",
        "            learning_rate=params['learning_rate'],\n",
        "            n_estimators=int(params['n_estimators']),\n",
        "            reg_lambda=params['reg_lambda'],\n",
        "            reg_alpha=params['reg_alpha'],\n",
        "            random_state=int(params['random_state']),\n",
        "            eval_metric='logloss',\n",
        "            eval_size=0.2,\n",
        "            early_stopping_rounds=10\n",
        "        )\n",
        "        \n",
        "        selector = xgb_selector\n",
        "\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = estimator\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "space = {\n",
        "    'max_depth':          hp.quniform('max_depth', 3, 10, 1),  # Integer between 3 and 10\n",
        "    'min_child_weight':   hp.quniform('min_child_weight', 1, 10, 1), # Integer between 1 and 10\n",
        "    'subsample':          hp.uniform('subsample', 0.6, 1.0),  # Float between 0.6 and 1.0\n",
        "    'colsample_bytree':   hp.uniform('colsample_bytree', 0.6, 1.0), # Float between 0.6 and 1.0\n",
        "    'gamma':              hp.uniform('gamma', 0, 0.5),      # Float between 0 and 0.5\n",
        "    'learning_rate':      hp.loguniform('learning_rate', -5, 0), # Float on a log scale (0.0067 to 1)\n",
        "    'n_estimators':       hp.quniform('n_estimators', 100, 1000, 50), # Integer between 100 and 1000, steps of 50\n",
        "    'reg_lambda':         hp.uniform('reg_lambda', 0, 1),     # Float between 0 and 1 (L2 regularization)\n",
        "    'reg_alpha':          hp.uniform('reg_alpha', 0, 0.5),    # Float between 0 and 0.5 (L1 regularization)\n",
        "    'random_state':       42 # Keeping random_state fixed\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {},
      "outputs": [],
      "source": [
        "# saving the best hyperparameters so I dont have to run hyperopt again\n",
        "best = {'colsample_bytree': 0.7761731107644282, 'gamma': 0.38565216552801934, 'learning_rate': 0.007015335191520455, 'max_depth': 6.0, 'min_child_weight': 8.0, 'n_estimators': 450.0, 'reg_alpha': 0.06506803991327365, 'reg_lambda': 0.8834960461115632, 'subsample': 0.8805329501115369}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6464714530752267\n",
            "Training completed for Fold_1 with AUC: 0.618421688201908\n",
            "Training completed for Fold_2 with AUC: 0.5362996158770805\n",
            "Training completed for Fold_3 with AUC: 0.5950255102040817\n",
            "Training completed for Fold_4 with AUC: 0.6094320291989498\n",
            "Final AUC after hyperparameter tuning: 0.6011300593114494\n",
            "Difference from baseline: 0.0266\n",
            "Improvement over previous mean AUC: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Run the final model with the best hyperparameters\n",
        "best_params = {\n",
        "    'max_depth': int(best['max_depth']),\n",
        "    'min_child_weight': int(best['min_child_weight']),\n",
        "    'subsample': best['subsample'],\n",
        "    'colsample_bytree': best['colsample_bytree'],\n",
        "    'gamma': best['gamma'],\n",
        "    'learning_rate': best['learning_rate'],\n",
        "    'n_estimators': int(best['n_estimators']),\n",
        "    'reg_lambda': best['reg_lambda'],\n",
        "    'reg_alpha': best['reg_alpha'],\n",
        "    'random_state': 42,\n",
        "}\n",
        "\n",
        "# Final model training with the best hyperparameters\n",
        "final_estimator = EvXGBClassifier(\n",
        "    **best_params,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "# Perform cross-validation with the best hyperparameters\n",
        "final_results = perform_cross_validation(\n",
        "    X, y, groups, final_estimator,\n",
        "    normalize=True, select=[xgb_selector], oversample=True, random_state=42\n",
        ")\n",
        "\n",
        "auc_values = [final_results[i].metrics['AUC'] for i in range(len(final_results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(\"Final AUC after hyperparameter tuning:\", mean_auc)\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Improvement over previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Trying different combinations for xgb"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection\n",
        "feat_final = pd.concat([ feat_current_ESM ,feat_today_ESM,feat_sleep,feat_time ],axis=1)\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6464714530752267\n",
            "Training completed for Fold_1 with AUC: 0.618421688201908\n",
            "Training completed for Fold_2 with AUC: 0.5362996158770805\n",
            "Training completed for Fold_3 with AUC: 0.5950255102040817\n",
            "Training completed for Fold_4 with AUC: 0.6094320291989498\n",
            "Final AUC: 0.6011300593114494\n",
            "Difference from baseline: 0.0266\n",
            "Improvement over previous mean AUC: 0.0000\n"
          ]
        }
      ],
      "source": [
        "\n",
        "xgb_classifier = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10\n",
        ")\n",
        "\n",
        "xgb_selector = SelectFromModel(\n",
        "    estimator=xgb_classifier,\n",
        "    threshold='mean'  # Select features with importance above the mean\n",
        ")\n",
        "\n",
        "# Perform cross-validation with the best hyperparameters\n",
        "final_results = perform_cross_validation(\n",
        "    X, y, groups, xgb_classifier,\n",
        "    normalize=True, select=[xgb_selector], oversample=True, random_state=42\n",
        ")\n",
        "auc_values = [final_results[i].metrics['AUC'] for i in range(len(final_results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(\"Final AUC:\", mean_auc)\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Improvement over previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPz966iD6yoH"
      },
      "source": [
        "## Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for **tabular data** to improve model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfbskLs7qmc"
      },
      "source": [
        "Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for **tabular data** and see if there is performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Defining a sklearn-compatible wrapper for TabNet\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "class SklearnTabNet(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = TabNetClassifier(**kwargs)\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y, **fit_params):\n",
        "        # Convert DataFrame to NumPy if needed\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        y = self.label_encoder.fit_transform(y)\n",
        "        self.model.fit(X, y, **fit_params)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        preds = self.model.predict(X)\n",
        "        return self.label_encoder.inverse_transform(preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        return self.model.predict_proba(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbE_mod6TUIn"
      },
      "source": [
        "You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDEeP0pkBsn6"
      },
      "source": [
        "Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "MZk2A2-I7nPd"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.94564 |  0:00:00s\n",
            "epoch 1  | loss: 0.75195 |  0:00:00s\n",
            "epoch 2  | loss: 0.74165 |  0:00:00s\n",
            "epoch 3  | loss: 0.70735 |  0:00:00s\n",
            "epoch 4  | loss: 0.69181 |  0:00:00s\n",
            "epoch 5  | loss: 0.69213 |  0:00:00s\n",
            "epoch 6  | loss: 0.68294 |  0:00:00s\n",
            "epoch 7  | loss: 0.68496 |  0:00:00s\n",
            "epoch 8  | loss: 0.68329 |  0:00:00s\n",
            "epoch 9  | loss: 0.67698 |  0:00:00s\n",
            "epoch 10 | loss: 0.6781  |  0:00:00s\n",
            "epoch 11 | loss: 0.67262 |  0:00:00s\n",
            "epoch 12 | loss: 0.67893 |  0:00:00s\n",
            "epoch 13 | loss: 0.67371 |  0:00:00s\n",
            "epoch 14 | loss: 0.67202 |  0:00:00s\n",
            "epoch 15 | loss: 0.6645  |  0:00:00s\n",
            "epoch 16 | loss: 0.66733 |  0:00:00s\n",
            "epoch 17 | loss: 0.67581 |  0:00:00s\n",
            "epoch 18 | loss: 0.66678 |  0:00:00s\n",
            "epoch 19 | loss: 0.67814 |  0:00:00s\n",
            "epoch 20 | loss: 0.67374 |  0:00:00s\n",
            "epoch 21 | loss: 0.67031 |  0:00:00s\n",
            "epoch 22 | loss: 0.67321 |  0:00:01s\n",
            "epoch 23 | loss: 0.67213 |  0:00:01s\n",
            "epoch 24 | loss: 0.67372 |  0:00:01s\n",
            "epoch 25 | loss: 0.66947 |  0:00:01s\n",
            "epoch 26 | loss: 0.67225 |  0:00:01s\n",
            "epoch 27 | loss: 0.66466 |  0:00:01s\n",
            "epoch 28 | loss: 0.66017 |  0:00:01s\n",
            "epoch 29 | loss: 0.66978 |  0:00:01s\n",
            "epoch 30 | loss: 0.66618 |  0:00:01s\n",
            "epoch 31 | loss: 0.66445 |  0:00:01s\n",
            "epoch 32 | loss: 0.66539 |  0:00:01s\n",
            "epoch 33 | loss: 0.67359 |  0:00:01s\n",
            "epoch 34 | loss: 0.66468 |  0:00:01s\n",
            "epoch 35 | loss: 0.6655  |  0:00:01s\n",
            "epoch 36 | loss: 0.67033 |  0:00:01s\n",
            "epoch 37 | loss: 0.67226 |  0:00:01s\n",
            "epoch 38 | loss: 0.66586 |  0:00:01s\n",
            "epoch 39 | loss: 0.66958 |  0:00:01s\n",
            "epoch 40 | loss: 0.66313 |  0:00:01s\n",
            "epoch 41 | loss: 0.66287 |  0:00:01s\n",
            "epoch 42 | loss: 0.66645 |  0:00:01s\n",
            "epoch 43 | loss: 0.66745 |  0:00:01s\n",
            "epoch 44 | loss: 0.66721 |  0:00:01s\n",
            "epoch 45 | loss: 0.66829 |  0:00:02s\n",
            "epoch 46 | loss: 0.67429 |  0:00:02s\n",
            "epoch 47 | loss: 0.66465 |  0:00:02s\n",
            "epoch 48 | loss: 0.66552 |  0:00:02s\n",
            "epoch 49 | loss: 0.6658  |  0:00:02s\n",
            "epoch 50 | loss: 0.66084 |  0:00:02s\n",
            "epoch 51 | loss: 0.6657  |  0:00:02s\n",
            "epoch 52 | loss: 0.66699 |  0:00:02s\n",
            "epoch 53 | loss: 0.66268 |  0:00:02s\n",
            "epoch 54 | loss: 0.66582 |  0:00:02s\n",
            "epoch 55 | loss: 0.67342 |  0:00:02s\n",
            "epoch 56 | loss: 0.67109 |  0:00:02s\n",
            "epoch 57 | loss: 0.66608 |  0:00:02s\n",
            "epoch 58 | loss: 0.66896 |  0:00:02s\n",
            "epoch 59 | loss: 0.668   |  0:00:02s\n",
            "epoch 60 | loss: 0.6659  |  0:00:02s\n",
            "epoch 61 | loss: 0.66992 |  0:00:02s\n",
            "epoch 62 | loss: 0.66731 |  0:00:02s\n",
            "epoch 63 | loss: 0.66756 |  0:00:02s\n",
            "epoch 64 | loss: 0.6676  |  0:00:02s\n",
            "epoch 65 | loss: 0.669   |  0:00:02s\n",
            "epoch 66 | loss: 0.66447 |  0:00:02s\n",
            "epoch 67 | loss: 0.66484 |  0:00:02s\n",
            "epoch 68 | loss: 0.66275 |  0:00:02s\n",
            "epoch 69 | loss: 0.66707 |  0:00:03s\n",
            "epoch 70 | loss: 0.66372 |  0:00:03s\n",
            "epoch 71 | loss: 0.66386 |  0:00:03s\n",
            "epoch 72 | loss: 0.66698 |  0:00:03s\n",
            "epoch 73 | loss: 0.66454 |  0:00:03s\n",
            "epoch 74 | loss: 0.66138 |  0:00:03s\n",
            "epoch 75 | loss: 0.66184 |  0:00:03s\n",
            "epoch 76 | loss: 0.66443 |  0:00:03s\n",
            "epoch 77 | loss: 0.66609 |  0:00:03s\n",
            "epoch 78 | loss: 0.66256 |  0:00:03s\n",
            "epoch 79 | loss: 0.66642 |  0:00:03s\n",
            "epoch 80 | loss: 0.66239 |  0:00:03s\n",
            "epoch 81 | loss: 0.66436 |  0:00:03s\n",
            "epoch 82 | loss: 0.66672 |  0:00:03s\n",
            "epoch 83 | loss: 0.66402 |  0:00:03s\n",
            "epoch 84 | loss: 0.66189 |  0:00:03s\n",
            "epoch 85 | loss: 0.66486 |  0:00:03s\n",
            "epoch 86 | loss: 0.66682 |  0:00:03s\n",
            "epoch 87 | loss: 0.66512 |  0:00:03s\n",
            "epoch 88 | loss: 0.66604 |  0:00:03s\n",
            "epoch 89 | loss: 0.66497 |  0:00:03s\n",
            "epoch 90 | loss: 0.66499 |  0:00:03s\n",
            "epoch 91 | loss: 0.66545 |  0:00:04s\n",
            "epoch 92 | loss: 0.66542 |  0:00:04s\n",
            "epoch 93 | loss: 0.66223 |  0:00:04s\n",
            "epoch 94 | loss: 0.6628  |  0:00:04s\n",
            "epoch 95 | loss: 0.6665  |  0:00:04s\n",
            "epoch 96 | loss: 0.66867 |  0:00:04s\n",
            "epoch 97 | loss: 0.66144 |  0:00:04s\n",
            "epoch 98 | loss: 0.66544 |  0:00:04s\n",
            "epoch 99 | loss: 0.66383 |  0:00:04s\n",
            "Training completed for Fold_0 with AUC: 0.6127297231070816\n",
            "epoch 0  | loss: 0.76262 |  0:00:00s\n",
            "epoch 1  | loss: 0.70596 |  0:00:00s\n",
            "epoch 2  | loss: 0.67551 |  0:00:00s\n",
            "epoch 3  | loss: 0.6889  |  0:00:00s\n",
            "epoch 4  | loss: 0.67666 |  0:00:00s\n",
            "epoch 5  | loss: 0.66018 |  0:00:00s\n",
            "epoch 6  | loss: 0.66241 |  0:00:00s\n",
            "epoch 7  | loss: 0.66514 |  0:00:00s\n",
            "epoch 8  | loss: 0.66512 |  0:00:00s\n",
            "epoch 9  | loss: 0.65556 |  0:00:00s\n",
            "epoch 10 | loss: 0.66133 |  0:00:00s\n",
            "epoch 11 | loss: 0.65458 |  0:00:00s\n",
            "epoch 12 | loss: 0.65773 |  0:00:00s\n",
            "epoch 13 | loss: 0.65599 |  0:00:00s\n",
            "epoch 14 | loss: 0.66161 |  0:00:00s\n",
            "epoch 15 | loss: 0.65465 |  0:00:00s\n",
            "epoch 16 | loss: 0.66117 |  0:00:00s\n",
            "epoch 17 | loss: 0.6617  |  0:00:00s\n",
            "epoch 18 | loss: 0.66366 |  0:00:00s\n",
            "epoch 19 | loss: 0.65704 |  0:00:01s\n",
            "epoch 20 | loss: 0.66181 |  0:00:01s\n",
            "epoch 21 | loss: 0.65831 |  0:00:01s\n",
            "epoch 22 | loss: 0.65917 |  0:00:01s\n",
            "epoch 23 | loss: 0.65345 |  0:00:01s\n",
            "epoch 24 | loss: 0.65568 |  0:00:01s\n",
            "epoch 25 | loss: 0.6524  |  0:00:01s\n",
            "epoch 26 | loss: 0.65869 |  0:00:01s\n",
            "epoch 27 | loss: 0.66175 |  0:00:01s\n",
            "epoch 28 | loss: 0.66268 |  0:00:01s\n",
            "epoch 29 | loss: 0.65965 |  0:00:01s\n",
            "epoch 30 | loss: 0.65765 |  0:00:01s\n",
            "epoch 31 | loss: 0.6593  |  0:00:01s\n",
            "epoch 32 | loss: 0.65989 |  0:00:01s\n",
            "epoch 33 | loss: 0.65969 |  0:00:01s\n",
            "epoch 34 | loss: 0.65098 |  0:00:01s\n",
            "epoch 35 | loss: 0.65971 |  0:00:01s\n",
            "epoch 36 | loss: 0.65555 |  0:00:01s\n",
            "epoch 37 | loss: 0.66276 |  0:00:01s\n",
            "epoch 38 | loss: 0.65951 |  0:00:01s\n",
            "epoch 39 | loss: 0.65457 |  0:00:01s\n",
            "epoch 40 | loss: 0.6579  |  0:00:02s\n",
            "epoch 41 | loss: 0.65255 |  0:00:02s\n",
            "epoch 42 | loss: 0.65971 |  0:00:02s\n",
            "epoch 43 | loss: 0.65982 |  0:00:02s\n",
            "epoch 44 | loss: 0.65992 |  0:00:02s\n",
            "epoch 45 | loss: 0.65429 |  0:00:02s\n",
            "epoch 46 | loss: 0.65412 |  0:00:02s\n",
            "epoch 47 | loss: 0.66115 |  0:00:02s\n",
            "epoch 48 | loss: 0.65343 |  0:00:02s\n",
            "epoch 49 | loss: 0.65646 |  0:00:02s\n",
            "epoch 50 | loss: 0.65465 |  0:00:02s\n",
            "epoch 51 | loss: 0.65796 |  0:00:02s\n",
            "epoch 52 | loss: 0.65749 |  0:00:02s\n",
            "epoch 53 | loss: 0.65689 |  0:00:02s\n",
            "epoch 54 | loss: 0.65426 |  0:00:02s\n",
            "epoch 55 | loss: 0.65339 |  0:00:02s\n",
            "epoch 56 | loss: 0.65342 |  0:00:02s\n",
            "epoch 57 | loss: 0.65736 |  0:00:02s\n",
            "epoch 58 | loss: 0.65617 |  0:00:02s\n",
            "epoch 59 | loss: 0.65626 |  0:00:02s\n",
            "epoch 60 | loss: 0.65711 |  0:00:02s\n",
            "epoch 61 | loss: 0.65564 |  0:00:02s\n",
            "epoch 62 | loss: 0.65647 |  0:00:02s\n",
            "epoch 63 | loss: 0.65459 |  0:00:03s\n",
            "epoch 64 | loss: 0.65926 |  0:00:03s\n",
            "epoch 65 | loss: 0.65825 |  0:00:03s\n",
            "epoch 66 | loss: 0.65925 |  0:00:03s\n",
            "epoch 67 | loss: 0.65101 |  0:00:03s\n",
            "epoch 68 | loss: 0.65472 |  0:00:03s\n",
            "epoch 69 | loss: 0.65571 |  0:00:03s\n",
            "epoch 70 | loss: 0.65182 |  0:00:03s\n",
            "epoch 71 | loss: 0.65832 |  0:00:03s\n",
            "epoch 72 | loss: 0.65906 |  0:00:03s\n",
            "epoch 73 | loss: 0.65768 |  0:00:03s\n",
            "epoch 74 | loss: 0.6557  |  0:00:03s\n",
            "epoch 75 | loss: 0.65945 |  0:00:03s\n",
            "epoch 76 | loss: 0.65853 |  0:00:03s\n",
            "epoch 77 | loss: 0.65873 |  0:00:03s\n",
            "epoch 78 | loss: 0.65649 |  0:00:03s\n",
            "epoch 79 | loss: 0.66237 |  0:00:03s\n",
            "epoch 80 | loss: 0.65053 |  0:00:03s\n",
            "epoch 81 | loss: 0.65729 |  0:00:03s\n",
            "epoch 82 | loss: 0.65252 |  0:00:03s\n",
            "epoch 83 | loss: 0.66293 |  0:00:03s\n",
            "epoch 84 | loss: 0.64981 |  0:00:03s\n",
            "epoch 85 | loss: 0.65552 |  0:00:03s\n",
            "epoch 86 | loss: 0.65359 |  0:00:04s\n",
            "epoch 87 | loss: 0.65951 |  0:00:04s\n",
            "epoch 88 | loss: 0.65949 |  0:00:04s\n",
            "epoch 89 | loss: 0.65763 |  0:00:04s\n",
            "epoch 90 | loss: 0.65826 |  0:00:04s\n",
            "epoch 91 | loss: 0.65047 |  0:00:04s\n",
            "epoch 92 | loss: 0.65849 |  0:00:04s\n",
            "epoch 93 | loss: 0.65462 |  0:00:04s\n",
            "epoch 94 | loss: 0.65411 |  0:00:04s\n",
            "epoch 95 | loss: 0.65961 |  0:00:04s\n",
            "epoch 96 | loss: 0.65753 |  0:00:04s\n",
            "epoch 97 | loss: 0.65229 |  0:00:04s\n",
            "epoch 98 | loss: 0.65487 |  0:00:04s\n",
            "epoch 99 | loss: 0.65314 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6237109044801353\n",
            "epoch 0  | loss: 0.79572 |  0:00:00s\n",
            "epoch 1  | loss: 0.7155  |  0:00:00s\n",
            "epoch 2  | loss: 0.68828 |  0:00:00s\n",
            "epoch 3  | loss: 0.68305 |  0:00:00s\n",
            "epoch 4  | loss: 0.68073 |  0:00:00s\n",
            "epoch 5  | loss: 0.68088 |  0:00:00s\n",
            "epoch 6  | loss: 0.67753 |  0:00:00s\n",
            "epoch 7  | loss: 0.67728 |  0:00:00s\n",
            "epoch 8  | loss: 0.67673 |  0:00:00s\n",
            "epoch 9  | loss: 0.67023 |  0:00:00s\n",
            "epoch 10 | loss: 0.67321 |  0:00:00s\n",
            "epoch 11 | loss: 0.67328 |  0:00:00s\n",
            "epoch 12 | loss: 0.6663  |  0:00:00s\n",
            "epoch 13 | loss: 0.66847 |  0:00:00s\n",
            "epoch 14 | loss: 0.66753 |  0:00:00s\n",
            "epoch 15 | loss: 0.66771 |  0:00:00s\n",
            "epoch 16 | loss: 0.66495 |  0:00:00s\n",
            "epoch 17 | loss: 0.66244 |  0:00:00s\n",
            "epoch 18 | loss: 0.66269 |  0:00:00s\n",
            "epoch 19 | loss: 0.65674 |  0:00:00s\n",
            "epoch 20 | loss: 0.6601  |  0:00:00s\n",
            "epoch 21 | loss: 0.65418 |  0:00:00s\n",
            "epoch 22 | loss: 0.65639 |  0:00:01s\n",
            "epoch 23 | loss: 0.65549 |  0:00:01s\n",
            "epoch 24 | loss: 0.66015 |  0:00:01s\n",
            "epoch 25 | loss: 0.65136 |  0:00:01s\n",
            "epoch 26 | loss: 0.6514  |  0:00:01s\n",
            "epoch 27 | loss: 0.65357 |  0:00:01s\n",
            "epoch 28 | loss: 0.65135 |  0:00:01s\n",
            "epoch 29 | loss: 0.64742 |  0:00:01s\n",
            "epoch 30 | loss: 0.63961 |  0:00:01s\n",
            "epoch 31 | loss: 0.64569 |  0:00:01s\n",
            "epoch 32 | loss: 0.64649 |  0:00:01s\n",
            "epoch 33 | loss: 0.6396  |  0:00:01s\n",
            "epoch 34 | loss: 0.64943 |  0:00:01s\n",
            "epoch 35 | loss: 0.64436 |  0:00:01s\n",
            "epoch 36 | loss: 0.65016 |  0:00:01s\n",
            "epoch 37 | loss: 0.64694 |  0:00:01s\n",
            "epoch 38 | loss: 0.64684 |  0:00:01s\n",
            "epoch 39 | loss: 0.64872 |  0:00:01s\n",
            "epoch 40 | loss: 0.64198 |  0:00:01s\n",
            "epoch 41 | loss: 0.64717 |  0:00:01s\n",
            "epoch 42 | loss: 0.64155 |  0:00:01s\n",
            "epoch 43 | loss: 0.64346 |  0:00:01s\n",
            "epoch 44 | loss: 0.64997 |  0:00:01s\n",
            "epoch 45 | loss: 0.64674 |  0:00:02s\n",
            "epoch 46 | loss: 0.64263 |  0:00:02s\n",
            "epoch 47 | loss: 0.63987 |  0:00:02s\n",
            "epoch 48 | loss: 0.65002 |  0:00:02s\n",
            "epoch 49 | loss: 0.6474  |  0:00:02s\n",
            "epoch 50 | loss: 0.6383  |  0:00:02s\n",
            "epoch 51 | loss: 0.64734 |  0:00:02s\n",
            "epoch 52 | loss: 0.64581 |  0:00:02s\n",
            "epoch 53 | loss: 0.64625 |  0:00:02s\n",
            "epoch 54 | loss: 0.64841 |  0:00:02s\n",
            "epoch 55 | loss: 0.64165 |  0:00:02s\n",
            "epoch 56 | loss: 0.6406  |  0:00:02s\n",
            "epoch 57 | loss: 0.63779 |  0:00:02s\n",
            "epoch 58 | loss: 0.63598 |  0:00:02s\n",
            "epoch 59 | loss: 0.6395  |  0:00:02s\n",
            "epoch 60 | loss: 0.63179 |  0:00:02s\n",
            "epoch 61 | loss: 0.62689 |  0:00:02s\n",
            "epoch 62 | loss: 0.63945 |  0:00:02s\n",
            "epoch 63 | loss: 0.63416 |  0:00:02s\n",
            "epoch 64 | loss: 0.6249  |  0:00:02s\n",
            "epoch 65 | loss: 0.63928 |  0:00:02s\n",
            "epoch 66 | loss: 0.63088 |  0:00:02s\n",
            "epoch 67 | loss: 0.62992 |  0:00:03s\n",
            "epoch 68 | loss: 0.63823 |  0:00:03s\n",
            "epoch 69 | loss: 0.6346  |  0:00:03s\n",
            "epoch 70 | loss: 0.63906 |  0:00:03s\n",
            "epoch 71 | loss: 0.63887 |  0:00:03s\n",
            "epoch 72 | loss: 0.63487 |  0:00:03s\n",
            "epoch 73 | loss: 0.62756 |  0:00:03s\n",
            "epoch 74 | loss: 0.63328 |  0:00:03s\n",
            "epoch 75 | loss: 0.62502 |  0:00:03s\n",
            "epoch 76 | loss: 0.63866 |  0:00:03s\n",
            "epoch 77 | loss: 0.63477 |  0:00:03s\n",
            "epoch 78 | loss: 0.62787 |  0:00:03s\n",
            "epoch 79 | loss: 0.6214  |  0:00:03s\n",
            "epoch 80 | loss: 0.62204 |  0:00:03s\n",
            "epoch 81 | loss: 0.62813 |  0:00:03s\n",
            "epoch 82 | loss: 0.6329  |  0:00:03s\n",
            "epoch 83 | loss: 0.62599 |  0:00:03s\n",
            "epoch 84 | loss: 0.62917 |  0:00:03s\n",
            "epoch 85 | loss: 0.62804 |  0:00:03s\n",
            "epoch 86 | loss: 0.6254  |  0:00:03s\n",
            "epoch 87 | loss: 0.63671 |  0:00:03s\n",
            "epoch 88 | loss: 0.62893 |  0:00:03s\n",
            "epoch 89 | loss: 0.62858 |  0:00:04s\n",
            "epoch 90 | loss: 0.635   |  0:00:04s\n",
            "epoch 91 | loss: 0.63101 |  0:00:04s\n",
            "epoch 92 | loss: 0.62974 |  0:00:04s\n",
            "epoch 93 | loss: 0.61811 |  0:00:04s\n",
            "epoch 94 | loss: 0.63086 |  0:00:04s\n",
            "epoch 95 | loss: 0.62479 |  0:00:04s\n",
            "epoch 96 | loss: 0.62767 |  0:00:04s\n",
            "epoch 97 | loss: 0.6272  |  0:00:04s\n",
            "epoch 98 | loss: 0.63117 |  0:00:04s\n",
            "epoch 99 | loss: 0.62389 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.6500365831351747\n",
            "epoch 0  | loss: 1.01135 |  0:00:00s\n",
            "epoch 1  | loss: 0.7492  |  0:00:00s\n",
            "epoch 2  | loss: 0.72223 |  0:00:00s\n",
            "epoch 3  | loss: 0.70269 |  0:00:00s\n",
            "epoch 4  | loss: 0.68013 |  0:00:00s\n",
            "epoch 5  | loss: 0.67597 |  0:00:00s\n",
            "epoch 6  | loss: 0.67429 |  0:00:00s\n",
            "epoch 7  | loss: 0.6714  |  0:00:00s\n",
            "epoch 8  | loss: 0.67299 |  0:00:00s\n",
            "epoch 9  | loss: 0.66777 |  0:00:00s\n",
            "epoch 10 | loss: 0.66772 |  0:00:00s\n",
            "epoch 11 | loss: 0.66514 |  0:00:00s\n",
            "epoch 12 | loss: 0.66238 |  0:00:00s\n",
            "epoch 13 | loss: 0.66092 |  0:00:00s\n",
            "epoch 14 | loss: 0.65496 |  0:00:00s\n",
            "epoch 15 | loss: 0.66236 |  0:00:00s\n",
            "epoch 16 | loss: 0.66055 |  0:00:00s\n",
            "epoch 17 | loss: 0.65755 |  0:00:00s\n",
            "epoch 18 | loss: 0.65704 |  0:00:00s\n",
            "epoch 19 | loss: 0.65745 |  0:00:00s\n",
            "epoch 20 | loss: 0.65799 |  0:00:00s\n",
            "epoch 21 | loss: 0.65661 |  0:00:00s\n",
            "epoch 22 | loss: 0.65659 |  0:00:01s\n",
            "epoch 23 | loss: 0.65457 |  0:00:01s\n",
            "epoch 24 | loss: 0.65334 |  0:00:01s\n",
            "epoch 25 | loss: 0.66585 |  0:00:01s\n",
            "epoch 26 | loss: 0.65687 |  0:00:01s\n",
            "epoch 27 | loss: 0.65517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66089 |  0:00:01s\n",
            "epoch 29 | loss: 0.65408 |  0:00:01s\n",
            "epoch 30 | loss: 0.65534 |  0:00:01s\n",
            "epoch 31 | loss: 0.65446 |  0:00:01s\n",
            "epoch 32 | loss: 0.65586 |  0:00:01s\n",
            "epoch 33 | loss: 0.65494 |  0:00:01s\n",
            "epoch 34 | loss: 0.6488  |  0:00:01s\n",
            "epoch 35 | loss: 0.65407 |  0:00:01s\n",
            "epoch 36 | loss: 0.65535 |  0:00:01s\n",
            "epoch 37 | loss: 0.65035 |  0:00:01s\n",
            "epoch 38 | loss: 0.65239 |  0:00:01s\n",
            "epoch 39 | loss: 0.65243 |  0:00:01s\n",
            "epoch 40 | loss: 0.65332 |  0:00:01s\n",
            "epoch 41 | loss: 0.65119 |  0:00:01s\n",
            "epoch 42 | loss: 0.65277 |  0:00:01s\n",
            "epoch 43 | loss: 0.6485  |  0:00:01s\n",
            "epoch 44 | loss: 0.6484  |  0:00:01s\n",
            "epoch 45 | loss: 0.64969 |  0:00:02s\n",
            "epoch 46 | loss: 0.65243 |  0:00:02s\n",
            "epoch 47 | loss: 0.65818 |  0:00:02s\n",
            "epoch 48 | loss: 0.65407 |  0:00:02s\n",
            "epoch 49 | loss: 0.65117 |  0:00:02s\n",
            "epoch 50 | loss: 0.64731 |  0:00:02s\n",
            "epoch 51 | loss: 0.65217 |  0:00:02s\n",
            "epoch 52 | loss: 0.64451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65382 |  0:00:02s\n",
            "epoch 54 | loss: 0.65336 |  0:00:02s\n",
            "epoch 55 | loss: 0.65046 |  0:00:02s\n",
            "epoch 56 | loss: 0.64916 |  0:00:02s\n",
            "epoch 57 | loss: 0.65029 |  0:00:02s\n",
            "epoch 58 | loss: 0.64894 |  0:00:02s\n",
            "epoch 59 | loss: 0.65058 |  0:00:02s\n",
            "epoch 60 | loss: 0.64799 |  0:00:02s\n",
            "epoch 61 | loss: 0.64739 |  0:00:02s\n",
            "epoch 62 | loss: 0.64808 |  0:00:02s\n",
            "epoch 63 | loss: 0.65268 |  0:00:02s\n",
            "epoch 64 | loss: 0.64901 |  0:00:02s\n",
            "epoch 65 | loss: 0.65303 |  0:00:02s\n",
            "epoch 66 | loss: 0.65483 |  0:00:02s\n",
            "epoch 67 | loss: 0.64465 |  0:00:02s\n",
            "epoch 68 | loss: 0.65127 |  0:00:03s\n",
            "epoch 69 | loss: 0.64651 |  0:00:03s\n",
            "epoch 70 | loss: 0.65507 |  0:00:03s\n",
            "epoch 71 | loss: 0.6415  |  0:00:03s\n",
            "epoch 72 | loss: 0.64616 |  0:00:03s\n",
            "epoch 73 | loss: 0.6496  |  0:00:03s\n",
            "epoch 74 | loss: 0.6496  |  0:00:03s\n",
            "epoch 75 | loss: 0.6508  |  0:00:03s\n",
            "epoch 76 | loss: 0.64427 |  0:00:03s\n",
            "epoch 77 | loss: 0.64147 |  0:00:03s\n",
            "epoch 78 | loss: 0.64537 |  0:00:03s\n",
            "epoch 79 | loss: 0.64672 |  0:00:03s\n",
            "epoch 80 | loss: 0.64724 |  0:00:03s\n",
            "epoch 81 | loss: 0.64849 |  0:00:03s\n",
            "epoch 82 | loss: 0.64666 |  0:00:03s\n",
            "epoch 83 | loss: 0.64377 |  0:00:03s\n",
            "epoch 84 | loss: 0.64425 |  0:00:03s\n",
            "epoch 85 | loss: 0.64077 |  0:00:03s\n",
            "epoch 86 | loss: 0.64927 |  0:00:03s\n",
            "epoch 87 | loss: 0.63928 |  0:00:03s\n",
            "epoch 88 | loss: 0.6458  |  0:00:03s\n",
            "epoch 89 | loss: 0.64565 |  0:00:03s\n",
            "epoch 90 | loss: 0.64475 |  0:00:03s\n",
            "epoch 91 | loss: 0.64383 |  0:00:04s\n",
            "epoch 92 | loss: 0.64176 |  0:00:04s\n",
            "epoch 93 | loss: 0.64961 |  0:00:04s\n",
            "epoch 94 | loss: 0.64352 |  0:00:04s\n",
            "epoch 95 | loss: 0.64463 |  0:00:04s\n",
            "epoch 96 | loss: 0.64736 |  0:00:04s\n",
            "epoch 97 | loss: 0.64046 |  0:00:04s\n",
            "epoch 98 | loss: 0.64965 |  0:00:04s\n",
            "epoch 99 | loss: 0.6476  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6076404462884054\n",
            "epoch 0  | loss: 1.11307 |  0:00:00s\n",
            "epoch 1  | loss: 0.77795 |  0:00:00s\n",
            "epoch 2  | loss: 0.72808 |  0:00:00s\n",
            "epoch 3  | loss: 0.73197 |  0:00:00s\n",
            "epoch 4  | loss: 0.69599 |  0:00:00s\n",
            "epoch 5  | loss: 0.68255 |  0:00:00s\n",
            "epoch 6  | loss: 0.68006 |  0:00:00s\n",
            "epoch 7  | loss: 0.67181 |  0:00:00s\n",
            "epoch 8  | loss: 0.67489 |  0:00:00s\n",
            "epoch 9  | loss: 0.67395 |  0:00:00s\n",
            "epoch 10 | loss: 0.67006 |  0:00:00s\n",
            "epoch 11 | loss: 0.67264 |  0:00:00s\n",
            "epoch 12 | loss: 0.67225 |  0:00:00s\n",
            "epoch 13 | loss: 0.67816 |  0:00:00s\n",
            "epoch 14 | loss: 0.67283 |  0:00:00s\n",
            "epoch 15 | loss: 0.66799 |  0:00:00s\n",
            "epoch 16 | loss: 0.66295 |  0:00:00s\n",
            "epoch 17 | loss: 0.66585 |  0:00:01s\n",
            "epoch 18 | loss: 0.65976 |  0:00:01s\n",
            "epoch 19 | loss: 0.67017 |  0:00:01s\n",
            "epoch 20 | loss: 0.66394 |  0:00:01s\n",
            "epoch 21 | loss: 0.66338 |  0:00:01s\n",
            "epoch 22 | loss: 0.66686 |  0:00:01s\n",
            "epoch 23 | loss: 0.66478 |  0:00:01s\n",
            "epoch 24 | loss: 0.66445 |  0:00:01s\n",
            "epoch 25 | loss: 0.6624  |  0:00:01s\n",
            "epoch 26 | loss: 0.65757 |  0:00:01s\n",
            "epoch 27 | loss: 0.66056 |  0:00:01s\n",
            "epoch 28 | loss: 0.65945 |  0:00:01s\n",
            "epoch 29 | loss: 0.65951 |  0:00:01s\n",
            "epoch 30 | loss: 0.6609  |  0:00:01s\n",
            "epoch 31 | loss: 0.65343 |  0:00:01s\n",
            "epoch 32 | loss: 0.65531 |  0:00:01s\n",
            "epoch 33 | loss: 0.66535 |  0:00:01s\n",
            "epoch 34 | loss: 0.65749 |  0:00:01s\n",
            "epoch 35 | loss: 0.65759 |  0:00:01s\n",
            "epoch 36 | loss: 0.65495 |  0:00:01s\n",
            "epoch 37 | loss: 0.65726 |  0:00:01s\n",
            "epoch 38 | loss: 0.65978 |  0:00:01s\n",
            "epoch 39 | loss: 0.6592  |  0:00:02s\n",
            "epoch 40 | loss: 0.657   |  0:00:02s\n",
            "epoch 41 | loss: 0.65827 |  0:00:02s\n",
            "epoch 42 | loss: 0.65713 |  0:00:02s\n",
            "epoch 43 | loss: 0.65603 |  0:00:02s\n",
            "epoch 44 | loss: 0.65391 |  0:00:02s\n",
            "epoch 45 | loss: 0.65392 |  0:00:02s\n",
            "epoch 46 | loss: 0.64854 |  0:00:02s\n",
            "epoch 47 | loss: 0.65514 |  0:00:02s\n",
            "epoch 48 | loss: 0.6493  |  0:00:02s\n",
            "epoch 49 | loss: 0.65214 |  0:00:02s\n",
            "epoch 50 | loss: 0.6485  |  0:00:02s\n",
            "epoch 51 | loss: 0.65383 |  0:00:02s\n",
            "epoch 52 | loss: 0.6559  |  0:00:02s\n",
            "epoch 53 | loss: 0.65519 |  0:00:02s\n",
            "epoch 54 | loss: 0.65219 |  0:00:02s\n",
            "epoch 55 | loss: 0.65004 |  0:00:02s\n",
            "epoch 56 | loss: 0.64604 |  0:00:02s\n",
            "epoch 57 | loss: 0.65232 |  0:00:02s\n",
            "epoch 58 | loss: 0.6463  |  0:00:03s\n",
            "epoch 59 | loss: 0.64845 |  0:00:03s\n",
            "epoch 60 | loss: 0.65117 |  0:00:03s\n",
            "epoch 61 | loss: 0.65793 |  0:00:03s\n",
            "epoch 62 | loss: 0.64809 |  0:00:03s\n",
            "epoch 63 | loss: 0.65311 |  0:00:03s\n",
            "epoch 64 | loss: 0.64579 |  0:00:03s\n",
            "epoch 65 | loss: 0.65086 |  0:00:03s\n",
            "epoch 66 | loss: 0.64656 |  0:00:03s\n",
            "epoch 67 | loss: 0.64561 |  0:00:03s\n",
            "epoch 68 | loss: 0.64285 |  0:00:03s\n",
            "epoch 69 | loss: 0.6459  |  0:00:03s\n",
            "epoch 70 | loss: 0.65078 |  0:00:03s\n",
            "epoch 71 | loss: 0.65501 |  0:00:03s\n",
            "epoch 72 | loss: 0.6523  |  0:00:03s\n",
            "epoch 73 | loss: 0.64628 |  0:00:03s\n",
            "epoch 74 | loss: 0.64715 |  0:00:03s\n",
            "epoch 75 | loss: 0.65391 |  0:00:03s\n",
            "epoch 76 | loss: 0.64611 |  0:00:03s\n",
            "epoch 77 | loss: 0.64867 |  0:00:03s\n",
            "epoch 78 | loss: 0.64451 |  0:00:03s\n",
            "epoch 79 | loss: 0.64396 |  0:00:04s\n",
            "epoch 80 | loss: 0.64358 |  0:00:04s\n",
            "epoch 81 | loss: 0.64865 |  0:00:04s\n",
            "epoch 82 | loss: 0.65023 |  0:00:04s\n",
            "epoch 83 | loss: 0.64649 |  0:00:04s\n",
            "epoch 84 | loss: 0.63901 |  0:00:04s\n",
            "epoch 85 | loss: 0.64505 |  0:00:04s\n",
            "epoch 86 | loss: 0.64881 |  0:00:04s\n",
            "epoch 87 | loss: 0.64666 |  0:00:04s\n",
            "epoch 88 | loss: 0.64082 |  0:00:04s\n",
            "epoch 89 | loss: 0.64109 |  0:00:04s\n",
            "epoch 90 | loss: 0.64345 |  0:00:04s\n",
            "epoch 91 | loss: 0.64278 |  0:00:04s\n",
            "epoch 92 | loss: 0.64975 |  0:00:04s\n",
            "epoch 93 | loss: 0.64627 |  0:00:04s\n",
            "epoch 94 | loss: 0.64295 |  0:00:04s\n",
            "epoch 95 | loss: 0.64204 |  0:00:04s\n",
            "epoch 96 | loss: 0.64513 |  0:00:04s\n",
            "epoch 97 | loss: 0.64912 |  0:00:04s\n",
            "epoch 98 | loss: 0.64815 |  0:00:04s\n",
            "epoch 99 | loss: 0.6456  |  0:00:05s\n",
            "Training completed for Fold_4 with AUC: 0.6608423512838573\n",
            "0.6309920016589309\n",
            "Difference from baseline: 0.0565\n",
            "Difference from previous mean AUC: 0.0299\n"
          ]
        }
      ],
      "source": [
        "# Running tabnet\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "import torch\n",
        "# import randomforest\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "tabnet_classifier = SklearnTabNet(\n",
        "    n_d=8,\n",
        "    n_a=8,\n",
        "    n_steps=3,\n",
        "    gamma=1.5,\n",
        "    n_independent=2,\n",
        "    n_shared=2,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    mask_type='sparsemax',\n",
        "    verbose=0,\n",
        "    seed=RANDOM_STATE\n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, tabnet_classifier, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Difference from previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hyperparameter optimization for TabNet\n",
        "from hyperopt import hp\n",
        "import torch\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            encoder = OrdinalEncoder()\n",
        "            X_train_scaled[:, -1] = encoder.fit_transform(X_train_scaled[:, -1].reshape(-1, 1)).ravel()\n",
        "\n",
        "        adasyn = ADASYN(random_state=int(params['random_state']))\n",
        "\n",
        "        X_train_os, y_train_os = adasyn.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        # embedded feature selection\n",
        "        estimator = SklearnTabNet(\n",
        "            n_d=params['n_d'],\n",
        "            n_a=params['n_a'],\n",
        "            n_steps=params['n_steps'],\n",
        "            gamma=params['gamma'],\n",
        "            n_independent=params['n_independent'],\n",
        "            n_shared=params['n_shared'],\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params={'lr': params['optimizer_params']['lr']},\n",
        "            mask_type=params['mask_type'],\n",
        "            seed=int(params['seed']),\n",
        "            verbose=0\n",
        "        )\n",
        "        \n",
        "        selector =  xgb_selector\n",
        "\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = estimator\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "tabnet_space = {\n",
        "    'n_d': hp.choice('n_d', [8, 16, 24, 32, 64]),\n",
        "    'n_a': hp.choice('n_a', [8, 16, 24, 32, 64]),\n",
        "    'n_steps': hp.choice('n_steps', [3, 4, 5, 6, 7]),\n",
        "    'gamma': hp.uniform('gamma', 1.0, 2.5),\n",
        "    'n_shared': hp.choice('n_shared', [1, 2, 3]),\n",
        "    'n_independent': hp.choice('n_independent', [1, 2, 3]),\n",
        "    'optimizer_fn': torch.optim.Adam,  # Keep fixed for simplicity\n",
        "    'optimizer_params': {\n",
        "        'lr': hp.loguniform('lr', np.log(1e-4), np.log(2e-2))  # 0.0001 to 0.02\n",
        "    },\n",
        "    'mask_type': hp.choice('mask_type', ['entmax', 'sparsemax']),\n",
        "    'seed': RANDOM_STATE,  # Fixed for reproducibility,\n",
        "    'random_state': RANDOM_STATE,\n",
        "    'max_epochs':10, # Optional: max epochs for training\n",
        "    'early_stopping_rounds': 10  # Optional: early stopping rounds\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best_tabnet_parameters = fmin(\n",
        "    fn=objective,\n",
        "    space=tabnet_space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=20,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best_tabnet_parameters)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {},
      "outputs": [],
      "source": [
        "# hard code the best parameters for TabNet so that I dont have to run hyperopt again\n",
        "best_tabnet_parameters = {'gamma': 1.392443662732613, 'lr': 0.01992926577113052, 'mask_type': 1, 'n_a': 2, 'n_d': 4, 'n_independent': 0, 'n_shared': 1, 'n_steps': 1}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.94564 |  0:00:00s\n",
            "epoch 1  | loss: 0.75195 |  0:00:00s\n",
            "epoch 2  | loss: 0.74165 |  0:00:00s\n",
            "epoch 3  | loss: 0.70735 |  0:00:00s\n",
            "epoch 4  | loss: 0.69181 |  0:00:00s\n",
            "epoch 5  | loss: 0.69213 |  0:00:00s\n",
            "epoch 6  | loss: 0.68294 |  0:00:00s\n",
            "epoch 7  | loss: 0.68496 |  0:00:00s\n",
            "epoch 8  | loss: 0.68329 |  0:00:00s\n",
            "epoch 9  | loss: 0.67698 |  0:00:00s\n",
            "epoch 10 | loss: 0.6781  |  0:00:00s\n",
            "epoch 11 | loss: 0.67262 |  0:00:00s\n",
            "epoch 12 | loss: 0.67893 |  0:00:00s\n",
            "epoch 13 | loss: 0.67371 |  0:00:00s\n",
            "epoch 14 | loss: 0.67202 |  0:00:00s\n",
            "epoch 15 | loss: 0.6645  |  0:00:00s\n",
            "epoch 16 | loss: 0.66733 |  0:00:00s\n",
            "epoch 17 | loss: 0.67581 |  0:00:00s\n",
            "epoch 18 | loss: 0.66678 |  0:00:00s\n",
            "epoch 19 | loss: 0.67814 |  0:00:00s\n",
            "epoch 20 | loss: 0.67374 |  0:00:00s\n",
            "epoch 21 | loss: 0.67031 |  0:00:00s\n",
            "epoch 22 | loss: 0.67321 |  0:00:00s\n",
            "epoch 23 | loss: 0.67213 |  0:00:01s\n",
            "epoch 24 | loss: 0.67372 |  0:00:01s\n",
            "epoch 25 | loss: 0.66947 |  0:00:01s\n",
            "epoch 26 | loss: 0.67225 |  0:00:01s\n",
            "epoch 27 | loss: 0.66466 |  0:00:01s\n",
            "epoch 28 | loss: 0.66017 |  0:00:01s\n",
            "epoch 29 | loss: 0.66978 |  0:00:01s\n",
            "epoch 30 | loss: 0.66618 |  0:00:01s\n",
            "epoch 31 | loss: 0.66445 |  0:00:01s\n",
            "epoch 32 | loss: 0.66539 |  0:00:01s\n",
            "epoch 33 | loss: 0.67359 |  0:00:01s\n",
            "epoch 34 | loss: 0.66468 |  0:00:01s\n",
            "epoch 35 | loss: 0.6655  |  0:00:01s\n",
            "epoch 36 | loss: 0.67033 |  0:00:01s\n",
            "epoch 37 | loss: 0.67226 |  0:00:01s\n",
            "epoch 38 | loss: 0.66586 |  0:00:01s\n",
            "epoch 39 | loss: 0.66958 |  0:00:01s\n",
            "epoch 40 | loss: 0.66313 |  0:00:01s\n",
            "epoch 41 | loss: 0.66287 |  0:00:01s\n",
            "epoch 42 | loss: 0.66645 |  0:00:02s\n",
            "epoch 43 | loss: 0.66745 |  0:00:02s\n",
            "epoch 44 | loss: 0.66721 |  0:00:02s\n",
            "epoch 45 | loss: 0.66829 |  0:00:02s\n",
            "epoch 46 | loss: 0.67429 |  0:00:02s\n",
            "epoch 47 | loss: 0.66465 |  0:00:02s\n",
            "epoch 48 | loss: 0.66552 |  0:00:02s\n",
            "epoch 49 | loss: 0.6658  |  0:00:02s\n",
            "epoch 50 | loss: 0.66084 |  0:00:02s\n",
            "epoch 51 | loss: 0.6657  |  0:00:02s\n",
            "epoch 52 | loss: 0.66699 |  0:00:02s\n",
            "epoch 53 | loss: 0.66268 |  0:00:02s\n",
            "epoch 54 | loss: 0.66582 |  0:00:02s\n",
            "epoch 55 | loss: 0.67342 |  0:00:02s\n",
            "epoch 56 | loss: 0.67109 |  0:00:02s\n",
            "epoch 57 | loss: 0.66608 |  0:00:02s\n",
            "epoch 58 | loss: 0.66896 |  0:00:02s\n",
            "epoch 59 | loss: 0.668   |  0:00:02s\n",
            "epoch 60 | loss: 0.6659  |  0:00:02s\n",
            "epoch 61 | loss: 0.66992 |  0:00:02s\n",
            "epoch 62 | loss: 0.66731 |  0:00:02s\n",
            "epoch 63 | loss: 0.66756 |  0:00:02s\n",
            "epoch 64 | loss: 0.6676  |  0:00:02s\n",
            "epoch 65 | loss: 0.669   |  0:00:03s\n",
            "epoch 66 | loss: 0.66447 |  0:00:03s\n",
            "epoch 67 | loss: 0.66484 |  0:00:03s\n",
            "epoch 68 | loss: 0.66275 |  0:00:03s\n",
            "epoch 69 | loss: 0.66707 |  0:00:03s\n",
            "epoch 70 | loss: 0.66372 |  0:00:03s\n",
            "epoch 71 | loss: 0.66386 |  0:00:03s\n",
            "epoch 72 | loss: 0.66698 |  0:00:03s\n",
            "epoch 73 | loss: 0.66454 |  0:00:03s\n",
            "epoch 74 | loss: 0.66138 |  0:00:03s\n",
            "epoch 75 | loss: 0.66184 |  0:00:03s\n",
            "epoch 76 | loss: 0.66443 |  0:00:03s\n",
            "epoch 77 | loss: 0.66609 |  0:00:03s\n",
            "epoch 78 | loss: 0.66256 |  0:00:03s\n",
            "epoch 79 | loss: 0.66642 |  0:00:03s\n",
            "epoch 80 | loss: 0.66239 |  0:00:03s\n",
            "epoch 81 | loss: 0.66436 |  0:00:03s\n",
            "epoch 82 | loss: 0.66672 |  0:00:03s\n",
            "epoch 83 | loss: 0.66402 |  0:00:03s\n",
            "epoch 84 | loss: 0.66189 |  0:00:03s\n",
            "epoch 85 | loss: 0.66486 |  0:00:03s\n",
            "epoch 86 | loss: 0.66682 |  0:00:03s\n",
            "epoch 87 | loss: 0.66512 |  0:00:03s\n",
            "epoch 88 | loss: 0.66604 |  0:00:04s\n",
            "epoch 89 | loss: 0.66497 |  0:00:04s\n",
            "epoch 90 | loss: 0.66499 |  0:00:04s\n",
            "epoch 91 | loss: 0.66545 |  0:00:04s\n",
            "epoch 92 | loss: 0.66542 |  0:00:04s\n",
            "epoch 93 | loss: 0.66223 |  0:00:04s\n",
            "epoch 94 | loss: 0.6628  |  0:00:04s\n",
            "epoch 95 | loss: 0.6665  |  0:00:04s\n",
            "epoch 96 | loss: 0.66867 |  0:00:04s\n",
            "epoch 97 | loss: 0.66144 |  0:00:04s\n",
            "epoch 98 | loss: 0.66544 |  0:00:04s\n",
            "epoch 99 | loss: 0.66383 |  0:00:04s\n",
            "Training completed for Fold_0 with AUC: 0.6127297231070816\n",
            "epoch 0  | loss: 0.76262 |  0:00:00s\n",
            "epoch 1  | loss: 0.70596 |  0:00:00s\n",
            "epoch 2  | loss: 0.67551 |  0:00:00s\n",
            "epoch 3  | loss: 0.6889  |  0:00:00s\n",
            "epoch 4  | loss: 0.67666 |  0:00:00s\n",
            "epoch 5  | loss: 0.66018 |  0:00:00s\n",
            "epoch 6  | loss: 0.66241 |  0:00:00s\n",
            "epoch 7  | loss: 0.66514 |  0:00:00s\n",
            "epoch 8  | loss: 0.66512 |  0:00:00s\n",
            "epoch 9  | loss: 0.65556 |  0:00:00s\n",
            "epoch 10 | loss: 0.66133 |  0:00:00s\n",
            "epoch 11 | loss: 0.65458 |  0:00:00s\n",
            "epoch 12 | loss: 0.65773 |  0:00:00s\n",
            "epoch 13 | loss: 0.65599 |  0:00:00s\n",
            "epoch 14 | loss: 0.66161 |  0:00:00s\n",
            "epoch 15 | loss: 0.65465 |  0:00:00s\n",
            "epoch 16 | loss: 0.66117 |  0:00:00s\n",
            "epoch 17 | loss: 0.6617  |  0:00:00s\n",
            "epoch 18 | loss: 0.66366 |  0:00:00s\n",
            "epoch 19 | loss: 0.65704 |  0:00:00s\n",
            "epoch 20 | loss: 0.66181 |  0:00:00s\n",
            "epoch 21 | loss: 0.65831 |  0:00:00s\n",
            "epoch 22 | loss: 0.65917 |  0:00:01s\n",
            "epoch 23 | loss: 0.65345 |  0:00:01s\n",
            "epoch 24 | loss: 0.65568 |  0:00:01s\n",
            "epoch 25 | loss: 0.6524  |  0:00:01s\n",
            "epoch 26 | loss: 0.65869 |  0:00:01s\n",
            "epoch 27 | loss: 0.66175 |  0:00:01s\n",
            "epoch 28 | loss: 0.66268 |  0:00:01s\n",
            "epoch 29 | loss: 0.65965 |  0:00:01s\n",
            "epoch 30 | loss: 0.65765 |  0:00:01s\n",
            "epoch 31 | loss: 0.6593  |  0:00:01s\n",
            "epoch 32 | loss: 0.65989 |  0:00:01s\n",
            "epoch 33 | loss: 0.65969 |  0:00:01s\n",
            "epoch 34 | loss: 0.65098 |  0:00:01s\n",
            "epoch 35 | loss: 0.65971 |  0:00:01s\n",
            "epoch 36 | loss: 0.65555 |  0:00:01s\n",
            "epoch 37 | loss: 0.66276 |  0:00:01s\n",
            "epoch 38 | loss: 0.65951 |  0:00:01s\n",
            "epoch 39 | loss: 0.65457 |  0:00:01s\n",
            "epoch 40 | loss: 0.6579  |  0:00:01s\n",
            "epoch 41 | loss: 0.65255 |  0:00:01s\n",
            "epoch 42 | loss: 0.65971 |  0:00:01s\n",
            "epoch 43 | loss: 0.65982 |  0:00:01s\n",
            "epoch 44 | loss: 0.65992 |  0:00:01s\n",
            "epoch 45 | loss: 0.65429 |  0:00:02s\n",
            "epoch 46 | loss: 0.65412 |  0:00:02s\n",
            "epoch 47 | loss: 0.66115 |  0:00:02s\n",
            "epoch 48 | loss: 0.65343 |  0:00:02s\n",
            "epoch 49 | loss: 0.65646 |  0:00:02s\n",
            "epoch 50 | loss: 0.65465 |  0:00:02s\n",
            "epoch 51 | loss: 0.65796 |  0:00:02s\n",
            "epoch 52 | loss: 0.65749 |  0:00:02s\n",
            "epoch 53 | loss: 0.65689 |  0:00:02s\n",
            "epoch 54 | loss: 0.65426 |  0:00:02s\n",
            "epoch 55 | loss: 0.65339 |  0:00:02s\n",
            "epoch 56 | loss: 0.65342 |  0:00:02s\n",
            "epoch 57 | loss: 0.65736 |  0:00:02s\n",
            "epoch 58 | loss: 0.65617 |  0:00:02s\n",
            "epoch 59 | loss: 0.65626 |  0:00:02s\n",
            "epoch 60 | loss: 0.65711 |  0:00:02s\n",
            "epoch 61 | loss: 0.65564 |  0:00:02s\n",
            "epoch 62 | loss: 0.65647 |  0:00:02s\n",
            "epoch 63 | loss: 0.65459 |  0:00:02s\n",
            "epoch 64 | loss: 0.65926 |  0:00:02s\n",
            "epoch 65 | loss: 0.65825 |  0:00:02s\n",
            "epoch 66 | loss: 0.65925 |  0:00:02s\n",
            "epoch 67 | loss: 0.65101 |  0:00:03s\n",
            "epoch 68 | loss: 0.65472 |  0:00:03s\n",
            "epoch 69 | loss: 0.65571 |  0:00:03s\n",
            "epoch 70 | loss: 0.65182 |  0:00:03s\n",
            "epoch 71 | loss: 0.65832 |  0:00:03s\n",
            "epoch 72 | loss: 0.65906 |  0:00:03s\n",
            "epoch 73 | loss: 0.65768 |  0:00:03s\n",
            "epoch 74 | loss: 0.6557  |  0:00:03s\n",
            "epoch 75 | loss: 0.65945 |  0:00:03s\n",
            "epoch 76 | loss: 0.65853 |  0:00:03s\n",
            "epoch 77 | loss: 0.65873 |  0:00:03s\n",
            "epoch 78 | loss: 0.65649 |  0:00:03s\n",
            "epoch 79 | loss: 0.66237 |  0:00:03s\n",
            "epoch 80 | loss: 0.65053 |  0:00:03s\n",
            "epoch 81 | loss: 0.65729 |  0:00:03s\n",
            "epoch 82 | loss: 0.65252 |  0:00:03s\n",
            "epoch 83 | loss: 0.66293 |  0:00:03s\n",
            "epoch 84 | loss: 0.64981 |  0:00:03s\n",
            "epoch 85 | loss: 0.65552 |  0:00:03s\n",
            "epoch 86 | loss: 0.65359 |  0:00:03s\n",
            "epoch 87 | loss: 0.65951 |  0:00:03s\n",
            "epoch 88 | loss: 0.65949 |  0:00:03s\n",
            "epoch 89 | loss: 0.65763 |  0:00:04s\n",
            "epoch 90 | loss: 0.65826 |  0:00:04s\n",
            "epoch 91 | loss: 0.65047 |  0:00:04s\n",
            "epoch 92 | loss: 0.65849 |  0:00:04s\n",
            "epoch 93 | loss: 0.65462 |  0:00:04s\n",
            "epoch 94 | loss: 0.65411 |  0:00:04s\n",
            "epoch 95 | loss: 0.65961 |  0:00:04s\n",
            "epoch 96 | loss: 0.65753 |  0:00:04s\n",
            "epoch 97 | loss: 0.65229 |  0:00:04s\n",
            "epoch 98 | loss: 0.65487 |  0:00:04s\n",
            "epoch 99 | loss: 0.65314 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6237109044801353\n",
            "epoch 0  | loss: 0.79572 |  0:00:00s\n",
            "epoch 1  | loss: 0.7155  |  0:00:00s\n",
            "epoch 2  | loss: 0.68828 |  0:00:00s\n",
            "epoch 3  | loss: 0.68305 |  0:00:00s\n",
            "epoch 4  | loss: 0.68073 |  0:00:00s\n",
            "epoch 5  | loss: 0.68088 |  0:00:00s\n",
            "epoch 6  | loss: 0.67753 |  0:00:00s\n",
            "epoch 7  | loss: 0.67728 |  0:00:00s\n",
            "epoch 8  | loss: 0.67673 |  0:00:00s\n",
            "epoch 9  | loss: 0.67023 |  0:00:00s\n",
            "epoch 10 | loss: 0.67321 |  0:00:00s\n",
            "epoch 11 | loss: 0.67328 |  0:00:00s\n",
            "epoch 12 | loss: 0.6663  |  0:00:00s\n",
            "epoch 13 | loss: 0.66847 |  0:00:00s\n",
            "epoch 14 | loss: 0.66753 |  0:00:00s\n",
            "epoch 15 | loss: 0.66771 |  0:00:00s\n",
            "epoch 16 | loss: 0.66495 |  0:00:00s\n",
            "epoch 17 | loss: 0.66244 |  0:00:00s\n",
            "epoch 18 | loss: 0.66269 |  0:00:00s\n",
            "epoch 19 | loss: 0.65674 |  0:00:00s\n",
            "epoch 20 | loss: 0.6601  |  0:00:00s\n",
            "epoch 21 | loss: 0.65418 |  0:00:00s\n",
            "epoch 22 | loss: 0.65639 |  0:00:01s\n",
            "epoch 23 | loss: 0.65549 |  0:00:01s\n",
            "epoch 24 | loss: 0.66015 |  0:00:01s\n",
            "epoch 25 | loss: 0.65136 |  0:00:01s\n",
            "epoch 26 | loss: 0.6514  |  0:00:01s\n",
            "epoch 27 | loss: 0.65357 |  0:00:01s\n",
            "epoch 28 | loss: 0.65135 |  0:00:01s\n",
            "epoch 29 | loss: 0.64742 |  0:00:01s\n",
            "epoch 30 | loss: 0.63961 |  0:00:01s\n",
            "epoch 31 | loss: 0.64569 |  0:00:01s\n",
            "epoch 32 | loss: 0.64649 |  0:00:01s\n",
            "epoch 33 | loss: 0.6396  |  0:00:01s\n",
            "epoch 34 | loss: 0.64943 |  0:00:01s\n",
            "epoch 35 | loss: 0.64436 |  0:00:01s\n",
            "epoch 36 | loss: 0.65016 |  0:00:01s\n",
            "epoch 37 | loss: 0.64694 |  0:00:01s\n",
            "epoch 38 | loss: 0.64684 |  0:00:01s\n",
            "epoch 39 | loss: 0.64872 |  0:00:01s\n",
            "epoch 40 | loss: 0.64198 |  0:00:02s\n",
            "epoch 41 | loss: 0.64717 |  0:00:02s\n",
            "epoch 42 | loss: 0.64155 |  0:00:02s\n",
            "epoch 43 | loss: 0.64346 |  0:00:02s\n",
            "epoch 44 | loss: 0.64997 |  0:00:02s\n",
            "epoch 45 | loss: 0.64674 |  0:00:02s\n",
            "epoch 46 | loss: 0.64263 |  0:00:02s\n",
            "epoch 47 | loss: 0.63987 |  0:00:02s\n",
            "epoch 48 | loss: 0.65002 |  0:00:02s\n",
            "epoch 49 | loss: 0.6474  |  0:00:02s\n",
            "epoch 50 | loss: 0.6383  |  0:00:02s\n",
            "epoch 51 | loss: 0.64734 |  0:00:02s\n",
            "epoch 52 | loss: 0.64581 |  0:00:02s\n",
            "epoch 53 | loss: 0.64625 |  0:00:02s\n",
            "epoch 54 | loss: 0.64841 |  0:00:02s\n",
            "epoch 55 | loss: 0.64165 |  0:00:02s\n",
            "epoch 56 | loss: 0.6406  |  0:00:02s\n",
            "epoch 57 | loss: 0.63779 |  0:00:02s\n",
            "epoch 58 | loss: 0.63598 |  0:00:02s\n",
            "epoch 59 | loss: 0.6395  |  0:00:02s\n",
            "epoch 60 | loss: 0.63179 |  0:00:02s\n",
            "epoch 61 | loss: 0.62689 |  0:00:03s\n",
            "epoch 62 | loss: 0.63945 |  0:00:03s\n",
            "epoch 63 | loss: 0.63416 |  0:00:03s\n",
            "epoch 64 | loss: 0.6249  |  0:00:03s\n",
            "epoch 65 | loss: 0.63928 |  0:00:03s\n",
            "epoch 66 | loss: 0.63088 |  0:00:03s\n",
            "epoch 67 | loss: 0.62992 |  0:00:03s\n",
            "epoch 68 | loss: 0.63823 |  0:00:03s\n",
            "epoch 69 | loss: 0.6346  |  0:00:03s\n",
            "epoch 70 | loss: 0.63906 |  0:00:03s\n",
            "epoch 71 | loss: 0.63887 |  0:00:03s\n",
            "epoch 72 | loss: 0.63487 |  0:00:03s\n",
            "epoch 73 | loss: 0.62756 |  0:00:03s\n",
            "epoch 74 | loss: 0.63328 |  0:00:03s\n",
            "epoch 75 | loss: 0.62502 |  0:00:03s\n",
            "epoch 76 | loss: 0.63866 |  0:00:03s\n",
            "epoch 77 | loss: 0.63477 |  0:00:03s\n",
            "epoch 78 | loss: 0.62787 |  0:00:03s\n",
            "epoch 79 | loss: 0.6214  |  0:00:03s\n",
            "epoch 80 | loss: 0.62204 |  0:00:03s\n",
            "epoch 81 | loss: 0.62813 |  0:00:03s\n",
            "epoch 82 | loss: 0.6329  |  0:00:03s\n",
            "epoch 83 | loss: 0.62599 |  0:00:04s\n",
            "epoch 84 | loss: 0.62917 |  0:00:04s\n",
            "epoch 85 | loss: 0.62804 |  0:00:04s\n",
            "epoch 86 | loss: 0.6254  |  0:00:04s\n",
            "epoch 87 | loss: 0.63671 |  0:00:04s\n",
            "epoch 88 | loss: 0.62893 |  0:00:04s\n",
            "epoch 89 | loss: 0.62858 |  0:00:04s\n",
            "epoch 90 | loss: 0.635   |  0:00:04s\n",
            "epoch 91 | loss: 0.63101 |  0:00:04s\n",
            "epoch 92 | loss: 0.62974 |  0:00:04s\n",
            "epoch 93 | loss: 0.61811 |  0:00:04s\n",
            "epoch 94 | loss: 0.63086 |  0:00:04s\n",
            "epoch 95 | loss: 0.62479 |  0:00:04s\n",
            "epoch 96 | loss: 0.62767 |  0:00:04s\n",
            "epoch 97 | loss: 0.6272  |  0:00:04s\n",
            "epoch 98 | loss: 0.63117 |  0:00:04s\n",
            "epoch 99 | loss: 0.62389 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.6500365831351747\n",
            "epoch 0  | loss: 1.01135 |  0:00:00s\n",
            "epoch 1  | loss: 0.7492  |  0:00:00s\n",
            "epoch 2  | loss: 0.72223 |  0:00:00s\n",
            "epoch 3  | loss: 0.70269 |  0:00:00s\n",
            "epoch 4  | loss: 0.68013 |  0:00:00s\n",
            "epoch 5  | loss: 0.67597 |  0:00:00s\n",
            "epoch 6  | loss: 0.67429 |  0:00:00s\n",
            "epoch 7  | loss: 0.6714  |  0:00:00s\n",
            "epoch 8  | loss: 0.67299 |  0:00:00s\n",
            "epoch 9  | loss: 0.66777 |  0:00:00s\n",
            "epoch 10 | loss: 0.66772 |  0:00:00s\n",
            "epoch 11 | loss: 0.66514 |  0:00:00s\n",
            "epoch 12 | loss: 0.66238 |  0:00:00s\n",
            "epoch 13 | loss: 0.66092 |  0:00:00s\n",
            "epoch 14 | loss: 0.65496 |  0:00:00s\n",
            "epoch 15 | loss: 0.66236 |  0:00:00s\n",
            "epoch 16 | loss: 0.66055 |  0:00:00s\n",
            "epoch 17 | loss: 0.65755 |  0:00:00s\n",
            "epoch 18 | loss: 0.65704 |  0:00:00s\n",
            "epoch 19 | loss: 0.65745 |  0:00:00s\n",
            "epoch 20 | loss: 0.65799 |  0:00:01s\n",
            "epoch 21 | loss: 0.65661 |  0:00:01s\n",
            "epoch 22 | loss: 0.65659 |  0:00:01s\n",
            "epoch 23 | loss: 0.65457 |  0:00:01s\n",
            "epoch 24 | loss: 0.65334 |  0:00:01s\n",
            "epoch 25 | loss: 0.66585 |  0:00:01s\n",
            "epoch 26 | loss: 0.65687 |  0:00:01s\n",
            "epoch 27 | loss: 0.65517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66089 |  0:00:01s\n",
            "epoch 29 | loss: 0.65408 |  0:00:01s\n",
            "epoch 30 | loss: 0.65534 |  0:00:01s\n",
            "epoch 31 | loss: 0.65446 |  0:00:01s\n",
            "epoch 32 | loss: 0.65586 |  0:00:01s\n",
            "epoch 33 | loss: 0.65494 |  0:00:01s\n",
            "epoch 34 | loss: 0.6488  |  0:00:01s\n",
            "epoch 35 | loss: 0.65407 |  0:00:01s\n",
            "epoch 36 | loss: 0.65535 |  0:00:01s\n",
            "epoch 37 | loss: 0.65035 |  0:00:01s\n",
            "epoch 38 | loss: 0.65239 |  0:00:01s\n",
            "epoch 39 | loss: 0.65243 |  0:00:01s\n",
            "epoch 40 | loss: 0.65332 |  0:00:01s\n",
            "epoch 41 | loss: 0.65119 |  0:00:01s\n",
            "epoch 42 | loss: 0.65277 |  0:00:01s\n",
            "epoch 43 | loss: 0.6485  |  0:00:02s\n",
            "epoch 44 | loss: 0.6484  |  0:00:02s\n",
            "epoch 45 | loss: 0.64969 |  0:00:02s\n",
            "epoch 46 | loss: 0.65243 |  0:00:02s\n",
            "epoch 47 | loss: 0.65818 |  0:00:02s\n",
            "epoch 48 | loss: 0.65407 |  0:00:02s\n",
            "epoch 49 | loss: 0.65117 |  0:00:02s\n",
            "epoch 50 | loss: 0.64731 |  0:00:02s\n",
            "epoch 51 | loss: 0.65217 |  0:00:02s\n",
            "epoch 52 | loss: 0.64451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65382 |  0:00:02s\n",
            "epoch 54 | loss: 0.65336 |  0:00:02s\n",
            "epoch 55 | loss: 0.65046 |  0:00:02s\n",
            "epoch 56 | loss: 0.64916 |  0:00:02s\n",
            "epoch 57 | loss: 0.65029 |  0:00:02s\n",
            "epoch 58 | loss: 0.64894 |  0:00:02s\n",
            "epoch 59 | loss: 0.65058 |  0:00:02s\n",
            "epoch 60 | loss: 0.64799 |  0:00:02s\n",
            "epoch 61 | loss: 0.64739 |  0:00:02s\n",
            "epoch 62 | loss: 0.64808 |  0:00:02s\n",
            "epoch 63 | loss: 0.65268 |  0:00:02s\n",
            "epoch 64 | loss: 0.64901 |  0:00:02s\n",
            "epoch 65 | loss: 0.65303 |  0:00:02s\n",
            "epoch 66 | loss: 0.65483 |  0:00:03s\n",
            "epoch 67 | loss: 0.64465 |  0:00:03s\n",
            "epoch 68 | loss: 0.65127 |  0:00:03s\n",
            "epoch 69 | loss: 0.64651 |  0:00:03s\n",
            "epoch 70 | loss: 0.65507 |  0:00:03s\n",
            "epoch 71 | loss: 0.6415  |  0:00:03s\n",
            "epoch 72 | loss: 0.64616 |  0:00:03s\n",
            "epoch 73 | loss: 0.6496  |  0:00:03s\n",
            "epoch 74 | loss: 0.6496  |  0:00:03s\n",
            "epoch 75 | loss: 0.6508  |  0:00:03s\n",
            "epoch 76 | loss: 0.64427 |  0:00:03s\n",
            "epoch 77 | loss: 0.64147 |  0:00:03s\n",
            "epoch 78 | loss: 0.64537 |  0:00:03s\n",
            "epoch 79 | loss: 0.64672 |  0:00:03s\n",
            "epoch 80 | loss: 0.64724 |  0:00:03s\n",
            "epoch 81 | loss: 0.64849 |  0:00:03s\n",
            "epoch 82 | loss: 0.64666 |  0:00:03s\n",
            "epoch 83 | loss: 0.64377 |  0:00:03s\n",
            "epoch 84 | loss: 0.64425 |  0:00:03s\n",
            "epoch 85 | loss: 0.64077 |  0:00:03s\n",
            "epoch 86 | loss: 0.64927 |  0:00:03s\n",
            "epoch 87 | loss: 0.63928 |  0:00:03s\n",
            "epoch 88 | loss: 0.6458  |  0:00:03s\n",
            "epoch 89 | loss: 0.64565 |  0:00:03s\n",
            "epoch 90 | loss: 0.64475 |  0:00:04s\n",
            "epoch 91 | loss: 0.64383 |  0:00:04s\n",
            "epoch 92 | loss: 0.64176 |  0:00:04s\n",
            "epoch 93 | loss: 0.64961 |  0:00:04s\n",
            "epoch 94 | loss: 0.64352 |  0:00:04s\n",
            "epoch 95 | loss: 0.64463 |  0:00:04s\n",
            "epoch 96 | loss: 0.64736 |  0:00:04s\n",
            "epoch 97 | loss: 0.64046 |  0:00:04s\n",
            "epoch 98 | loss: 0.64965 |  0:00:04s\n",
            "epoch 99 | loss: 0.6476  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6076404462884054\n",
            "epoch 0  | loss: 1.11307 |  0:00:00s\n",
            "epoch 1  | loss: 0.77795 |  0:00:00s\n",
            "epoch 2  | loss: 0.72808 |  0:00:00s\n",
            "epoch 3  | loss: 0.73197 |  0:00:00s\n",
            "epoch 4  | loss: 0.69599 |  0:00:00s\n",
            "epoch 5  | loss: 0.68255 |  0:00:00s\n",
            "epoch 6  | loss: 0.68006 |  0:00:00s\n",
            "epoch 7  | loss: 0.67181 |  0:00:00s\n",
            "epoch 8  | loss: 0.67489 |  0:00:00s\n",
            "epoch 9  | loss: 0.67395 |  0:00:00s\n",
            "epoch 10 | loss: 0.67006 |  0:00:00s\n",
            "epoch 11 | loss: 0.67264 |  0:00:00s\n",
            "epoch 12 | loss: 0.67225 |  0:00:00s\n",
            "epoch 13 | loss: 0.67816 |  0:00:00s\n",
            "epoch 14 | loss: 0.67283 |  0:00:00s\n",
            "epoch 15 | loss: 0.66799 |  0:00:00s\n",
            "epoch 16 | loss: 0.66295 |  0:00:00s\n",
            "epoch 17 | loss: 0.66585 |  0:00:00s\n",
            "epoch 18 | loss: 0.65976 |  0:00:00s\n",
            "epoch 19 | loss: 0.67017 |  0:00:01s\n",
            "epoch 20 | loss: 0.66394 |  0:00:01s\n",
            "epoch 21 | loss: 0.66338 |  0:00:01s\n",
            "epoch 22 | loss: 0.66686 |  0:00:01s\n",
            "epoch 23 | loss: 0.66478 |  0:00:01s\n",
            "epoch 24 | loss: 0.66445 |  0:00:01s\n",
            "epoch 25 | loss: 0.6624  |  0:00:01s\n",
            "epoch 26 | loss: 0.65757 |  0:00:01s\n",
            "epoch 27 | loss: 0.66056 |  0:00:01s\n",
            "epoch 28 | loss: 0.65945 |  0:00:01s\n",
            "epoch 29 | loss: 0.65951 |  0:00:01s\n",
            "epoch 30 | loss: 0.6609  |  0:00:01s\n",
            "epoch 31 | loss: 0.65343 |  0:00:01s\n",
            "epoch 32 | loss: 0.65531 |  0:00:01s\n",
            "epoch 33 | loss: 0.66535 |  0:00:01s\n",
            "epoch 34 | loss: 0.65749 |  0:00:01s\n",
            "epoch 35 | loss: 0.65759 |  0:00:01s\n",
            "epoch 36 | loss: 0.65495 |  0:00:01s\n",
            "epoch 37 | loss: 0.65726 |  0:00:01s\n",
            "epoch 38 | loss: 0.65978 |  0:00:01s\n",
            "epoch 39 | loss: 0.6592  |  0:00:01s\n",
            "epoch 40 | loss: 0.657   |  0:00:01s\n",
            "epoch 41 | loss: 0.65827 |  0:00:02s\n",
            "epoch 42 | loss: 0.65713 |  0:00:02s\n",
            "epoch 43 | loss: 0.65603 |  0:00:02s\n",
            "epoch 44 | loss: 0.65391 |  0:00:02s\n",
            "epoch 45 | loss: 0.65392 |  0:00:02s\n",
            "epoch 46 | loss: 0.64854 |  0:00:02s\n",
            "epoch 47 | loss: 0.65514 |  0:00:02s\n",
            "epoch 48 | loss: 0.6493  |  0:00:02s\n",
            "epoch 49 | loss: 0.65214 |  0:00:02s\n",
            "epoch 50 | loss: 0.6485  |  0:00:02s\n",
            "epoch 51 | loss: 0.65383 |  0:00:02s\n",
            "epoch 52 | loss: 0.6559  |  0:00:02s\n",
            "epoch 53 | loss: 0.65519 |  0:00:02s\n",
            "epoch 54 | loss: 0.65219 |  0:00:02s\n",
            "epoch 55 | loss: 0.65004 |  0:00:02s\n",
            "epoch 56 | loss: 0.64604 |  0:00:02s\n",
            "epoch 57 | loss: 0.65232 |  0:00:02s\n",
            "epoch 58 | loss: 0.6463  |  0:00:02s\n",
            "epoch 59 | loss: 0.64845 |  0:00:02s\n",
            "epoch 60 | loss: 0.65117 |  0:00:02s\n",
            "epoch 61 | loss: 0.65793 |  0:00:02s\n",
            "epoch 62 | loss: 0.64809 |  0:00:02s\n",
            "epoch 63 | loss: 0.65311 |  0:00:03s\n",
            "epoch 64 | loss: 0.64579 |  0:00:03s\n",
            "epoch 65 | loss: 0.65086 |  0:00:03s\n",
            "epoch 66 | loss: 0.64656 |  0:00:03s\n",
            "epoch 67 | loss: 0.64561 |  0:00:03s\n",
            "epoch 68 | loss: 0.64285 |  0:00:03s\n",
            "epoch 69 | loss: 0.6459  |  0:00:03s\n",
            "epoch 70 | loss: 0.65078 |  0:00:03s\n",
            "epoch 71 | loss: 0.65501 |  0:00:03s\n",
            "epoch 72 | loss: 0.6523  |  0:00:03s\n",
            "epoch 73 | loss: 0.64628 |  0:00:03s\n",
            "epoch 74 | loss: 0.64715 |  0:00:03s\n",
            "epoch 75 | loss: 0.65391 |  0:00:03s\n",
            "epoch 76 | loss: 0.64611 |  0:00:03s\n",
            "epoch 77 | loss: 0.64867 |  0:00:03s\n",
            "epoch 78 | loss: 0.64451 |  0:00:03s\n",
            "epoch 79 | loss: 0.64396 |  0:00:03s\n",
            "epoch 80 | loss: 0.64358 |  0:00:03s\n",
            "epoch 81 | loss: 0.64865 |  0:00:03s\n",
            "epoch 82 | loss: 0.65023 |  0:00:03s\n",
            "epoch 83 | loss: 0.64649 |  0:00:03s\n",
            "epoch 84 | loss: 0.63901 |  0:00:03s\n",
            "epoch 85 | loss: 0.64505 |  0:00:03s\n",
            "epoch 86 | loss: 0.64881 |  0:00:04s\n",
            "epoch 87 | loss: 0.64666 |  0:00:04s\n",
            "epoch 88 | loss: 0.64082 |  0:00:04s\n",
            "epoch 89 | loss: 0.64109 |  0:00:04s\n",
            "epoch 90 | loss: 0.64345 |  0:00:04s\n",
            "epoch 91 | loss: 0.64278 |  0:00:04s\n",
            "epoch 92 | loss: 0.64975 |  0:00:04s\n",
            "epoch 93 | loss: 0.64627 |  0:00:04s\n",
            "epoch 94 | loss: 0.64295 |  0:00:04s\n",
            "epoch 95 | loss: 0.64204 |  0:00:04s\n",
            "epoch 96 | loss: 0.64513 |  0:00:04s\n",
            "epoch 97 | loss: 0.64912 |  0:00:04s\n",
            "epoch 98 | loss: 0.64815 |  0:00:04s\n",
            "epoch 99 | loss: 0.6456  |  0:00:04s\n",
            "Training completed for Fold_4 with AUC: 0.6608423512838573\n",
            "0.6309920016589309\n",
            "Difference from baseline: 0.0565\n",
            "Difference from previous mean AUC: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Final model training with the best hyperparameters\n",
        "\n",
        "final_tabnet_classifier = SklearnTabNet(\n",
        "    n_d=8,\n",
        "    n_a=32,\n",
        "    n_steps=4,\n",
        "    gamma=best_tabnet_parameters['gamma'],\n",
        "    n_independent=1,\n",
        "    n_shared=3,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params={'lr': best_tabnet_parameters['lr']},\n",
        "    mask_type='entmax' if best_tabnet_parameters['mask_type'] == 0 else 'sparsemax',\n",
        "    seed=RANDOM_STATE,\n",
        "    verbose=0\n",
        ")\n",
        "results = perform_cross_validation(X, y, groups, final_tabnet_classifier, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Difference from previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZdspTz_G2D2"
      },
      "source": [
        "## Assignment 5. Please try combining all the above methods to push the model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftBi462fHGQn"
      },
      "source": [
        "Hint: Methods other than the above methods are also okay to use to improve model performance.\n",
        "\n",
        "Please avoid data leakage when conducting hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Feature selection\n",
        "feat_final = pd.concat([ feat_current_ESM ,feat_today_ESM,feat_sleep,feat_time ],axis=1)\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {},
      "outputs": [],
      "source": [
        "# defining soft voting ensemble wrapper\n",
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "\n",
        "class SoftVotingEnsemble(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, models, weights=None):\n",
        "        self.models = models\n",
        "        self.weights = weights if weights else [1.0] * len(models)\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        for model in self.models:\n",
        "            model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        probs = np.array([model.predict_proba(X) for model in self.models])\n",
        "        weighted_probs = np.average(probs, axis=0, weights=self.weights)\n",
        "        return weighted_probs\n",
        "\n",
        "    def predict(self, X):\n",
        "        return np.argmax(self.predict_proba(X), axis=1)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.94564 |  0:00:00s\n",
            "epoch 1  | loss: 0.75195 |  0:00:00s\n",
            "epoch 2  | loss: 0.74165 |  0:00:00s\n",
            "epoch 3  | loss: 0.70735 |  0:00:00s\n",
            "epoch 4  | loss: 0.69181 |  0:00:00s\n",
            "epoch 5  | loss: 0.69213 |  0:00:00s\n",
            "epoch 6  | loss: 0.68294 |  0:00:00s\n",
            "epoch 7  | loss: 0.68496 |  0:00:00s\n",
            "epoch 8  | loss: 0.68329 |  0:00:00s\n",
            "epoch 9  | loss: 0.67698 |  0:00:00s\n",
            "epoch 10 | loss: 0.6781  |  0:00:00s\n",
            "epoch 11 | loss: 0.67262 |  0:00:00s\n",
            "epoch 12 | loss: 0.67893 |  0:00:00s\n",
            "epoch 13 | loss: 0.67371 |  0:00:00s\n",
            "epoch 14 | loss: 0.67202 |  0:00:00s\n",
            "epoch 15 | loss: 0.6645  |  0:00:00s\n",
            "epoch 16 | loss: 0.66733 |  0:00:00s\n",
            "epoch 17 | loss: 0.67581 |  0:00:00s\n",
            "epoch 18 | loss: 0.66678 |  0:00:00s\n",
            "epoch 19 | loss: 0.67814 |  0:00:00s\n",
            "epoch 20 | loss: 0.67374 |  0:00:00s\n",
            "epoch 21 | loss: 0.67031 |  0:00:00s\n",
            "epoch 22 | loss: 0.67321 |  0:00:00s\n",
            "epoch 23 | loss: 0.67213 |  0:00:01s\n",
            "epoch 24 | loss: 0.67372 |  0:00:01s\n",
            "epoch 25 | loss: 0.66947 |  0:00:01s\n",
            "epoch 26 | loss: 0.67225 |  0:00:01s\n",
            "epoch 27 | loss: 0.66466 |  0:00:01s\n",
            "epoch 28 | loss: 0.66017 |  0:00:01s\n",
            "epoch 29 | loss: 0.66978 |  0:00:01s\n",
            "epoch 30 | loss: 0.66618 |  0:00:01s\n",
            "epoch 31 | loss: 0.66445 |  0:00:01s\n",
            "epoch 32 | loss: 0.66539 |  0:00:01s\n",
            "epoch 33 | loss: 0.67359 |  0:00:01s\n",
            "epoch 34 | loss: 0.66468 |  0:00:01s\n",
            "epoch 35 | loss: 0.6655  |  0:00:01s\n",
            "epoch 36 | loss: 0.67033 |  0:00:01s\n",
            "epoch 37 | loss: 0.67226 |  0:00:01s\n",
            "epoch 38 | loss: 0.66586 |  0:00:01s\n",
            "epoch 39 | loss: 0.66958 |  0:00:01s\n",
            "epoch 40 | loss: 0.66313 |  0:00:01s\n",
            "epoch 41 | loss: 0.66287 |  0:00:01s\n",
            "epoch 42 | loss: 0.66645 |  0:00:01s\n",
            "epoch 43 | loss: 0.66745 |  0:00:01s\n",
            "epoch 44 | loss: 0.66721 |  0:00:01s\n",
            "epoch 45 | loss: 0.66829 |  0:00:02s\n",
            "epoch 46 | loss: 0.67429 |  0:00:02s\n",
            "epoch 47 | loss: 0.66465 |  0:00:02s\n",
            "epoch 48 | loss: 0.66552 |  0:00:02s\n",
            "epoch 49 | loss: 0.6658  |  0:00:02s\n",
            "epoch 50 | loss: 0.66084 |  0:00:02s\n",
            "epoch 51 | loss: 0.6657  |  0:00:02s\n",
            "epoch 52 | loss: 0.66699 |  0:00:02s\n",
            "epoch 53 | loss: 0.66268 |  0:00:02s\n",
            "epoch 54 | loss: 0.66582 |  0:00:02s\n",
            "epoch 55 | loss: 0.67342 |  0:00:02s\n",
            "epoch 56 | loss: 0.67109 |  0:00:02s\n",
            "epoch 57 | loss: 0.66608 |  0:00:02s\n",
            "epoch 58 | loss: 0.66896 |  0:00:02s\n",
            "epoch 59 | loss: 0.668   |  0:00:02s\n",
            "epoch 60 | loss: 0.6659  |  0:00:02s\n",
            "epoch 61 | loss: 0.66992 |  0:00:02s\n",
            "epoch 62 | loss: 0.66731 |  0:00:02s\n",
            "epoch 63 | loss: 0.66756 |  0:00:02s\n",
            "epoch 64 | loss: 0.6676  |  0:00:02s\n",
            "epoch 65 | loss: 0.669   |  0:00:02s\n",
            "epoch 66 | loss: 0.66447 |  0:00:03s\n",
            "epoch 67 | loss: 0.66484 |  0:00:03s\n",
            "epoch 68 | loss: 0.66275 |  0:00:03s\n",
            "epoch 69 | loss: 0.66707 |  0:00:03s\n",
            "epoch 70 | loss: 0.66372 |  0:00:03s\n",
            "epoch 71 | loss: 0.66386 |  0:00:03s\n",
            "epoch 72 | loss: 0.66698 |  0:00:03s\n",
            "epoch 73 | loss: 0.66454 |  0:00:03s\n",
            "epoch 74 | loss: 0.66138 |  0:00:03s\n",
            "epoch 75 | loss: 0.66184 |  0:00:03s\n",
            "epoch 76 | loss: 0.66443 |  0:00:03s\n",
            "epoch 77 | loss: 0.66609 |  0:00:03s\n",
            "epoch 78 | loss: 0.66256 |  0:00:03s\n",
            "epoch 79 | loss: 0.66642 |  0:00:03s\n",
            "epoch 80 | loss: 0.66239 |  0:00:03s\n",
            "epoch 81 | loss: 0.66436 |  0:00:03s\n",
            "epoch 82 | loss: 0.66672 |  0:00:03s\n",
            "epoch 83 | loss: 0.66402 |  0:00:03s\n",
            "epoch 84 | loss: 0.66189 |  0:00:03s\n",
            "epoch 85 | loss: 0.66486 |  0:00:03s\n",
            "epoch 86 | loss: 0.66682 |  0:00:03s\n",
            "epoch 87 | loss: 0.66512 |  0:00:03s\n",
            "epoch 88 | loss: 0.66604 |  0:00:03s\n",
            "epoch 89 | loss: 0.66497 |  0:00:04s\n",
            "epoch 90 | loss: 0.66499 |  0:00:04s\n",
            "epoch 91 | loss: 0.66545 |  0:00:04s\n",
            "epoch 92 | loss: 0.66542 |  0:00:04s\n",
            "epoch 93 | loss: 0.66223 |  0:00:04s\n",
            "epoch 94 | loss: 0.6628  |  0:00:04s\n",
            "epoch 95 | loss: 0.6665  |  0:00:04s\n",
            "epoch 96 | loss: 0.66867 |  0:00:04s\n",
            "epoch 97 | loss: 0.66144 |  0:00:04s\n",
            "epoch 98 | loss: 0.66544 |  0:00:04s\n",
            "epoch 99 | loss: 0.66383 |  0:00:04s\n",
            "Training completed for Fold_0 with AUC: 0.6191129625091889\n",
            "epoch 0  | loss: 0.76262 |  0:00:00s\n",
            "epoch 1  | loss: 0.70596 |  0:00:00s\n",
            "epoch 2  | loss: 0.67551 |  0:00:00s\n",
            "epoch 3  | loss: 0.6889  |  0:00:00s\n",
            "epoch 4  | loss: 0.67666 |  0:00:00s\n",
            "epoch 5  | loss: 0.66018 |  0:00:00s\n",
            "epoch 6  | loss: 0.66241 |  0:00:00s\n",
            "epoch 7  | loss: 0.66514 |  0:00:00s\n",
            "epoch 8  | loss: 0.66512 |  0:00:00s\n",
            "epoch 9  | loss: 0.65556 |  0:00:00s\n",
            "epoch 10 | loss: 0.66133 |  0:00:00s\n",
            "epoch 11 | loss: 0.65458 |  0:00:00s\n",
            "epoch 12 | loss: 0.65773 |  0:00:00s\n",
            "epoch 13 | loss: 0.65599 |  0:00:00s\n",
            "epoch 14 | loss: 0.66161 |  0:00:00s\n",
            "epoch 15 | loss: 0.65465 |  0:00:00s\n",
            "epoch 16 | loss: 0.66117 |  0:00:00s\n",
            "epoch 17 | loss: 0.6617  |  0:00:00s\n",
            "epoch 18 | loss: 0.66366 |  0:00:00s\n",
            "epoch 19 | loss: 0.65704 |  0:00:00s\n",
            "epoch 20 | loss: 0.66181 |  0:00:00s\n",
            "epoch 21 | loss: 0.65831 |  0:00:00s\n",
            "epoch 22 | loss: 0.65917 |  0:00:00s\n",
            "epoch 23 | loss: 0.65345 |  0:00:01s\n",
            "epoch 24 | loss: 0.65568 |  0:00:01s\n",
            "epoch 25 | loss: 0.6524  |  0:00:01s\n",
            "epoch 26 | loss: 0.65869 |  0:00:01s\n",
            "epoch 27 | loss: 0.66175 |  0:00:01s\n",
            "epoch 28 | loss: 0.66268 |  0:00:01s\n",
            "epoch 29 | loss: 0.65965 |  0:00:01s\n",
            "epoch 30 | loss: 0.65765 |  0:00:01s\n",
            "epoch 31 | loss: 0.6593  |  0:00:01s\n",
            "epoch 32 | loss: 0.65989 |  0:00:01s\n",
            "epoch 33 | loss: 0.65969 |  0:00:01s\n",
            "epoch 34 | loss: 0.65098 |  0:00:01s\n",
            "epoch 35 | loss: 0.65971 |  0:00:01s\n",
            "epoch 36 | loss: 0.65555 |  0:00:01s\n",
            "epoch 37 | loss: 0.66276 |  0:00:01s\n",
            "epoch 38 | loss: 0.65951 |  0:00:01s\n",
            "epoch 39 | loss: 0.65457 |  0:00:01s\n",
            "epoch 40 | loss: 0.6579  |  0:00:01s\n",
            "epoch 41 | loss: 0.65255 |  0:00:01s\n",
            "epoch 42 | loss: 0.65971 |  0:00:01s\n",
            "epoch 43 | loss: 0.65982 |  0:00:01s\n",
            "epoch 44 | loss: 0.65992 |  0:00:02s\n",
            "epoch 45 | loss: 0.65429 |  0:00:02s\n",
            "epoch 46 | loss: 0.65412 |  0:00:02s\n",
            "epoch 47 | loss: 0.66115 |  0:00:02s\n",
            "epoch 48 | loss: 0.65343 |  0:00:02s\n",
            "epoch 49 | loss: 0.65646 |  0:00:02s\n",
            "epoch 50 | loss: 0.65465 |  0:00:02s\n",
            "epoch 51 | loss: 0.65796 |  0:00:02s\n",
            "epoch 52 | loss: 0.65749 |  0:00:02s\n",
            "epoch 53 | loss: 0.65689 |  0:00:02s\n",
            "epoch 54 | loss: 0.65426 |  0:00:02s\n",
            "epoch 55 | loss: 0.65339 |  0:00:02s\n",
            "epoch 56 | loss: 0.65342 |  0:00:02s\n",
            "epoch 57 | loss: 0.65736 |  0:00:02s\n",
            "epoch 58 | loss: 0.65617 |  0:00:02s\n",
            "epoch 59 | loss: 0.65626 |  0:00:02s\n",
            "epoch 60 | loss: 0.65711 |  0:00:02s\n",
            "epoch 61 | loss: 0.65564 |  0:00:02s\n",
            "epoch 62 | loss: 0.65647 |  0:00:02s\n",
            "epoch 63 | loss: 0.65459 |  0:00:02s\n",
            "epoch 64 | loss: 0.65926 |  0:00:02s\n",
            "epoch 65 | loss: 0.65825 |  0:00:03s\n",
            "epoch 66 | loss: 0.65925 |  0:00:03s\n",
            "epoch 67 | loss: 0.65101 |  0:00:03s\n",
            "epoch 68 | loss: 0.65472 |  0:00:03s\n",
            "epoch 69 | loss: 0.65571 |  0:00:03s\n",
            "epoch 70 | loss: 0.65182 |  0:00:03s\n",
            "epoch 71 | loss: 0.65832 |  0:00:03s\n",
            "epoch 72 | loss: 0.65906 |  0:00:03s\n",
            "epoch 73 | loss: 0.65768 |  0:00:03s\n",
            "epoch 74 | loss: 0.6557  |  0:00:03s\n",
            "epoch 75 | loss: 0.65945 |  0:00:03s\n",
            "epoch 76 | loss: 0.65853 |  0:00:03s\n",
            "epoch 77 | loss: 0.65873 |  0:00:03s\n",
            "epoch 78 | loss: 0.65649 |  0:00:03s\n",
            "epoch 79 | loss: 0.66237 |  0:00:03s\n",
            "epoch 80 | loss: 0.65053 |  0:00:03s\n",
            "epoch 81 | loss: 0.65729 |  0:00:03s\n",
            "epoch 82 | loss: 0.65252 |  0:00:03s\n",
            "epoch 83 | loss: 0.66293 |  0:00:03s\n",
            "epoch 84 | loss: 0.64981 |  0:00:03s\n",
            "epoch 85 | loss: 0.65552 |  0:00:03s\n",
            "epoch 86 | loss: 0.65359 |  0:00:03s\n",
            "epoch 87 | loss: 0.65951 |  0:00:03s\n",
            "epoch 88 | loss: 0.65949 |  0:00:04s\n",
            "epoch 89 | loss: 0.65763 |  0:00:04s\n",
            "epoch 90 | loss: 0.65826 |  0:00:04s\n",
            "epoch 91 | loss: 0.65047 |  0:00:04s\n",
            "epoch 92 | loss: 0.65849 |  0:00:04s\n",
            "epoch 93 | loss: 0.65462 |  0:00:04s\n",
            "epoch 94 | loss: 0.65411 |  0:00:04s\n",
            "epoch 95 | loss: 0.65961 |  0:00:04s\n",
            "epoch 96 | loss: 0.65753 |  0:00:04s\n",
            "epoch 97 | loss: 0.65229 |  0:00:04s\n",
            "epoch 98 | loss: 0.65487 |  0:00:04s\n",
            "epoch 99 | loss: 0.65314 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6214647989373264\n",
            "epoch 0  | loss: 0.79572 |  0:00:00s\n",
            "epoch 1  | loss: 0.7155  |  0:00:00s\n",
            "epoch 2  | loss: 0.68828 |  0:00:00s\n",
            "epoch 3  | loss: 0.68305 |  0:00:00s\n",
            "epoch 4  | loss: 0.68073 |  0:00:00s\n",
            "epoch 5  | loss: 0.68088 |  0:00:00s\n",
            "epoch 6  | loss: 0.67753 |  0:00:00s\n",
            "epoch 7  | loss: 0.67728 |  0:00:00s\n",
            "epoch 8  | loss: 0.67673 |  0:00:00s\n",
            "epoch 9  | loss: 0.67023 |  0:00:00s\n",
            "epoch 10 | loss: 0.67321 |  0:00:00s\n",
            "epoch 11 | loss: 0.67328 |  0:00:00s\n",
            "epoch 12 | loss: 0.6663  |  0:00:00s\n",
            "epoch 13 | loss: 0.66847 |  0:00:00s\n",
            "epoch 14 | loss: 0.66753 |  0:00:00s\n",
            "epoch 15 | loss: 0.66771 |  0:00:00s\n",
            "epoch 16 | loss: 0.66495 |  0:00:00s\n",
            "epoch 17 | loss: 0.66244 |  0:00:00s\n",
            "epoch 18 | loss: 0.66269 |  0:00:00s\n",
            "epoch 19 | loss: 0.65674 |  0:00:00s\n",
            "epoch 20 | loss: 0.6601  |  0:00:01s\n",
            "epoch 21 | loss: 0.65418 |  0:00:01s\n",
            "epoch 22 | loss: 0.65639 |  0:00:01s\n",
            "epoch 23 | loss: 0.65549 |  0:00:01s\n",
            "epoch 24 | loss: 0.66015 |  0:00:01s\n",
            "epoch 25 | loss: 0.65136 |  0:00:01s\n",
            "epoch 26 | loss: 0.6514  |  0:00:01s\n",
            "epoch 27 | loss: 0.65357 |  0:00:01s\n",
            "epoch 28 | loss: 0.65135 |  0:00:01s\n",
            "epoch 29 | loss: 0.64742 |  0:00:01s\n",
            "epoch 30 | loss: 0.63961 |  0:00:01s\n",
            "epoch 31 | loss: 0.64569 |  0:00:01s\n",
            "epoch 32 | loss: 0.64649 |  0:00:01s\n",
            "epoch 33 | loss: 0.6396  |  0:00:01s\n",
            "epoch 34 | loss: 0.64943 |  0:00:01s\n",
            "epoch 35 | loss: 0.64436 |  0:00:01s\n",
            "epoch 36 | loss: 0.65016 |  0:00:01s\n",
            "epoch 37 | loss: 0.64694 |  0:00:01s\n",
            "epoch 38 | loss: 0.64684 |  0:00:01s\n",
            "epoch 39 | loss: 0.64872 |  0:00:01s\n",
            "epoch 40 | loss: 0.64198 |  0:00:01s\n",
            "epoch 41 | loss: 0.64717 |  0:00:01s\n",
            "epoch 42 | loss: 0.64155 |  0:00:02s\n",
            "epoch 43 | loss: 0.64346 |  0:00:02s\n",
            "epoch 44 | loss: 0.64997 |  0:00:02s\n",
            "epoch 45 | loss: 0.64674 |  0:00:02s\n",
            "epoch 46 | loss: 0.64263 |  0:00:02s\n",
            "epoch 47 | loss: 0.63987 |  0:00:02s\n",
            "epoch 48 | loss: 0.65002 |  0:00:02s\n",
            "epoch 49 | loss: 0.6474  |  0:00:02s\n",
            "epoch 50 | loss: 0.6383  |  0:00:02s\n",
            "epoch 51 | loss: 0.64734 |  0:00:02s\n",
            "epoch 52 | loss: 0.64581 |  0:00:02s\n",
            "epoch 53 | loss: 0.64625 |  0:00:02s\n",
            "epoch 54 | loss: 0.64841 |  0:00:02s\n",
            "epoch 55 | loss: 0.64165 |  0:00:02s\n",
            "epoch 56 | loss: 0.6406  |  0:00:02s\n",
            "epoch 57 | loss: 0.63779 |  0:00:02s\n",
            "epoch 58 | loss: 0.63598 |  0:00:02s\n",
            "epoch 59 | loss: 0.6395  |  0:00:02s\n",
            "epoch 60 | loss: 0.63179 |  0:00:02s\n",
            "epoch 61 | loss: 0.62689 |  0:00:02s\n",
            "epoch 62 | loss: 0.63945 |  0:00:02s\n",
            "epoch 63 | loss: 0.63416 |  0:00:02s\n",
            "epoch 64 | loss: 0.6249  |  0:00:03s\n",
            "epoch 65 | loss: 0.63928 |  0:00:03s\n",
            "epoch 66 | loss: 0.63088 |  0:00:03s\n",
            "epoch 67 | loss: 0.62992 |  0:00:03s\n",
            "epoch 68 | loss: 0.63823 |  0:00:03s\n",
            "epoch 69 | loss: 0.6346  |  0:00:03s\n",
            "epoch 70 | loss: 0.63906 |  0:00:03s\n",
            "epoch 71 | loss: 0.63887 |  0:00:03s\n",
            "epoch 72 | loss: 0.63487 |  0:00:03s\n",
            "epoch 73 | loss: 0.62756 |  0:00:03s\n",
            "epoch 74 | loss: 0.63328 |  0:00:03s\n",
            "epoch 75 | loss: 0.62502 |  0:00:03s\n",
            "epoch 76 | loss: 0.63866 |  0:00:03s\n",
            "epoch 77 | loss: 0.63477 |  0:00:03s\n",
            "epoch 78 | loss: 0.62787 |  0:00:03s\n",
            "epoch 79 | loss: 0.6214  |  0:00:03s\n",
            "epoch 80 | loss: 0.62204 |  0:00:03s\n",
            "epoch 81 | loss: 0.62813 |  0:00:03s\n",
            "epoch 82 | loss: 0.6329  |  0:00:03s\n",
            "epoch 83 | loss: 0.62599 |  0:00:03s\n",
            "epoch 84 | loss: 0.62917 |  0:00:03s\n",
            "epoch 85 | loss: 0.62804 |  0:00:04s\n",
            "epoch 86 | loss: 0.6254  |  0:00:04s\n",
            "epoch 87 | loss: 0.63671 |  0:00:04s\n",
            "epoch 88 | loss: 0.62893 |  0:00:04s\n",
            "epoch 89 | loss: 0.62858 |  0:00:04s\n",
            "epoch 90 | loss: 0.635   |  0:00:04s\n",
            "epoch 91 | loss: 0.63101 |  0:00:04s\n",
            "epoch 92 | loss: 0.62974 |  0:00:04s\n",
            "epoch 93 | loss: 0.61811 |  0:00:04s\n",
            "epoch 94 | loss: 0.63086 |  0:00:04s\n",
            "epoch 95 | loss: 0.62479 |  0:00:04s\n",
            "epoch 96 | loss: 0.62767 |  0:00:04s\n",
            "epoch 97 | loss: 0.6272  |  0:00:04s\n",
            "epoch 98 | loss: 0.63117 |  0:00:04s\n",
            "epoch 99 | loss: 0.62389 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.6396103896103897\n",
            "epoch 0  | loss: 1.01135 |  0:00:00s\n",
            "epoch 1  | loss: 0.7492  |  0:00:00s\n",
            "epoch 2  | loss: 0.72223 |  0:00:00s\n",
            "epoch 3  | loss: 0.70269 |  0:00:00s\n",
            "epoch 4  | loss: 0.68013 |  0:00:00s\n",
            "epoch 5  | loss: 0.67597 |  0:00:00s\n",
            "epoch 6  | loss: 0.67429 |  0:00:00s\n",
            "epoch 7  | loss: 0.6714  |  0:00:00s\n",
            "epoch 8  | loss: 0.67299 |  0:00:00s\n",
            "epoch 9  | loss: 0.66777 |  0:00:00s\n",
            "epoch 10 | loss: 0.66772 |  0:00:00s\n",
            "epoch 11 | loss: 0.66514 |  0:00:00s\n",
            "epoch 12 | loss: 0.66238 |  0:00:00s\n",
            "epoch 13 | loss: 0.66092 |  0:00:00s\n",
            "epoch 14 | loss: 0.65496 |  0:00:00s\n",
            "epoch 15 | loss: 0.66236 |  0:00:00s\n",
            "epoch 16 | loss: 0.66055 |  0:00:00s\n",
            "epoch 17 | loss: 0.65755 |  0:00:00s\n",
            "epoch 18 | loss: 0.65704 |  0:00:00s\n",
            "epoch 19 | loss: 0.65745 |  0:00:00s\n",
            "epoch 20 | loss: 0.65799 |  0:00:00s\n",
            "epoch 21 | loss: 0.65661 |  0:00:01s\n",
            "epoch 22 | loss: 0.65659 |  0:00:01s\n",
            "epoch 23 | loss: 0.65457 |  0:00:01s\n",
            "epoch 24 | loss: 0.65334 |  0:00:01s\n",
            "epoch 25 | loss: 0.66585 |  0:00:01s\n",
            "epoch 26 | loss: 0.65687 |  0:00:01s\n",
            "epoch 27 | loss: 0.65517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66089 |  0:00:01s\n",
            "epoch 29 | loss: 0.65408 |  0:00:01s\n",
            "epoch 30 | loss: 0.65534 |  0:00:01s\n",
            "epoch 31 | loss: 0.65446 |  0:00:01s\n",
            "epoch 32 | loss: 0.65586 |  0:00:01s\n",
            "epoch 33 | loss: 0.65494 |  0:00:01s\n",
            "epoch 34 | loss: 0.6488  |  0:00:01s\n",
            "epoch 35 | loss: 0.65407 |  0:00:01s\n",
            "epoch 36 | loss: 0.65535 |  0:00:01s\n",
            "epoch 37 | loss: 0.65035 |  0:00:01s\n",
            "epoch 38 | loss: 0.65239 |  0:00:01s\n",
            "epoch 39 | loss: 0.65243 |  0:00:01s\n",
            "epoch 40 | loss: 0.65332 |  0:00:01s\n",
            "epoch 41 | loss: 0.65119 |  0:00:01s\n",
            "epoch 42 | loss: 0.65277 |  0:00:02s\n",
            "epoch 43 | loss: 0.6485  |  0:00:02s\n",
            "epoch 44 | loss: 0.6484  |  0:00:02s\n",
            "epoch 45 | loss: 0.64969 |  0:00:02s\n",
            "epoch 46 | loss: 0.65243 |  0:00:02s\n",
            "epoch 47 | loss: 0.65818 |  0:00:02s\n",
            "epoch 48 | loss: 0.65407 |  0:00:02s\n",
            "epoch 49 | loss: 0.65117 |  0:00:02s\n",
            "epoch 50 | loss: 0.64731 |  0:00:02s\n",
            "epoch 51 | loss: 0.65217 |  0:00:02s\n",
            "epoch 52 | loss: 0.64451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65382 |  0:00:02s\n",
            "epoch 54 | loss: 0.65336 |  0:00:02s\n",
            "epoch 55 | loss: 0.65046 |  0:00:02s\n",
            "epoch 56 | loss: 0.64916 |  0:00:02s\n",
            "epoch 57 | loss: 0.65029 |  0:00:02s\n",
            "epoch 58 | loss: 0.64894 |  0:00:02s\n",
            "epoch 59 | loss: 0.65058 |  0:00:02s\n",
            "epoch 60 | loss: 0.64799 |  0:00:02s\n",
            "epoch 61 | loss: 0.64739 |  0:00:02s\n",
            "epoch 62 | loss: 0.64808 |  0:00:02s\n",
            "epoch 63 | loss: 0.65268 |  0:00:02s\n",
            "epoch 64 | loss: 0.64901 |  0:00:03s\n",
            "epoch 65 | loss: 0.65303 |  0:00:03s\n",
            "epoch 66 | loss: 0.65483 |  0:00:03s\n",
            "epoch 67 | loss: 0.64465 |  0:00:03s\n",
            "epoch 68 | loss: 0.65127 |  0:00:03s\n",
            "epoch 69 | loss: 0.64651 |  0:00:03s\n",
            "epoch 70 | loss: 0.65507 |  0:00:03s\n",
            "epoch 71 | loss: 0.6415  |  0:00:03s\n",
            "epoch 72 | loss: 0.64616 |  0:00:03s\n",
            "epoch 73 | loss: 0.6496  |  0:00:03s\n",
            "epoch 74 | loss: 0.6496  |  0:00:03s\n",
            "epoch 75 | loss: 0.6508  |  0:00:03s\n",
            "epoch 76 | loss: 0.64427 |  0:00:03s\n",
            "epoch 77 | loss: 0.64147 |  0:00:03s\n",
            "epoch 78 | loss: 0.64537 |  0:00:03s\n",
            "epoch 79 | loss: 0.64672 |  0:00:03s\n",
            "epoch 80 | loss: 0.64724 |  0:00:03s\n",
            "epoch 81 | loss: 0.64849 |  0:00:03s\n",
            "epoch 82 | loss: 0.64666 |  0:00:03s\n",
            "epoch 83 | loss: 0.64377 |  0:00:03s\n",
            "epoch 84 | loss: 0.64425 |  0:00:03s\n",
            "epoch 85 | loss: 0.64077 |  0:00:03s\n",
            "epoch 86 | loss: 0.64927 |  0:00:04s\n",
            "epoch 87 | loss: 0.63928 |  0:00:04s\n",
            "epoch 88 | loss: 0.6458  |  0:00:04s\n",
            "epoch 89 | loss: 0.64565 |  0:00:04s\n",
            "epoch 90 | loss: 0.64475 |  0:00:04s\n",
            "epoch 91 | loss: 0.64383 |  0:00:04s\n",
            "epoch 92 | loss: 0.64176 |  0:00:04s\n",
            "epoch 93 | loss: 0.64961 |  0:00:04s\n",
            "epoch 94 | loss: 0.64352 |  0:00:04s\n",
            "epoch 95 | loss: 0.64463 |  0:00:04s\n",
            "epoch 96 | loss: 0.64736 |  0:00:04s\n",
            "epoch 97 | loss: 0.64046 |  0:00:04s\n",
            "epoch 98 | loss: 0.64965 |  0:00:04s\n",
            "epoch 99 | loss: 0.6476  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6087197241533976\n",
            "epoch 0  | loss: 1.11307 |  0:00:00s\n",
            "epoch 1  | loss: 0.77795 |  0:00:00s\n",
            "epoch 2  | loss: 0.72808 |  0:00:00s\n",
            "epoch 3  | loss: 0.73197 |  0:00:00s\n",
            "epoch 4  | loss: 0.69599 |  0:00:00s\n",
            "epoch 5  | loss: 0.68255 |  0:00:00s\n",
            "epoch 6  | loss: 0.68006 |  0:00:00s\n",
            "epoch 7  | loss: 0.67181 |  0:00:00s\n",
            "epoch 8  | loss: 0.67489 |  0:00:00s\n",
            "epoch 9  | loss: 0.67395 |  0:00:00s\n",
            "epoch 10 | loss: 0.67006 |  0:00:00s\n",
            "epoch 11 | loss: 0.67264 |  0:00:00s\n",
            "epoch 12 | loss: 0.67225 |  0:00:00s\n",
            "epoch 13 | loss: 0.67816 |  0:00:00s\n",
            "epoch 14 | loss: 0.67283 |  0:00:00s\n",
            "epoch 15 | loss: 0.66799 |  0:00:00s\n",
            "epoch 16 | loss: 0.66295 |  0:00:00s\n",
            "epoch 17 | loss: 0.66585 |  0:00:00s\n",
            "epoch 18 | loss: 0.65976 |  0:00:00s\n",
            "epoch 19 | loss: 0.67017 |  0:00:00s\n",
            "epoch 20 | loss: 0.66394 |  0:00:01s\n",
            "epoch 21 | loss: 0.66338 |  0:00:01s\n",
            "epoch 22 | loss: 0.66686 |  0:00:01s\n",
            "epoch 23 | loss: 0.66478 |  0:00:01s\n",
            "epoch 24 | loss: 0.66445 |  0:00:01s\n",
            "epoch 25 | loss: 0.6624  |  0:00:01s\n",
            "epoch 26 | loss: 0.65757 |  0:00:01s\n",
            "epoch 27 | loss: 0.66056 |  0:00:01s\n",
            "epoch 28 | loss: 0.65945 |  0:00:01s\n",
            "epoch 29 | loss: 0.65951 |  0:00:01s\n",
            "epoch 30 | loss: 0.6609  |  0:00:01s\n",
            "epoch 31 | loss: 0.65343 |  0:00:01s\n",
            "epoch 32 | loss: 0.65531 |  0:00:01s\n",
            "epoch 33 | loss: 0.66535 |  0:00:01s\n",
            "epoch 34 | loss: 0.65749 |  0:00:01s\n",
            "epoch 35 | loss: 0.65759 |  0:00:01s\n",
            "epoch 36 | loss: 0.65495 |  0:00:01s\n",
            "epoch 37 | loss: 0.65726 |  0:00:01s\n",
            "epoch 38 | loss: 0.65978 |  0:00:01s\n",
            "epoch 39 | loss: 0.6592  |  0:00:01s\n",
            "epoch 40 | loss: 0.657   |  0:00:02s\n",
            "epoch 41 | loss: 0.65827 |  0:00:02s\n",
            "epoch 42 | loss: 0.65713 |  0:00:02s\n",
            "epoch 43 | loss: 0.65603 |  0:00:02s\n",
            "epoch 44 | loss: 0.65391 |  0:00:02s\n",
            "epoch 45 | loss: 0.65392 |  0:00:02s\n",
            "epoch 46 | loss: 0.64854 |  0:00:02s\n",
            "epoch 47 | loss: 0.65514 |  0:00:02s\n",
            "epoch 48 | loss: 0.6493  |  0:00:02s\n",
            "epoch 49 | loss: 0.65214 |  0:00:02s\n",
            "epoch 50 | loss: 0.6485  |  0:00:02s\n",
            "epoch 51 | loss: 0.65383 |  0:00:02s\n",
            "epoch 52 | loss: 0.6559  |  0:00:02s\n",
            "epoch 53 | loss: 0.65519 |  0:00:02s\n",
            "epoch 54 | loss: 0.65219 |  0:00:02s\n",
            "epoch 55 | loss: 0.65004 |  0:00:02s\n",
            "epoch 56 | loss: 0.64604 |  0:00:02s\n",
            "epoch 57 | loss: 0.65232 |  0:00:02s\n",
            "epoch 58 | loss: 0.6463  |  0:00:02s\n",
            "epoch 59 | loss: 0.64845 |  0:00:02s\n",
            "epoch 60 | loss: 0.65117 |  0:00:03s\n",
            "epoch 61 | loss: 0.65793 |  0:00:03s\n",
            "epoch 62 | loss: 0.64809 |  0:00:03s\n",
            "epoch 63 | loss: 0.65311 |  0:00:03s\n",
            "epoch 64 | loss: 0.64579 |  0:00:03s\n",
            "epoch 65 | loss: 0.65086 |  0:00:03s\n",
            "epoch 66 | loss: 0.64656 |  0:00:03s\n",
            "epoch 67 | loss: 0.64561 |  0:00:03s\n",
            "epoch 68 | loss: 0.64285 |  0:00:03s\n",
            "epoch 69 | loss: 0.6459  |  0:00:03s\n",
            "epoch 70 | loss: 0.65078 |  0:00:03s\n",
            "epoch 71 | loss: 0.65501 |  0:00:03s\n",
            "epoch 72 | loss: 0.6523  |  0:00:03s\n",
            "epoch 73 | loss: 0.64628 |  0:00:03s\n",
            "epoch 74 | loss: 0.64715 |  0:00:03s\n",
            "epoch 75 | loss: 0.65391 |  0:00:03s\n",
            "epoch 76 | loss: 0.64611 |  0:00:03s\n",
            "epoch 77 | loss: 0.64867 |  0:00:03s\n",
            "epoch 78 | loss: 0.64451 |  0:00:03s\n",
            "epoch 79 | loss: 0.64396 |  0:00:03s\n",
            "epoch 80 | loss: 0.64358 |  0:00:03s\n",
            "epoch 81 | loss: 0.64865 |  0:00:03s\n",
            "epoch 82 | loss: 0.65023 |  0:00:04s\n",
            "epoch 83 | loss: 0.64649 |  0:00:04s\n",
            "epoch 84 | loss: 0.63901 |  0:00:04s\n",
            "epoch 85 | loss: 0.64505 |  0:00:04s\n",
            "epoch 86 | loss: 0.64881 |  0:00:04s\n",
            "epoch 87 | loss: 0.64666 |  0:00:04s\n",
            "epoch 88 | loss: 0.64082 |  0:00:04s\n",
            "epoch 89 | loss: 0.64109 |  0:00:04s\n",
            "epoch 90 | loss: 0.64345 |  0:00:04s\n",
            "epoch 91 | loss: 0.64278 |  0:00:04s\n",
            "epoch 92 | loss: 0.64975 |  0:00:04s\n",
            "epoch 93 | loss: 0.64627 |  0:00:04s\n",
            "epoch 94 | loss: 0.64295 |  0:00:04s\n",
            "epoch 95 | loss: 0.64204 |  0:00:04s\n",
            "epoch 96 | loss: 0.64513 |  0:00:04s\n",
            "epoch 97 | loss: 0.64912 |  0:00:04s\n",
            "epoch 98 | loss: 0.64815 |  0:00:04s\n",
            "epoch 99 | loss: 0.6456  |  0:00:04s\n",
            "Training completed for Fold_4 with AUC: 0.662315105333931\n",
            "0.6302445961088468\n",
            "Difference from baseline: 0.0557\n",
            "Difference from previous mean AUC: 0.0080\n"
          ]
        }
      ],
      "source": [
        "# Adding tabnet to the ensemble with soft voting\n",
        "xgb_classifier_weigth = 0.05\n",
        "soft_voting_ensemble = SoftVotingEnsemble(\n",
        "    models=[xgb_classifier, final_tabnet_classifier],\n",
        "    weights=[xgb_classifier_weigth, 1-xgb_classifier_weigth]  \n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, soft_voting_ensemble, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)\n",
        "\n",
        "print(f\"Difference from baseline: {mean_auc - BASELINE_SCORE:.4f}\")\n",
        "print(f\"Difference from previous mean AUC: {mean_auc - previous_mean_auc:.4f}\")\n",
        "previous_mean_auc = mean_auc"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "intro-machine-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

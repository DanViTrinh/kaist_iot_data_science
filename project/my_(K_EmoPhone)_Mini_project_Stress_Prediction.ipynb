{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqjSsKeM3fhL"
      },
      "source": [
        "# CS565-DS522 IoT Data Science Mini Project for K-EmoPhone dataset\n",
        "*This material is a joint work of TAs from IC Lab at KAIST, including Panyu Zhang, Soowon Kang, and Woohyeok Choi. This work is licensed under CC BY-SA 4.0.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQ3qZRo52Wi"
      },
      "source": [
        "## Instruction\n",
        "In this mini-project, we will build a model to predict users' self-reported stress using extracted features from K-EmoPhone dataset. This material mainly refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) conducting indepedent reproducibility experiments on K-EmoPhone dataset. In order to save time, we provide the extracted features from the raw data instead of starting from scratch. Besides, traditional machine learning model is used considering limited number of labels and multimodality issue in the in-the-wild K-EmoPhone dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbgf8BGx5MsB"
      },
      "source": [
        "## Guidance\n",
        "\n",
        "1. Before running the code, please first download the extracted features from the following [link](https://drive.google.com/file/d/1HcyFvzWEzO21osyP5E8VpVmHROX1ew7q/view?usp=sharing).\n",
        "\n",
        "2. Please change your runtime type to T4-GPU or other runtime types with GPU available since later we may use GPU for\n",
        "xgboost execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4izzUnJ6hp"
      },
      "source": [
        "Install latest version of xgboost > 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDlc8HW-Kt4",
        "outputId": "c6d197a5-7b27-4c57-f371-dc2f6c053009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: xgboost in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (3.0.1)\n",
            "Requirement already satisfied: numpy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.14.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "XnKkV_aTVZ_Z"
      },
      "outputs": [],
      "source": [
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import cloudpickle\n",
        "from datetime import datetime\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "import time\n",
        "from typing import Optional\n",
        "from contextlib import contextmanager\n",
        "\n",
        "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
        "\n",
        "RANDOM_STATE =42\n",
        "\n",
        "\n",
        "def log(msg: any):\n",
        "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aF5NVMMUzBb"
      },
      "source": [
        "## 1.Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5n57OnHIOM"
      },
      "source": [
        "### 1.1. Mount to Your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9KU0Kq4WZs",
        "outputId": "a9ea32a9-c6b4-4be2-c90f-df5028d7fe7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import drive\\n\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# not relevant for local execution\n",
        "'''\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9u_nJzcIKXF"
      },
      "source": [
        "### 1.2. Load Extracted Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg5JSHftIP_f",
        "outputId": "a770ce09-541d-4a64-f0d8-f368a7ce0ba1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "#PATH = '/content/drive/MyDrive/IoT_Data_Science/Project/Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "PATH = './Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "\n",
        "X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffYa1CuFIx7Q"
      },
      "source": [
        "X is the extracted features and the feature extraction process refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) and the immediate past time window is set as 15 minutes. y is the array of labels while groups is the user ids.\n",
        "\n",
        "Please note that here y is binarized using theoretical threshold (if ESM stress > 0, binarize as 1, else 0, ESM label scale [-3, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0jkopjUl2h"
      },
      "source": [
        "Since features are already extracted, we do not need to work on preprocessing and feature extraction again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLIYf29UYES"
      },
      "source": [
        "## 2.Feature Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOgjC9HXo5cP"
      },
      "source": [
        "There exist multiple types of features. Please try different combinations of features to see if there is any model performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "PE57GSJPVOWi"
      },
      "outputs": [],
      "source": [
        "\n",
        "#The following code is designed for reordering the data\n",
        "#################################################\n",
        "# Create a DataFrame with user_id and datetime\n",
        "\n",
        "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "\n",
        "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "\n",
        "# Sort the DataFrame by datetime\n",
        "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Update groups and datetimes\n",
        "groups = df_merged['user_id'].to_numpy()\n",
        "datetimes = df_merged['datetime'].to_numpy()\n",
        "y = df_merged['label'].to_numpy()\n",
        "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "\n",
        "\n",
        "#Divide the features into different categories\n",
        "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]\n",
        "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]\n",
        "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]\n",
        "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]\n",
        "\n",
        "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
        "\n",
        "#################################################################################\n",
        "#Below are the available features\n",
        "#Divide the time window features into sensor/ESM self-report features\n",
        "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]] #Current sensor features (value right before label)\n",
        "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]] #Current ESM features (value right before label)\n",
        "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]] #Immediate past sensor features (in past 15 minutes before label)\n",
        "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  #Immediate past ESM features\n",
        "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]] #Today epoch sensor features\n",
        "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]] #Today epoch ESM features\n",
        "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]] #Yesterday sensor features\n",
        "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] #Yesterday ESM features\n",
        "\n",
        "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]\n",
        "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]\n",
        "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]\n",
        "################################################################################\n",
        "\n",
        "#Prepare the final feature set\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_baseline  ],axis=1)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "VIqbJ_A8JGUS",
        "outputId": "59928fd1-9982-4580-c4a1-25f4afbb070c"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ESM#LastLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2619 rows × 1 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "      ESM#LastLabel\n",
              "0               0.0\n",
              "1               1.0\n",
              "2               1.0\n",
              "3               0.0\n",
              "4               0.0\n",
              "...             ...\n",
              "2614            0.0\n",
              "2615            0.0\n",
              "2616            0.0\n",
              "2617            1.0\n",
              "2618            0.0\n",
              "\n",
              "[2619 rows x 1 columns]"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat_current_ESM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPIZll5fXQld"
      },
      "source": [
        "## 3.Model Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sE6PaldpCNU"
      },
      "source": [
        "Here is the revised XGBoost Classifier. We will use random eval_size percent of training set data as evaluation set for early stoppping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "cxqMVtSVXTfH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier, DMatrix\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from typing import Union\n",
        "\n",
        "#Function for revised xgboost classifier\n",
        "class EvXGBClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Enhanced XGBClassifier with built-in validation set approach for early stopping.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_size=None,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        random_state=None,\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the custom XGBoost Classifier.\n",
        "\n",
        "        Args:\n",
        "            eval_size (float): The proportion of the dataset to include in the evaluation split.\n",
        "            eval_metric (str): The evaluation metric used for model training.\n",
        "            early_stopping_rounds (int): The number of rounds to stop training if hold-out metric doesn't improve.\n",
        "            random_state (int): Seed for the random number generator for reproducibility.\n",
        "            **kwargs: Additional arguments to be passed to the underlying XGBClassifier.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.eval_size = eval_size\n",
        "        self.eval_metric = eval_metric\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        # Initialize the XGBClassifier with specified arguments and GPU acceleration.\n",
        "        self.model = XGBClassifier(\n",
        "            random_state=self.random_state,\n",
        "            eval_metric=self.eval_metric,\n",
        "            early_stopping_rounds=self.early_stopping_rounds,\n",
        "            tree_method = \"hist\", device = \"cuda\", #Use gpu for acceleration\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\" Returns the feature importances from the fitted model. \"\"\"\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "    @property\n",
        "    def feature_names_in_(self):\n",
        "        \"\"\" Returns the feature names from the input dataset used for fitting. \"\"\"\n",
        "        return self.model.feature_names_in_\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the XGBoost model with optional early stopping using a validation set.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Training features.\n",
        "            y (np.ndarray): Target values.\n",
        "        \"\"\"\n",
        "        if self.eval_size:\n",
        "            # Split data for early stopping evaluation if eval_size is specified.\n",
        "            X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "                X, y, test_size=self.eval_size, random_state=self.random_state)\n",
        "            # Fit the model with early stopping.\n",
        "            self.model.fit(\n",
        "                X_train_sub, y_train_sub,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "        else:\n",
        "            # Fit the model without early stopping.\n",
        "            self.model.fit(X, y, verbose=False)\n",
        "\n",
        "        # Store the best iteration number for predictions.\n",
        "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the classes for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_yate5pYRg"
      },
      "source": [
        "The following is defined functions for model training and model evaluation (cross-validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoHTuB3qpsfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    print(message)  # Simple logging to stdout or enhance as needed\n",
        "\n",
        "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate the model for a single fold.\n",
        "    Args:\n",
        "        dir_result (str): Directory to store results.\n",
        "        fold_name (str): Name of the fold for identification.\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        C_cat, C_num (array): Lists of categorical and numeric feature names.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if normalize:\n",
        "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "            # Standard scaler only applied to numeric data\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        #Applying the LASSO feature selection method\n",
        "        if select:\n",
        "\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for i, s in enumerate(select):\n",
        "                C = np.asarray(X_train.columns)\n",
        "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
        "                C_sel = C[M]\n",
        "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
        "                C_num = C_num[np.isin(C_num, C_sel)]\n",
        "\n",
        "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "\n",
        "\n",
        "                X_train = pd.DataFrame(\n",
        "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "\n",
        "        if oversample:\n",
        "            # TODO: CHANGE TO ADASYN\n",
        "            #If there is any categorical data, apply SMOTE-NC, otherwise just SMOTE\n",
        "            if len(C_cat) > 0:\n",
        "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        #Deafult average method for roc_auc_score is macro\n",
        "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle =True, random_state = 42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
        "\n",
        "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "\n",
        "    return futures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOD1KJTup7il"
      },
      "source": [
        "Here, we define the feature selection method and classifier and execute the code. AUC-ROC is calculated as mean of macro AUC-ROC for all folds/users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqvkNsznrEDe",
        "outputId": "6d0b3b54-c64d-4d54-838c-e3dd6b6bdcb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:39] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:39] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.610757167360941\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:44] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:44] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_1 with AUC: 0.5570281366984664\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:48] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:48] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.5699012255350283\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:53] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:53] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_3 with AUC: 0.506083202511774\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:57] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:11:57] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_4 with AUC: 0.5600147275405007\n",
            "0.5607568919293421\n"
          ]
        }
      ],
      "source": [
        "#Featur Selection, you may want to change the feature selection methods\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005\n",
        ")\n",
        "#Classifier\n",
        "#There could exist more parameters. Please search in your defined parameter\n",
        "#space for model performance improvement\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01,\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EZcuiA56Ls"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuOqHMwFzWHv"
      },
      "source": [
        "## Assignment 1. Improve the model performance using different types of feature combinations. (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kR-jSPN0i52"
      },
      "source": [
        " Hint: Currently we are only using feat_baseline. You may want to try other feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "id": "wD6HOgC_zuiq"
      },
      "outputs": [],
      "source": [
        "#######You may need to go back to the feature preparation code and check#########\n",
        "feat_recent_ESM = pd.concat([feat_current_ESM,feat_ImmediatePast_ESM ], axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_recent_ESM, feat_time],axis=1)\n",
        "\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6745160499877482\n",
            "Training completed for Fold_1 with AUC: 0.5925673227871029\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.5935339308578745\n",
            "Training completed for Fold_3 with AUC: 0.6148589930477686\n",
            "Training completed for Fold_4 with AUC: 0.6126336684382403\n",
            "0.6176219930237469\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:14:22] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMEhtgwJz9IP"
      },
      "source": [
        "## Assignment 2. Please try different feature selection methods (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FCRh8eF0zud"
      },
      "source": [
        "Hint: Currently, we are using LASSO filter for feature selection. Please consider using embedded method as well(same model for both feature selection and model training). Besides, the threshold for LASSO filter may also affect the performance. **Sepcifically, there is a method called 'mean' which is using mean of feature importances of all features as threshold.** Please try both different feature selection methods and different thresholds for filtering features to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "id": "Bwna8wpv3Dhj"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "Training completed for Fold_1 with AUC: 0.6017932616833715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.6393451618803732\n",
            "Training completed for Fold_3 with AUC: 0.6202273491814307\n",
            "Training completed for Fold_4 with AUC: 0.6385029134917077\n",
            "0.6351244356056223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [15:44:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [15:44:11] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        }
      ],
      "source": [
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "xgb_selector = SelectFromModel(\n",
        "    estimator=XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
        "    threshold='mean' # Or a different threshold\n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_FJZurk3ulY"
      },
      "source": [
        "## Assignment 3. Please try using hyperopt for model hyperparameter tuning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQcs-te34PK"
      },
      "source": [
        "Hint: Please be aware that for revised xgboost classifier EvXGBClassifier, there exist other parameters other than default XGBClassifier parameters such as eval_size.\n",
        "\n",
        "For hyperparameter tuning, we will use 20% of training set as validation set to avoid data leakage.\n",
        "\n",
        "If it is too timeconsuming to run the code in colab, please run the code locally and consider using [ray tune](https://docs.ray.io/en/latest/tune/index.html) if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8PyEHuv4R-L",
        "outputId": "ee08935d-a307-409f-ba56-901261230535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "100%|██████████| 100/100 [00:47<00:00,  2.10trial/s, best loss: -0.6189693231281869]\n",
            "Best hyperparameters: {'colsample_bytree': 0.9012966810519976, 'gamma': 0.22247273381633864, 'learning_rate': 0.028542060596548048, 'max_depth': 7.0, 'min_child_weight': 8.0, 'n_estimators': 800.0, 'reg_alpha': 0.2608104132350774, 'reg_lambda': 0.2620754838361904, 'subsample': 0.8360036468008175}\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            smote = SMOTENC(\n",
        "                categorical_features=[X_train_scaled.shape[1]-1],\n",
        "                random_state=int(params['random_state'])\n",
        "            )\n",
        "        else:\n",
        "            smote = SMOTE(random_state=int(params['random_state']))\n",
        "        X_train_os, y_train_os = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        selector = SelectFromModel(\n",
        "            LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                               random_state=int(params['random_state'])),\n",
        "            threshold='mean'\n",
        "        )\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = LogisticRegression(\n",
        "            random_state=int(params['random_state']),\n",
        "            max_iter=1000\n",
        "        )\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "space = {\n",
        "    'max_depth':          hp.quniform('max_depth', 3, 10, 1),  # Integer between 3 and 10\n",
        "    'min_child_weight':   hp.quniform('min_child_weight', 1, 10, 1), # Integer between 1 and 10\n",
        "    'subsample':          hp.uniform('subsample', 0.6, 1.0),  # Float between 0.6 and 1.0\n",
        "    'colsample_bytree':   hp.uniform('colsample_bytree', 0.6, 1.0), # Float between 0.6 and 1.0\n",
        "    'gamma':              hp.uniform('gamma', 0, 0.5),      # Float between 0 and 0.5\n",
        "    'learning_rate':      hp.loguniform('learning_rate', -5, 0), # Float on a log scale (0.0067 to 1)\n",
        "    'n_estimators':       hp.quniform('n_estimators', 100, 1000, 50), # Integer between 100 and 1000, steps of 50\n",
        "    'reg_lambda':         hp.uniform('reg_lambda', 0, 1),     # Float between 0 and 1 (L2 regularization)\n",
        "    'reg_alpha':          hp.uniform('reg_alpha', 0, 0.5),    # Float between 0 and 0.5 (L1 regularization)\n",
        "    'random_state':       42 # Keeping random_state fixed\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:52:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "Training completed for Fold_1 with AUC: 0.6017932616833715\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:52:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:52:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:52:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:19] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:52:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_2 with AUC: 0.6393451618803732\n",
            "Training completed for Fold_3 with AUC: 0.6202273491814307\n",
            "Training completed for Fold_4 with AUC: 0.6385029134917077\n",
            "0.6351244356056223\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:20] WARNING: /workspace/src/context.cc:49: No visible GPU is found, setting device to CPU.\n",
            "  self.starting_round = model.num_boosted_rounds()\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/callback.py:386: UserWarning: [16:52:20] WARNING: /workspace/src/context.cc:203: XGBoost is not compiled with CUDA support.\n",
            "  self.starting_round = model.num_boosted_rounds()\n"
          ]
        }
      ],
      "source": [
        "\n",
        "#Classifier\n",
        "estimator = EvXGBClassifier(\n",
        "    colsample_bytree=best['colsample_bytree'],\n",
        "    gamma=best['gamma'],\n",
        "    learning_rate=best['learning_rate'],\n",
        "    max_depth=int(best['max_depth']),\n",
        "    min_child_weight=int(best['min_child_weight']),\n",
        "    n_estimators=int(best['n_estimators']),\n",
        "    reg_alpha=best['reg_alpha'],\n",
        "    reg_lambda=best['reg_lambda'],\n",
        "    subsample=best['subsample'],\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    use_label_encoder=False,  # Avoid warning about label encoder\n",
        ")\n",
        "\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPz966iD6yoH"
      },
      "source": [
        "## Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for **tabular data** to improve model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfbskLs7qmc"
      },
      "source": [
        "Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for **tabular data** and see if there is performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {},
      "outputs": [],
      "source": [
        "from sklearn.base import BaseEstimator, ClassifierMixin\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "\n",
        "class SklearnTabNet(BaseEstimator, ClassifierMixin):\n",
        "    def __init__(self, **kwargs):\n",
        "        self.model = TabNetClassifier(**kwargs)\n",
        "        self.fit_params = kwargs\n",
        "        self.label_encoder = LabelEncoder()\n",
        "\n",
        "    def fit(self, X, y):\n",
        "        # Convert DataFrame to NumPy if needed\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        y = self.label_encoder.fit_transform(y)\n",
        "        self.model.fit(X, y)\n",
        "        return self\n",
        "\n",
        "    def predict(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        preds = self.model.predict(X)\n",
        "        return self.label_encoder.inverse_transform(preds)\n",
        "\n",
        "    def predict_proba(self, X):\n",
        "        if hasattr(X, \"values\"):\n",
        "            X = X.values\n",
        "        return self.model.predict_proba(X)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbE_mod6TUIn"
      },
      "source": [
        "You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDEeP0pkBsn6"
      },
      "source": [
        "Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "MZk2A2-I7nPd"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82129 |  0:00:00s\n",
            "epoch 1  | loss: 0.69413 |  0:00:00s\n",
            "epoch 2  | loss: 0.68169 |  0:00:00s\n",
            "epoch 3  | loss: 0.67449 |  0:00:00s\n",
            "epoch 4  | loss: 0.66989 |  0:00:00s\n",
            "epoch 5  | loss: 0.66785 |  0:00:00s\n",
            "epoch 6  | loss: 0.66801 |  0:00:00s\n",
            "epoch 7  | loss: 0.6692  |  0:00:00s\n",
            "epoch 8  | loss: 0.67093 |  0:00:00s\n",
            "epoch 9  | loss: 0.66066 |  0:00:00s\n",
            "epoch 10 | loss: 0.66997 |  0:00:00s\n",
            "epoch 11 | loss: 0.66499 |  0:00:00s\n",
            "epoch 12 | loss: 0.6652  |  0:00:00s\n",
            "epoch 13 | loss: 0.66063 |  0:00:00s\n",
            "epoch 14 | loss: 0.66369 |  0:00:00s\n",
            "epoch 15 | loss: 0.66042 |  0:00:00s\n",
            "epoch 16 | loss: 0.66606 |  0:00:01s\n",
            "epoch 17 | loss: 0.6549  |  0:00:01s\n",
            "epoch 18 | loss: 0.66593 |  0:00:01s\n",
            "epoch 19 | loss: 0.66275 |  0:00:01s\n",
            "epoch 20 | loss: 0.66116 |  0:00:01s\n",
            "epoch 21 | loss: 0.67011 |  0:00:01s\n",
            "epoch 22 | loss: 0.66674 |  0:00:01s\n",
            "epoch 23 | loss: 0.66347 |  0:00:01s\n",
            "epoch 24 | loss: 0.66219 |  0:00:01s\n",
            "epoch 25 | loss: 0.66776 |  0:00:01s\n",
            "epoch 26 | loss: 0.66323 |  0:00:01s\n",
            "epoch 27 | loss: 0.66517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66195 |  0:00:01s\n",
            "epoch 29 | loss: 0.66681 |  0:00:01s\n",
            "epoch 30 | loss: 0.66132 |  0:00:01s\n",
            "epoch 31 | loss: 0.66383 |  0:00:01s\n",
            "epoch 32 | loss: 0.66425 |  0:00:02s\n",
            "epoch 33 | loss: 0.65932 |  0:00:02s\n",
            "epoch 34 | loss: 0.66514 |  0:00:02s\n",
            "epoch 35 | loss: 0.66176 |  0:00:02s\n",
            "epoch 36 | loss: 0.66962 |  0:00:02s\n",
            "epoch 37 | loss: 0.66413 |  0:00:02s\n",
            "epoch 38 | loss: 0.66864 |  0:00:02s\n",
            "epoch 39 | loss: 0.66556 |  0:00:02s\n",
            "epoch 40 | loss: 0.66459 |  0:00:02s\n",
            "epoch 41 | loss: 0.66431 |  0:00:02s\n",
            "epoch 42 | loss: 0.66639 |  0:00:02s\n",
            "epoch 43 | loss: 0.66566 |  0:00:02s\n",
            "epoch 44 | loss: 0.66461 |  0:00:02s\n",
            "epoch 45 | loss: 0.66274 |  0:00:02s\n",
            "epoch 46 | loss: 0.66234 |  0:00:02s\n",
            "epoch 47 | loss: 0.66805 |  0:00:02s\n",
            "epoch 48 | loss: 0.66645 |  0:00:02s\n",
            "epoch 49 | loss: 0.6657  |  0:00:03s\n",
            "epoch 50 | loss: 0.6629  |  0:00:03s\n",
            "epoch 51 | loss: 0.65916 |  0:00:03s\n",
            "epoch 52 | loss: 0.66155 |  0:00:03s\n",
            "epoch 53 | loss: 0.66205 |  0:00:03s\n",
            "epoch 54 | loss: 0.65815 |  0:00:03s\n",
            "epoch 55 | loss: 0.66152 |  0:00:03s\n",
            "epoch 56 | loss: 0.66028 |  0:00:03s\n",
            "epoch 57 | loss: 0.66463 |  0:00:03s\n",
            "epoch 58 | loss: 0.65953 |  0:00:03s\n",
            "epoch 59 | loss: 0.66682 |  0:00:03s\n",
            "epoch 60 | loss: 0.66321 |  0:00:03s\n",
            "epoch 61 | loss: 0.66373 |  0:00:04s\n",
            "epoch 62 | loss: 0.66355 |  0:00:04s\n",
            "epoch 63 | loss: 0.66078 |  0:00:04s\n",
            "epoch 64 | loss: 0.66516 |  0:00:04s\n",
            "epoch 65 | loss: 0.66152 |  0:00:04s\n",
            "epoch 66 | loss: 0.66482 |  0:00:04s\n",
            "epoch 67 | loss: 0.66327 |  0:00:04s\n",
            "epoch 68 | loss: 0.66354 |  0:00:04s\n",
            "epoch 69 | loss: 0.66134 |  0:00:04s\n",
            "epoch 70 | loss: 0.66747 |  0:00:04s\n",
            "epoch 71 | loss: 0.65962 |  0:00:04s\n",
            "epoch 72 | loss: 0.66135 |  0:00:04s\n",
            "epoch 73 | loss: 0.66312 |  0:00:04s\n",
            "epoch 74 | loss: 0.66194 |  0:00:04s\n",
            "epoch 75 | loss: 0.66819 |  0:00:04s\n",
            "epoch 76 | loss: 0.66151 |  0:00:04s\n",
            "epoch 77 | loss: 0.65716 |  0:00:04s\n",
            "epoch 78 | loss: 0.66099 |  0:00:04s\n",
            "epoch 79 | loss: 0.6632  |  0:00:05s\n",
            "epoch 80 | loss: 0.6629  |  0:00:05s\n",
            "epoch 81 | loss: 0.66266 |  0:00:05s\n",
            "epoch 82 | loss: 0.65956 |  0:00:05s\n",
            "epoch 83 | loss: 0.6576  |  0:00:05s\n",
            "epoch 84 | loss: 0.66103 |  0:00:05s\n",
            "epoch 85 | loss: 0.66449 |  0:00:05s\n",
            "epoch 86 | loss: 0.6628  |  0:00:05s\n",
            "epoch 87 | loss: 0.66488 |  0:00:05s\n",
            "epoch 88 | loss: 0.6673  |  0:00:05s\n",
            "epoch 89 | loss: 0.66309 |  0:00:05s\n",
            "epoch 90 | loss: 0.66404 |  0:00:05s\n",
            "epoch 91 | loss: 0.66472 |  0:00:05s\n",
            "epoch 92 | loss: 0.66572 |  0:00:05s\n",
            "epoch 93 | loss: 0.66462 |  0:00:05s\n",
            "epoch 94 | loss: 0.66543 |  0:00:05s\n",
            "epoch 95 | loss: 0.66423 |  0:00:05s\n",
            "epoch 96 | loss: 0.66119 |  0:00:05s\n",
            "epoch 97 | loss: 0.66243 |  0:00:06s\n",
            "epoch 98 | loss: 0.66079 |  0:00:06s\n",
            "epoch 99 | loss: 0.66241 |  0:00:06s\n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "epoch 0  | loss: 0.79829 |  0:00:00s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.66669 |  0:00:00s\n",
            "epoch 2  | loss: 0.66416 |  0:00:00s\n",
            "epoch 3  | loss: 0.65561 |  0:00:00s\n",
            "epoch 4  | loss: 0.65932 |  0:00:00s\n",
            "epoch 5  | loss: 0.66116 |  0:00:00s\n",
            "epoch 6  | loss: 0.65419 |  0:00:00s\n",
            "epoch 7  | loss: 0.65517 |  0:00:00s\n",
            "epoch 8  | loss: 0.66334 |  0:00:00s\n",
            "epoch 9  | loss: 0.65327 |  0:00:00s\n",
            "epoch 10 | loss: 0.65971 |  0:00:00s\n",
            "epoch 11 | loss: 0.659   |  0:00:00s\n",
            "epoch 12 | loss: 0.66051 |  0:00:00s\n",
            "epoch 13 | loss: 0.65711 |  0:00:00s\n",
            "epoch 14 | loss: 0.6592  |  0:00:00s\n",
            "epoch 15 | loss: 0.65609 |  0:00:00s\n",
            "epoch 16 | loss: 0.65753 |  0:00:00s\n",
            "epoch 17 | loss: 0.66103 |  0:00:01s\n",
            "epoch 18 | loss: 0.66029 |  0:00:01s\n",
            "epoch 19 | loss: 0.65484 |  0:00:01s\n",
            "epoch 20 | loss: 0.65865 |  0:00:01s\n",
            "epoch 21 | loss: 0.65416 |  0:00:01s\n",
            "epoch 22 | loss: 0.65564 |  0:00:01s\n",
            "epoch 23 | loss: 0.65826 |  0:00:01s\n",
            "epoch 24 | loss: 0.65388 |  0:00:01s\n",
            "epoch 25 | loss: 0.65951 |  0:00:01s\n",
            "epoch 26 | loss: 0.65773 |  0:00:01s\n",
            "epoch 27 | loss: 0.6569  |  0:00:01s\n",
            "epoch 28 | loss: 0.66149 |  0:00:01s\n",
            "epoch 29 | loss: 0.66262 |  0:00:01s\n",
            "epoch 30 | loss: 0.66118 |  0:00:01s\n",
            "epoch 31 | loss: 0.65915 |  0:00:01s\n",
            "epoch 32 | loss: 0.65585 |  0:00:01s\n",
            "epoch 33 | loss: 0.65769 |  0:00:01s\n",
            "epoch 34 | loss: 0.6549  |  0:00:01s\n",
            "epoch 35 | loss: 0.65304 |  0:00:01s\n",
            "epoch 36 | loss: 0.66283 |  0:00:01s\n",
            "epoch 37 | loss: 0.66036 |  0:00:01s\n",
            "epoch 38 | loss: 0.65727 |  0:00:01s\n",
            "epoch 39 | loss: 0.65843 |  0:00:01s\n",
            "epoch 40 | loss: 0.65764 |  0:00:02s\n",
            "epoch 41 | loss: 0.65725 |  0:00:02s\n",
            "epoch 42 | loss: 0.66286 |  0:00:02s\n",
            "epoch 43 | loss: 0.66094 |  0:00:02s\n",
            "epoch 44 | loss: 0.65994 |  0:00:02s\n",
            "epoch 45 | loss: 0.65736 |  0:00:02s\n",
            "epoch 46 | loss: 0.66132 |  0:00:02s\n",
            "epoch 47 | loss: 0.65876 |  0:00:02s\n",
            "epoch 48 | loss: 0.655   |  0:00:02s\n",
            "epoch 49 | loss: 0.6546  |  0:00:02s\n",
            "epoch 50 | loss: 0.6517  |  0:00:02s\n",
            "epoch 51 | loss: 0.65076 |  0:00:02s\n",
            "epoch 52 | loss: 0.66134 |  0:00:02s\n",
            "epoch 53 | loss: 0.65962 |  0:00:02s\n",
            "epoch 54 | loss: 0.65704 |  0:00:02s\n",
            "epoch 55 | loss: 0.65682 |  0:00:02s\n",
            "epoch 56 | loss: 0.65616 |  0:00:02s\n",
            "epoch 57 | loss: 0.66184 |  0:00:02s\n",
            "epoch 58 | loss: 0.66032 |  0:00:02s\n",
            "epoch 59 | loss: 0.6559  |  0:00:02s\n",
            "epoch 60 | loss: 0.65696 |  0:00:02s\n",
            "epoch 61 | loss: 0.65936 |  0:00:02s\n",
            "epoch 62 | loss: 0.65319 |  0:00:02s\n",
            "epoch 63 | loss: 0.65951 |  0:00:02s\n",
            "epoch 64 | loss: 0.6624  |  0:00:02s\n",
            "epoch 65 | loss: 0.65389 |  0:00:02s\n",
            "epoch 66 | loss: 0.65476 |  0:00:02s\n",
            "epoch 67 | loss: 0.6564  |  0:00:03s\n",
            "epoch 68 | loss: 0.6578  |  0:00:03s\n",
            "epoch 69 | loss: 0.65999 |  0:00:03s\n",
            "epoch 70 | loss: 0.66151 |  0:00:03s\n",
            "epoch 71 | loss: 0.65282 |  0:00:03s\n",
            "epoch 72 | loss: 0.65757 |  0:00:03s\n",
            "epoch 73 | loss: 0.65051 |  0:00:03s\n",
            "epoch 74 | loss: 0.65598 |  0:00:03s\n",
            "epoch 75 | loss: 0.65955 |  0:00:03s\n",
            "epoch 76 | loss: 0.65574 |  0:00:03s\n",
            "epoch 77 | loss: 0.66164 |  0:00:03s\n",
            "epoch 78 | loss: 0.65796 |  0:00:03s\n",
            "epoch 79 | loss: 0.65643 |  0:00:03s\n",
            "epoch 80 | loss: 0.65572 |  0:00:03s\n",
            "epoch 81 | loss: 0.65466 |  0:00:03s\n",
            "epoch 82 | loss: 0.65868 |  0:00:03s\n",
            "epoch 83 | loss: 0.66234 |  0:00:03s\n",
            "epoch 84 | loss: 0.65762 |  0:00:03s\n",
            "epoch 85 | loss: 0.65425 |  0:00:03s\n",
            "epoch 86 | loss: 0.65816 |  0:00:03s\n",
            "epoch 87 | loss: 0.65813 |  0:00:03s\n",
            "epoch 88 | loss: 0.66011 |  0:00:03s\n",
            "epoch 89 | loss: 0.65716 |  0:00:03s\n",
            "epoch 90 | loss: 0.65569 |  0:00:03s\n",
            "epoch 91 | loss: 0.6534  |  0:00:03s\n",
            "epoch 92 | loss: 0.65997 |  0:00:03s\n",
            "epoch 93 | loss: 0.65685 |  0:00:03s\n",
            "epoch 94 | loss: 0.65627 |  0:00:04s\n",
            "epoch 95 | loss: 0.65819 |  0:00:04s\n",
            "epoch 96 | loss: 0.65828 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.65309 |  0:00:04s\n",
            "epoch 99 | loss: 0.65704 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.74116 |  0:00:00s\n",
            "epoch 1  | loss: 0.70513 |  0:00:00s\n",
            "epoch 2  | loss: 0.69432 |  0:00:00s\n",
            "epoch 3  | loss: 0.67477 |  0:00:00s\n",
            "epoch 4  | loss: 0.67408 |  0:00:00s\n",
            "epoch 5  | loss: 0.66448 |  0:00:00s\n",
            "epoch 6  | loss: 0.66114 |  0:00:00s\n",
            "epoch 7  | loss: 0.67036 |  0:00:00s\n",
            "epoch 8  | loss: 0.66511 |  0:00:00s\n",
            "epoch 9  | loss: 0.66068 |  0:00:00s\n",
            "epoch 10 | loss: 0.66214 |  0:00:00s\n",
            "epoch 11 | loss: 0.6624  |  0:00:00s\n",
            "epoch 12 | loss: 0.66165 |  0:00:00s\n",
            "epoch 13 | loss: 0.66099 |  0:00:00s\n",
            "epoch 14 | loss: 0.66227 |  0:00:00s\n",
            "epoch 15 | loss: 0.66577 |  0:00:00s\n",
            "epoch 16 | loss: 0.6587  |  0:00:00s\n",
            "epoch 17 | loss: 0.66433 |  0:00:00s\n",
            "epoch 18 | loss: 0.6621  |  0:00:00s\n",
            "epoch 19 | loss: 0.6587  |  0:00:00s\n",
            "epoch 20 | loss: 0.65557 |  0:00:00s\n",
            "epoch 21 | loss: 0.65922 |  0:00:00s\n",
            "epoch 22 | loss: 0.66284 |  0:00:00s\n",
            "epoch 23 | loss: 0.65952 |  0:00:00s\n",
            "epoch 24 | loss: 0.66066 |  0:00:00s\n",
            "epoch 25 | loss: 0.65914 |  0:00:01s\n",
            "epoch 26 | loss: 0.65793 |  0:00:01s\n",
            "epoch 27 | loss: 0.66025 |  0:00:01s\n",
            "epoch 28 | loss: 0.66212 |  0:00:01s\n",
            "epoch 29 | loss: 0.66159 |  0:00:01s\n",
            "epoch 30 | loss: 0.65868 |  0:00:01s\n",
            "epoch 31 | loss: 0.66358 |  0:00:01s\n",
            "epoch 32 | loss: 0.65803 |  0:00:01s\n",
            "epoch 33 | loss: 0.65629 |  0:00:01s\n",
            "epoch 34 | loss: 0.658   |  0:00:01s\n",
            "epoch 35 | loss: 0.66108 |  0:00:01s\n",
            "epoch 36 | loss: 0.66254 |  0:00:01s\n",
            "epoch 37 | loss: 0.66275 |  0:00:01s\n",
            "epoch 38 | loss: 0.65842 |  0:00:01s\n",
            "epoch 39 | loss: 0.66139 |  0:00:01s\n",
            "epoch 40 | loss: 0.65955 |  0:00:01s\n",
            "epoch 41 | loss: 0.65772 |  0:00:01s\n",
            "epoch 42 | loss: 0.66374 |  0:00:01s\n",
            "epoch 43 | loss: 0.66061 |  0:00:01s\n",
            "epoch 44 | loss: 0.6623  |  0:00:01s\n",
            "epoch 45 | loss: 0.65957 |  0:00:01s\n",
            "epoch 46 | loss: 0.66121 |  0:00:01s\n",
            "epoch 47 | loss: 0.66013 |  0:00:01s\n",
            "epoch 48 | loss: 0.66283 |  0:00:01s\n",
            "epoch 49 | loss: 0.66251 |  0:00:02s\n",
            "epoch 50 | loss: 0.65744 |  0:00:02s\n",
            "epoch 51 | loss: 0.66202 |  0:00:02s\n",
            "epoch 52 | loss: 0.65722 |  0:00:02s\n",
            "epoch 53 | loss: 0.66317 |  0:00:02s\n",
            "epoch 54 | loss: 0.65923 |  0:00:02s\n",
            "epoch 55 | loss: 0.65988 |  0:00:02s\n",
            "epoch 56 | loss: 0.66293 |  0:00:02s\n",
            "epoch 57 | loss: 0.66161 |  0:00:02s\n",
            "epoch 58 | loss: 0.65706 |  0:00:02s\n",
            "epoch 59 | loss: 0.66115 |  0:00:02s\n",
            "epoch 60 | loss: 0.65831 |  0:00:02s\n",
            "epoch 61 | loss: 0.65972 |  0:00:02s\n",
            "epoch 62 | loss: 0.66007 |  0:00:02s\n",
            "epoch 63 | loss: 0.65941 |  0:00:02s\n",
            "epoch 64 | loss: 0.65281 |  0:00:02s\n",
            "epoch 65 | loss: 0.65539 |  0:00:02s\n",
            "epoch 66 | loss: 0.66362 |  0:00:02s\n",
            "epoch 67 | loss: 0.66359 |  0:00:02s\n",
            "epoch 68 | loss: 0.66279 |  0:00:02s\n",
            "epoch 69 | loss: 0.66138 |  0:00:02s\n",
            "epoch 70 | loss: 0.66039 |  0:00:02s\n",
            "epoch 71 | loss: 0.65481 |  0:00:02s\n",
            "epoch 72 | loss: 0.65951 |  0:00:03s\n",
            "epoch 73 | loss: 0.65521 |  0:00:03s\n",
            "epoch 74 | loss: 0.66144 |  0:00:03s\n",
            "epoch 75 | loss: 0.65836 |  0:00:03s\n",
            "epoch 76 | loss: 0.66008 |  0:00:03s\n",
            "epoch 77 | loss: 0.66101 |  0:00:03s\n",
            "epoch 78 | loss: 0.65836 |  0:00:03s\n",
            "epoch 79 | loss: 0.65731 |  0:00:03s\n",
            "epoch 80 | loss: 0.65836 |  0:00:03s\n",
            "epoch 81 | loss: 0.65784 |  0:00:03s\n",
            "epoch 82 | loss: 0.66167 |  0:00:03s\n",
            "epoch 83 | loss: 0.66068 |  0:00:03s\n",
            "epoch 84 | loss: 0.66122 |  0:00:03s\n",
            "epoch 85 | loss: 0.65947 |  0:00:03s\n",
            "epoch 86 | loss: 0.65888 |  0:00:03s\n",
            "epoch 87 | loss: 0.65789 |  0:00:03s\n",
            "epoch 88 | loss: 0.65418 |  0:00:03s\n",
            "epoch 89 | loss: 0.65439 |  0:00:03s\n",
            "epoch 90 | loss: 0.65631 |  0:00:03s\n",
            "epoch 91 | loss: 0.66406 |  0:00:03s\n",
            "epoch 92 | loss: 0.65971 |  0:00:03s\n",
            "epoch 93 | loss: 0.66105 |  0:00:03s\n",
            "epoch 94 | loss: 0.66296 |  0:00:03s\n",
            "epoch 95 | loss: 0.66138 |  0:00:03s\n",
            "epoch 96 | loss: 0.65509 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.66348 |  0:00:04s\n",
            "epoch 99 | loss: 0.65617 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.632787634900311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s\n",
            "epoch 1  | loss: 0.67173 |  0:00:00s\n",
            "epoch 2  | loss: 0.65725 |  0:00:00s\n",
            "epoch 3  | loss: 0.65813 |  0:00:00s\n",
            "epoch 4  | loss: 0.65162 |  0:00:00s\n",
            "epoch 5  | loss: 0.65938 |  0:00:00s\n",
            "epoch 6  | loss: 0.65699 |  0:00:00s\n",
            "epoch 7  | loss: 0.65609 |  0:00:00s\n",
            "epoch 8  | loss: 0.65957 |  0:00:00s\n",
            "epoch 9  | loss: 0.65455 |  0:00:00s\n",
            "epoch 10 | loss: 0.65117 |  0:00:00s\n",
            "epoch 11 | loss: 0.64928 |  0:00:00s\n",
            "epoch 12 | loss: 0.6484  |  0:00:00s\n",
            "epoch 13 | loss: 0.65202 |  0:00:00s\n",
            "epoch 14 | loss: 0.65083 |  0:00:00s\n",
            "epoch 15 | loss: 0.65236 |  0:00:00s\n",
            "epoch 16 | loss: 0.65407 |  0:00:00s\n",
            "epoch 17 | loss: 0.65722 |  0:00:00s\n",
            "epoch 18 | loss: 0.65453 |  0:00:00s\n",
            "epoch 19 | loss: 0.65006 |  0:00:00s\n",
            "epoch 20 | loss: 0.65286 |  0:00:00s\n",
            "epoch 21 | loss: 0.64635 |  0:00:00s\n",
            "epoch 22 | loss: 0.65601 |  0:00:00s\n",
            "epoch 23 | loss: 0.65498 |  0:00:01s\n",
            "epoch 24 | loss: 0.65045 |  0:00:01s\n",
            "epoch 25 | loss: 0.64957 |  0:00:01s\n",
            "epoch 26 | loss: 0.64914 |  0:00:01s\n",
            "epoch 27 | loss: 0.64831 |  0:00:01s\n",
            "epoch 28 | loss: 0.65263 |  0:00:01s\n",
            "epoch 29 | loss: 0.65653 |  0:00:01s\n",
            "epoch 30 | loss: 0.6524  |  0:00:01s\n",
            "epoch 31 | loss: 0.64977 |  0:00:01s\n",
            "epoch 32 | loss: 0.6519  |  0:00:01s\n",
            "epoch 33 | loss: 0.65507 |  0:00:01s\n",
            "epoch 34 | loss: 0.65584 |  0:00:01s\n",
            "epoch 35 | loss: 0.64758 |  0:00:01s\n",
            "epoch 36 | loss: 0.64753 |  0:00:01s\n",
            "epoch 37 | loss: 0.6508  |  0:00:01s\n",
            "epoch 38 | loss: 0.65147 |  0:00:01s\n",
            "epoch 39 | loss: 0.65669 |  0:00:01s\n",
            "epoch 40 | loss: 0.65063 |  0:00:01s\n",
            "epoch 41 | loss: 0.65125 |  0:00:01s\n",
            "epoch 42 | loss: 0.65242 |  0:00:01s\n",
            "epoch 43 | loss: 0.65139 |  0:00:01s\n",
            "epoch 44 | loss: 0.64979 |  0:00:01s\n",
            "epoch 45 | loss: 0.65282 |  0:00:01s\n",
            "epoch 46 | loss: 0.64992 |  0:00:02s\n",
            "epoch 47 | loss: 0.65172 |  0:00:02s\n",
            "epoch 48 | loss: 0.65339 |  0:00:02s\n",
            "epoch 49 | loss: 0.65177 |  0:00:02s\n",
            "epoch 50 | loss: 0.6483  |  0:00:02s\n",
            "epoch 51 | loss: 0.65488 |  0:00:02s\n",
            "epoch 52 | loss: 0.65112 |  0:00:02s\n",
            "epoch 53 | loss: 0.64882 |  0:00:02s\n",
            "epoch 54 | loss: 0.65205 |  0:00:02s\n",
            "epoch 55 | loss: 0.65058 |  0:00:02s\n",
            "epoch 56 | loss: 0.65775 |  0:00:02s\n",
            "epoch 57 | loss: 0.65229 |  0:00:02s\n",
            "epoch 58 | loss: 0.65413 |  0:00:02s\n",
            "epoch 59 | loss: 0.65194 |  0:00:02s\n",
            "epoch 60 | loss: 0.65233 |  0:00:02s\n",
            "epoch 61 | loss: 0.65317 |  0:00:02s\n",
            "epoch 62 | loss: 0.65004 |  0:00:02s\n",
            "epoch 63 | loss: 0.65206 |  0:00:02s\n",
            "epoch 64 | loss: 0.65329 |  0:00:02s\n",
            "epoch 65 | loss: 0.65854 |  0:00:02s\n",
            "epoch 66 | loss: 0.64967 |  0:00:02s\n",
            "epoch 67 | loss: 0.65222 |  0:00:02s\n",
            "epoch 68 | loss: 0.65413 |  0:00:02s\n",
            "epoch 69 | loss: 0.64616 |  0:00:03s\n",
            "epoch 70 | loss: 0.65072 |  0:00:03s\n",
            "epoch 71 | loss: 0.65243 |  0:00:03s\n",
            "epoch 72 | loss: 0.65491 |  0:00:03s\n",
            "epoch 73 | loss: 0.64967 |  0:00:03s\n",
            "epoch 74 | loss: 0.65569 |  0:00:03s\n",
            "epoch 75 | loss: 0.64423 |  0:00:03s\n",
            "epoch 76 | loss: 0.64815 |  0:00:03s\n",
            "epoch 77 | loss: 0.64981 |  0:00:03s\n",
            "epoch 78 | loss: 0.65015 |  0:00:03s\n",
            "epoch 79 | loss: 0.64704 |  0:00:03s\n",
            "epoch 80 | loss: 0.65555 |  0:00:03s\n",
            "epoch 81 | loss: 0.65336 |  0:00:03s\n",
            "epoch 82 | loss: 0.64887 |  0:00:03s\n",
            "epoch 83 | loss: 0.65124 |  0:00:03s\n",
            "epoch 84 | loss: 0.64734 |  0:00:03s\n",
            "epoch 85 | loss: 0.64845 |  0:00:03s\n",
            "epoch 86 | loss: 0.65064 |  0:00:03s\n",
            "epoch 87 | loss: 0.6533  |  0:00:03s\n",
            "epoch 88 | loss: 0.65552 |  0:00:04s\n",
            "epoch 89 | loss: 0.64917 |  0:00:04s\n",
            "epoch 90 | loss: 0.6497  |  0:00:04s\n",
            "epoch 91 | loss: 0.64727 |  0:00:04s\n",
            "epoch 92 | loss: 0.65358 |  0:00:04s\n",
            "epoch 93 | loss: 0.65438 |  0:00:04s\n",
            "epoch 94 | loss: 0.64729 |  0:00:04s\n",
            "epoch 95 | loss: 0.65835 |  0:00:04s\n",
            "epoch 96 | loss: 0.65608 |  0:00:04s\n",
            "epoch 97 | loss: 0.64505 |  0:00:04s\n",
            "epoch 98 | loss: 0.65118 |  0:00:04s\n",
            "epoch 99 | loss: 0.6487  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:53:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 1.16024 |  0:00:00s\n",
            "epoch 1  | loss: 0.71131 |  0:00:00s\n",
            "epoch 2  | loss: 0.70045 |  0:00:00s\n",
            "epoch 3  | loss: 0.69166 |  0:00:00s\n",
            "epoch 4  | loss: 0.67844 |  0:00:00s\n",
            "epoch 5  | loss: 0.67201 |  0:00:00s\n",
            "epoch 6  | loss: 0.66742 |  0:00:00s\n",
            "epoch 7  | loss: 0.65627 |  0:00:00s\n",
            "epoch 8  | loss: 0.65846 |  0:00:00s\n",
            "epoch 9  | loss: 0.66    |  0:00:00s\n",
            "epoch 10 | loss: 0.66268 |  0:00:00s\n",
            "epoch 11 | loss: 0.65486 |  0:00:00s\n",
            "epoch 12 | loss: 0.66073 |  0:00:00s\n",
            "epoch 13 | loss: 0.66184 |  0:00:00s\n",
            "epoch 14 | loss: 0.66056 |  0:00:00s\n",
            "epoch 15 | loss: 0.65274 |  0:00:00s\n",
            "epoch 16 | loss: 0.66015 |  0:00:00s\n",
            "epoch 17 | loss: 0.66601 |  0:00:00s\n",
            "epoch 18 | loss: 0.66265 |  0:00:00s\n",
            "epoch 19 | loss: 0.65902 |  0:00:00s\n",
            "epoch 20 | loss: 0.65729 |  0:00:00s\n",
            "epoch 21 | loss: 0.65535 |  0:00:01s\n",
            "epoch 22 | loss: 0.64845 |  0:00:01s\n",
            "epoch 23 | loss: 0.65655 |  0:00:01s\n",
            "epoch 24 | loss: 0.65627 |  0:00:01s\n",
            "epoch 25 | loss: 0.65279 |  0:00:01s\n",
            "epoch 26 | loss: 0.65325 |  0:00:01s\n",
            "epoch 27 | loss: 0.65968 |  0:00:01s\n",
            "epoch 28 | loss: 0.65152 |  0:00:01s\n",
            "epoch 29 | loss: 0.65759 |  0:00:01s\n",
            "epoch 30 | loss: 0.65344 |  0:00:01s\n",
            "epoch 31 | loss: 0.65118 |  0:00:01s\n",
            "epoch 32 | loss: 0.65427 |  0:00:01s\n",
            "epoch 33 | loss: 0.65642 |  0:00:01s\n",
            "epoch 34 | loss: 0.65423 |  0:00:01s\n",
            "epoch 35 | loss: 0.65555 |  0:00:01s\n",
            "epoch 36 | loss: 0.65816 |  0:00:01s\n",
            "epoch 37 | loss: 0.65569 |  0:00:01s\n",
            "epoch 38 | loss: 0.6559  |  0:00:01s\n",
            "epoch 39 | loss: 0.65315 |  0:00:01s\n",
            "epoch 40 | loss: 0.65212 |  0:00:02s\n",
            "epoch 41 | loss: 0.65438 |  0:00:02s\n",
            "epoch 42 | loss: 0.65336 |  0:00:02s\n",
            "epoch 43 | loss: 0.65686 |  0:00:02s\n",
            "epoch 44 | loss: 0.65681 |  0:00:02s\n",
            "epoch 45 | loss: 0.65714 |  0:00:02s\n",
            "epoch 46 | loss: 0.65902 |  0:00:02s\n",
            "epoch 47 | loss: 0.64796 |  0:00:02s\n",
            "epoch 48 | loss: 0.65183 |  0:00:02s\n",
            "epoch 49 | loss: 0.65885 |  0:00:02s\n",
            "epoch 50 | loss: 0.65517 |  0:00:02s\n",
            "epoch 51 | loss: 0.65815 |  0:00:02s\n",
            "epoch 52 | loss: 0.65451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65089 |  0:00:02s\n",
            "epoch 54 | loss: 0.6525  |  0:00:02s\n",
            "epoch 55 | loss: 0.6526  |  0:00:02s\n",
            "epoch 56 | loss: 0.6611  |  0:00:02s\n",
            "epoch 57 | loss: 0.65559 |  0:00:02s\n",
            "epoch 58 | loss: 0.65667 |  0:00:02s\n",
            "epoch 59 | loss: 0.65166 |  0:00:02s\n",
            "epoch 60 | loss: 0.6541  |  0:00:02s\n",
            "epoch 61 | loss: 0.64848 |  0:00:02s\n",
            "epoch 62 | loss: 0.65481 |  0:00:03s\n",
            "epoch 63 | loss: 0.65333 |  0:00:03s\n",
            "epoch 64 | loss: 0.65386 |  0:00:03s\n",
            "epoch 65 | loss: 0.64945 |  0:00:03s\n",
            "epoch 66 | loss: 0.64804 |  0:00:03s\n",
            "epoch 67 | loss: 0.65154 |  0:00:03s\n",
            "epoch 68 | loss: 0.65439 |  0:00:03s\n",
            "epoch 69 | loss: 0.65606 |  0:00:03s\n",
            "epoch 70 | loss: 0.65102 |  0:00:03s\n",
            "epoch 71 | loss: 0.65272 |  0:00:03s\n",
            "epoch 72 | loss: 0.65625 |  0:00:03s\n",
            "epoch 73 | loss: 0.65476 |  0:00:03s\n",
            "epoch 74 | loss: 0.64848 |  0:00:03s\n",
            "epoch 75 | loss: 0.65245 |  0:00:03s\n",
            "epoch 76 | loss: 0.65572 |  0:00:03s\n",
            "epoch 77 | loss: 0.66178 |  0:00:03s\n",
            "epoch 78 | loss: 0.65548 |  0:00:03s\n",
            "epoch 79 | loss: 0.65366 |  0:00:03s\n",
            "epoch 80 | loss: 0.66175 |  0:00:03s\n",
            "epoch 81 | loss: 0.65087 |  0:00:03s\n",
            "epoch 82 | loss: 0.65403 |  0:00:03s\n",
            "epoch 83 | loss: 0.654   |  0:00:03s\n",
            "epoch 84 | loss: 0.65244 |  0:00:03s\n",
            "epoch 85 | loss: 0.66005 |  0:00:04s\n",
            "epoch 86 | loss: 0.65493 |  0:00:04s\n",
            "epoch 87 | loss: 0.64949 |  0:00:04s\n",
            "epoch 88 | loss: 0.64943 |  0:00:04s\n",
            "epoch 89 | loss: 0.65977 |  0:00:04s\n",
            "epoch 90 | loss: 0.64923 |  0:00:04s\n",
            "epoch 91 | loss: 0.6526  |  0:00:04s\n",
            "epoch 92 | loss: 0.65189 |  0:00:04s\n",
            "epoch 93 | loss: 0.65097 |  0:00:04s\n",
            "epoch 94 | loss: 0.65219 |  0:00:04s\n",
            "epoch 95 | loss: 0.64573 |  0:00:04s\n",
            "epoch 96 | loss: 0.65529 |  0:00:04s\n",
            "epoch 97 | loss: 0.65213 |  0:00:04s\n",
            "epoch 98 | loss: 0.65517 |  0:00:04s\n",
            "epoch 99 | loss: 0.65794 |  0:00:04s\n",
            "Training completed for Fold_4 with AUC: 0.634548889031184\n",
            "0.6385634246774857\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "#######Your code for deep learning model#########\n",
        "\n",
        "estimator = SklearnTabNet(\n",
        "    n_d=8,\n",
        "    n_a=8,\n",
        "    n_steps=3,\n",
        "    gamma=1.3,\n",
        "    cat_idxs=[X.columns.get_loc(c) for c in cats],\n",
        "    cat_dims=[2] * len(cats),  # Assuming binary categorical features\n",
        "    cat_emb_dim=1,\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=2e-2),\n",
        "    scheduler_params=dict(step_size=50, gamma=0.9),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    verbose=0\n",
        ")\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  0%|          | 0/50 [00:00<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:54:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82129 |  0:00:00s                  \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                  \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                  \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                  \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                  \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                  \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                  \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                  \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                  \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                  \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                  \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                  \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                  \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                  \n",
            "epoch 14 | loss: 0.66369 |  0:00:01s                  \n",
            "epoch 15 | loss: 0.66042 |  0:00:01s                  \n",
            "epoch 16 | loss: 0.66606 |  0:00:01s                  \n",
            "epoch 17 | loss: 0.6549  |  0:00:01s                  \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                  \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                  \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                  \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                  \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                  \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                  \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                  \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                  \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                  \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                  \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                  \n",
            "epoch 29 | loss: 0.66681 |  0:00:02s                  \n",
            "epoch 30 | loss: 0.66132 |  0:00:02s                  \n",
            "epoch 31 | loss: 0.66383 |  0:00:02s                  \n",
            "epoch 32 | loss: 0.66425 |  0:00:02s                  \n",
            "epoch 33 | loss: 0.65932 |  0:00:02s                  \n",
            "epoch 34 | loss: 0.66514 |  0:00:02s                  \n",
            "epoch 35 | loss: 0.66176 |  0:00:02s                  \n",
            "epoch 36 | loss: 0.66962 |  0:00:02s                  \n",
            "epoch 37 | loss: 0.66413 |  0:00:02s                  \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                  \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                  \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                  \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                  \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                  \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                  \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                  \n",
            "epoch 45 | loss: 0.66274 |  0:00:03s                  \n",
            "epoch 46 | loss: 0.66234 |  0:00:03s                  \n",
            "epoch 47 | loss: 0.66805 |  0:00:03s                  \n",
            "epoch 48 | loss: 0.66645 |  0:00:03s                  \n",
            "epoch 49 | loss: 0.6657  |  0:00:03s                  \n",
            "epoch 50 | loss: 0.6629  |  0:00:03s                  \n",
            "epoch 51 | loss: 0.65916 |  0:00:03s                  \n",
            "epoch 52 | loss: 0.66155 |  0:00:03s                  \n",
            "epoch 53 | loss: 0.66205 |  0:00:03s                  \n",
            "epoch 54 | loss: 0.65815 |  0:00:03s                  \n",
            "epoch 55 | loss: 0.66152 |  0:00:03s                  \n",
            "epoch 56 | loss: 0.66028 |  0:00:04s                  \n",
            "epoch 57 | loss: 0.66463 |  0:00:04s                  \n",
            "epoch 58 | loss: 0.65953 |  0:00:04s                  \n",
            "epoch 59 | loss: 0.66682 |  0:00:04s                  \n",
            "epoch 60 | loss: 0.66321 |  0:00:04s                  \n",
            "epoch 61 | loss: 0.66373 |  0:00:04s                  \n",
            "epoch 62 | loss: 0.66355 |  0:00:04s                  \n",
            "epoch 63 | loss: 0.66078 |  0:00:04s                  \n",
            "epoch 64 | loss: 0.66516 |  0:00:04s                  \n",
            "epoch 65 | loss: 0.66152 |  0:00:04s                  \n",
            "epoch 66 | loss: 0.66482 |  0:00:04s                  \n",
            "epoch 67 | loss: 0.66327 |  0:00:04s                  \n",
            "epoch 68 | loss: 0.66354 |  0:00:04s                  \n",
            "epoch 69 | loss: 0.66134 |  0:00:04s                  \n",
            "epoch 70 | loss: 0.66747 |  0:00:04s                  \n",
            "epoch 71 | loss: 0.65962 |  0:00:04s                  \n",
            "epoch 72 | loss: 0.66135 |  0:00:05s                  \n",
            "epoch 73 | loss: 0.66312 |  0:00:05s                  \n",
            "epoch 74 | loss: 0.66194 |  0:00:05s                  \n",
            "epoch 75 | loss: 0.66819 |  0:00:05s                  \n",
            "epoch 76 | loss: 0.66151 |  0:00:05s                  \n",
            "epoch 77 | loss: 0.65716 |  0:00:05s                  \n",
            "epoch 78 | loss: 0.66099 |  0:00:05s                  \n",
            "epoch 79 | loss: 0.6632  |  0:00:05s                  \n",
            "epoch 80 | loss: 0.6629  |  0:00:05s                  \n",
            "epoch 81 | loss: 0.66266 |  0:00:05s                  \n",
            "epoch 82 | loss: 0.65956 |  0:00:05s                  \n",
            "epoch 83 | loss: 0.6576  |  0:00:05s                  \n",
            "epoch 84 | loss: 0.66103 |  0:00:05s                  \n",
            "epoch 85 | loss: 0.66449 |  0:00:05s                  \n",
            "epoch 86 | loss: 0.6628  |  0:00:05s                  \n",
            "epoch 87 | loss: 0.66488 |  0:00:06s                  \n",
            "epoch 88 | loss: 0.6673  |  0:00:06s                  \n",
            "epoch 89 | loss: 0.66309 |  0:00:06s                  \n",
            "epoch 90 | loss: 0.66404 |  0:00:06s                  \n",
            "epoch 91 | loss: 0.66472 |  0:00:06s                  \n",
            "epoch 92 | loss: 0.66572 |  0:00:06s                  \n",
            "epoch 93 | loss: 0.66462 |  0:00:06s                  \n",
            "epoch 94 | loss: 0.66543 |  0:00:06s                  \n",
            "epoch 95 | loss: 0.66423 |  0:00:06s                  \n",
            "epoch 96 | loss: 0.66119 |  0:00:06s                  \n",
            "epoch 97 | loss: 0.66243 |  0:00:06s                  \n",
            "epoch 98 | loss: 0.66079 |  0:00:06s                  \n",
            "epoch 99 | loss: 0.66241 |  0:00:06s                  \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                  \n",
            "  0%|          | 0/50 [00:07<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:54:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.66669 |  0:00:00s                  \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                  \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                  \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                  \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                  \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                  \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                  \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                  \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                  \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                  \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                  \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                  \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                  \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                  \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                  \n",
            "epoch 16 | loss: 0.65753 |  0:00:01s                  \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                  \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                  \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                  \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                  \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                  \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                  \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                  \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                  \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                  \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                  \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                  \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                  \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                  \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                  \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                  \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                  \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                  \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                  \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                  \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                  \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                  \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                  \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                  \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                  \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                  \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                  \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                  \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                  \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                  \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                  \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                  \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                  \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                  \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                  \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                  \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                  \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                  \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                  \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                  \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                  \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                  \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                  \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                  \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                  \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                  \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                  \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                  \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                  \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                  \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                  \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                  \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                  \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                  \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                  \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                  \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                  \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                  \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                  \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                  \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                  \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                  \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                  \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                  \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                  \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                  \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                  \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                  \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                  \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                  \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                  \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                  \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                  \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                  \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                  \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                  \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                  \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                  \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                  \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                  \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                  \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                  \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                  \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                  \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566\n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                  \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                  \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                  \n",
            "  0%|          | 0/50 [00:12<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                  \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                  \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                  \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                  \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                  \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                  \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                  \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                  \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                  \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                  \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                  \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                  \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                  \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                  \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                  \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                  \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                  \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                  \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                  \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                  \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                  \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                  \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                  \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                  \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                  \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                  \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                  \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                  \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                  \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                  \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                  \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                  \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                  \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                  \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                  \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                  \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                  \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                  \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                  \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                  \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                  \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                  \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                  \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                  \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                  \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                  \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                  \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                  \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                  \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                  \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                  \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                  \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                  \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                  \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                  \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                  \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                  \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                  \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                  \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                  \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                  \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                  \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                  \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                  \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                  \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                  \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                  \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                  \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                  \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                  \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                  \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                  \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                  \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                  \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                  \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                  \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                  \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                  \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                  \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                  \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                  \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                  \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                  \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                  \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                  \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                  \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                  \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                  \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                  \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                  \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                  \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                  \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                  \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                  \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                  \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                  \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                  \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311\n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                  \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                  \n",
            "  0%|          | 0/50 [00:17<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                  \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                  \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                  \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                  \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                  \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                  \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                  \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                  \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                  \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                  \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                  \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                  \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                  \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                  \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                  \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                  \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                  \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                  \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                  \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                  \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                  \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                  \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                  \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                  \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                  \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                  \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                  \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                  \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                  \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                  \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                  \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                  \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                  \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                  \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                  \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                  \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                  \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                  \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                  \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                  \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                  \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                  \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                  \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                  \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                  \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                  \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                  \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                  \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                  \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                  \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                  \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                  \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                  \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                  \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                  \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                  \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                  \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                  \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                  \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                  \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                  \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                  \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                  \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                  \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                  \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                  \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                  \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                  \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                  \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                  \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                  \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                  \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                  \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                  \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                  \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                  \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                  \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                  \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                  \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                  \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                  \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                  \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                  \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                  \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                  \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                  \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                  \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                  \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                  \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                  \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                  \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                  \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                  \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                  \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                  \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                  \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                  \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                  \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492\n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                  \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                  \n",
            "  0%|          | 0/50 [00:22<?, ?trial/s, best loss=?]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                  \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                  \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                  \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                  \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                  \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                  \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                  \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                  \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                  \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                  \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                  \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                  \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                  \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                  \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                  \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                  \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                  \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                  \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                  \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                  \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                  \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                  \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                  \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                  \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                  \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                  \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                  \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                  \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                  \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                  \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                  \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                  \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                  \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                  \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                  \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                  \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                  \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                  \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                  \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                  \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                  \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                  \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                  \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                  \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                  \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                  \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                  \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                  \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                  \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                  \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                  \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                  \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                  \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                  \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                  \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                  \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                  \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                  \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                  \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                  \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                  \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                  \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                  \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                  \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                  \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                  \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                  \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                  \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                  \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                  \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                  \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                  \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                  \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                  \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                  \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                  \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                  \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                  \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                  \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                  \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                  \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                  \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                  \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                  \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                  \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                  \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                  \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                  \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                  \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                  \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                  \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                  \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                  \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                  \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                  \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                  \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                  \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                  \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184\n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "  2%|▏         | 1/50 [00:27<22:14, 27.24s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:01s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:02s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:02s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "  2%|▏         | 1/50 [00:32<22:14, 27.24s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            "  2%|▏         | 1/50 [00:37<22:14, 27.24s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            "  2%|▏         | 1/50 [00:42<22:14, 27.24s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            "  2%|▏         | 1/50 [00:47<22:14, 27.24s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:04s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:04s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:05s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:05s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:05s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            "  4%|▍         | 2/50 [00:52<20:58, 26.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "  4%|▍         | 2/50 [00:57<20:58, 26.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            "  4%|▍         | 2/50 [01:03<20:58, 26.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            "  4%|▍         | 2/50 [01:08<20:58, 26.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:55:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "  4%|▍         | 2/50 [01:12<20:58, 26.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:01s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:01s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:01s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:02s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            "                                                                                 \r"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s\n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "  6%|▌         | 3/50 [01:23<20:13, 25.81s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            "  6%|▌         | 3/50 [01:28<20:13, 25.81s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            "  6%|▌         | 3/50 [01:33<20:13, 25.81s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:01s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:01s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:01s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:02s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:02s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:03s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:04s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:04s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            "  6%|▌         | 3/50 [01:38<20:13, 25.81s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            "  8%|▊         | 4/50 [01:43<19:36, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "  8%|▊         | 4/50 [01:48<19:36, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            "  8%|▊         | 4/50 [01:53<19:36, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:02s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:02s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            "  8%|▊         | 4/50 [01:58<19:36, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            "  8%|▊         | 4/50 [02:03<19:36, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            " 10%|█         | 5/50 [02:08<19:04, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:56:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            " 10%|█         | 5/50 [02:13<19:04, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            " 10%|█         | 5/50 [02:18<19:04, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            " 10%|█         | 5/50 [02:23<19:04, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            " 10%|█         | 5/50 [02:28<19:04, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            " 12%|█▏        | 6/50 [02:33<18:29, 25.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            " 12%|█▏        | 6/50 [02:38<18:29, 25.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            " 12%|█▏        | 6/50 [02:43<18:29, 25.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            " 12%|█▏        | 6/50 [02:48<18:29, 25.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            " 12%|█▏        | 6/50 [02:53<18:29, 25.22s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            " 14%|█▍        | 7/50 [02:58<17:59, 25.10s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            " 14%|█▍        | 7/50 [03:03<17:59, 25.10s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            " 14%|█▍        | 7/50 [03:08<17:59, 25.10s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:57:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            " 14%|█▍        | 7/50 [03:12<17:59, 25.10s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            " 14%|█▍        | 7/50 [03:17<17:59, 25.10s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:01s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:01s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:01s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:02s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:02s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            " 16%|█▌        | 8/50 [03:23<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            " 16%|█▌        | 8/50 [03:28<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            " 16%|█▌        | 8/50 [03:29<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            " 16%|█▌        | 8/50 [03:34<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            " 16%|█▌        | 8/50 [03:39<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            " 16%|█▌        | 8/50 [03:44<17:31, 25.03s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                             \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                             \n",
            " 18%|█▊        | 9/50 [03:49<17:22, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                             \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                       \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                             \n",
            " 18%|█▊        | 9/50 [03:54<17:22, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                             \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                       \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                             \n",
            " 18%|█▊        | 9/50 [03:59<17:22, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                             \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                             \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                             \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                             \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                        \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                             \n",
            " 18%|█▊        | 9/50 [04:04<17:22, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                             \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                             \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                             \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                             \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                             \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                             \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                       \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                             \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                             \n",
            " 18%|█▊        | 9/50 [04:09<17:22, 25.43s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:58:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                             \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                             \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                             \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                             \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                             \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                             \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                             \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                             \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                             \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                             \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                             \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                             \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                             \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                             \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                             \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                             \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                             \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                             \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                             \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                             \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                             \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                             \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                             \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                             \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                             \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                             \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                             \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                             \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                             \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                             \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                             \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                             \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                             \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                             \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                             \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                             \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                             \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                             \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                             \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                             \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                             \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                             \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                             \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                             \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                             \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                             \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                             \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                             \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                             \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                             \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                             \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                             \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                             \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                             \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                             \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                             \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                             \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                             \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                             \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                             \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                             \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                             \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                             \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                             \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                             \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                             \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                             \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                             \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                             \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                             \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                             \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                             \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                             \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                             \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                             \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                             \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                             \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                             \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                             \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                             \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                             \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                             \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                             \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                             \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                             \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                             \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                             \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                             \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                             \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                             \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                             \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                             \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                             \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                             \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                             \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                             \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                             \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                        \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 20%|██        | 10/50 [04:14<16:49, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 20%|██        | 10/50 [04:19<16:49, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 20%|██        | 10/50 [04:24<16:49, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 20%|██        | 10/50 [04:29<16:49, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 20%|██        | 10/50 [04:34<16:49, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 22%|██▏       | 11/50 [04:39<16:19, 25.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 22%|██▏       | 11/50 [04:44<16:19, 25.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 22%|██▏       | 11/50 [04:48<16:19, 25.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            " 22%|██▏       | 11/50 [04:53<16:19, 25.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 22%|██▏       | 11/50 [04:59<16:19, 25.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 24%|██▍       | 12/50 [05:04<15:52, 25.06s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 24%|██▍       | 12/50 [05:09<15:52, 25.06s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [16:59:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 24%|██▍       | 12/50 [05:13<15:52, 25.06s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 24%|██▍       | 12/50 [05:19<15:52, 25.06s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 24%|██▍       | 12/50 [05:23<15:52, 25.06s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 26%|██▌       | 13/50 [05:28<15:25, 25.01s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 26%|██▌       | 13/50 [05:34<15:25, 25.01s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 26%|██▌       | 13/50 [05:39<15:25, 25.01s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 26%|██▌       | 13/50 [05:44<15:25, 25.01s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 26%|██▌       | 13/50 [05:49<15:25, 25.01s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 28%|██▊       | 14/50 [05:54<15:03, 25.09s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 28%|██▊       | 14/50 [05:59<15:03, 25.09s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 28%|██▊       | 14/50 [06:04<15:03, 25.09s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:05s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:05s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            " 28%|██▊       | 14/50 [06:09<15:03, 25.09s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:00:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 28%|██▊       | 14/50 [06:14<15:03, 25.09s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 30%|███       | 15/50 [06:19<14:44, 25.27s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 30%|███       | 15/50 [06:24<14:44, 25.27s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 30%|███       | 15/50 [06:30<14:44, 25.27s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 30%|███       | 15/50 [06:35<14:44, 25.27s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 30%|███       | 15/50 [06:40<14:44, 25.27s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 32%|███▏      | 16/50 [06:45<14:17, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            " 32%|███▏      | 16/50 [06:50<14:17, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 32%|███▏      | 16/50 [06:55<14:17, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 32%|███▏      | 16/50 [07:00<14:17, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 32%|███▏      | 16/50 [07:05<14:17, 25.23s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:01:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 34%|███▍      | 17/50 [07:10<13:53, 25.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 34%|███▍      | 17/50 [07:15<13:53, 25.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 34%|███▍      | 17/50 [07:20<13:53, 25.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 34%|███▍      | 17/50 [07:25<13:53, 25.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 34%|███▍      | 17/50 [07:30<13:53, 25.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 36%|███▌      | 18/50 [07:35<13:26, 25.19s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:25] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 36%|███▌      | 18/50 [07:40<13:26, 25.19s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 36%|███▌      | 18/50 [07:45<13:26, 25.19s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 36%|███▌      | 18/50 [07:50<13:26, 25.19s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 36%|███▌      | 18/50 [07:56<13:26, 25.19s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 38%|███▊      | 19/50 [08:01<13:07, 25.41s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 38%|███▊      | 19/50 [08:06<13:07, 25.41s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:02:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 38%|███▊      | 19/50 [08:11<13:07, 25.41s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 38%|███▊      | 19/50 [08:16<13:07, 25.41s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 38%|███▊      | 19/50 [08:21<13:07, 25.41s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 40%|████      | 20/50 [08:26<12:41, 25.39s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 40%|████      | 20/50 [08:31<12:41, 25.39s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 40%|████      | 20/50 [08:36<12:41, 25.39s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 40%|████      | 20/50 [08:41<12:41, 25.39s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 40%|████      | 20/50 [08:47<12:41, 25.39s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 42%|████▏     | 21/50 [08:52<12:17, 25.44s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 42%|████▏     | 21/50 [08:57<12:17, 25.44s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 42%|████▏     | 21/50 [09:02<12:17, 25.44s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            " 42%|████▏     | 21/50 [09:07<12:17, 25.44s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:03:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:01s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:03s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 42%|████▏     | 21/50 [09:12<12:17, 25.44s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 44%|████▍     | 22/50 [09:17<11:53, 25.47s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 44%|████▍     | 22/50 [09:22<11:53, 25.47s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 44%|████▍     | 22/50 [09:27<11:53, 25.47s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 44%|████▍     | 22/50 [09:33<11:53, 25.47s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 44%|████▍     | 22/50 [09:38<11:53, 25.47s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 46%|████▌     | 23/50 [09:43<11:29, 25.54s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:33] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 46%|████▌     | 23/50 [09:48<11:29, 25.54s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 46%|████▌     | 23/50 [09:53<11:29, 25.54s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 46%|████▌     | 23/50 [09:58<11:29, 25.54s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 46%|████▌     | 23/50 [10:03<11:29, 25.54s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:54] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 48%|████▊     | 24/50 [10:09<11:03, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:04:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 48%|████▊     | 24/50 [10:14<11:03, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 48%|████▊     | 24/50 [10:19<11:03, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 48%|████▊     | 24/50 [10:24<11:03, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 48%|████▊     | 24/50 [10:29<11:03, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 50%|█████     | 25/50 [10:34<10:40, 25.61s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 50%|█████     | 25/50 [10:39<10:40, 25.61s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            " 50%|█████     | 25/50 [10:44<10:40, 25.61s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:01s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 50%|█████     | 25/50 [10:50<10:40, 25.61s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:40] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 50%|█████     | 25/50 [10:55<10:40, 25.61s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 52%|█████▏    | 26/50 [11:00<10:15, 25.64s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 52%|█████▏    | 26/50 [11:05<10:15, 25.64s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:05:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 52%|█████▏    | 26/50 [11:10<10:15, 25.64s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 52%|█████▏    | 26/50 [11:15<10:15, 25.64s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 52%|█████▏    | 26/50 [11:20<10:15, 25.64s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 54%|█████▍    | 27/50 [11:25<09:48, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 54%|█████▍    | 27/50 [11:30<09:48, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:02s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 54%|█████▍    | 27/50 [11:36<09:48, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 54%|█████▍    | 27/50 [11:41<09:48, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 54%|█████▍    | 27/50 [11:46<09:48, 25.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 56%|█████▌    | 28/50 [11:51<09:22, 25.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 56%|█████▌    | 28/50 [11:56<09:22, 25.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 56%|█████▌    | 28/50 [12:01<09:22, 25.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:51] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 56%|█████▌    | 28/50 [12:06<09:22, 25.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:06:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 56%|█████▌    | 28/50 [12:11<09:22, 25.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 58%|█████▊    | 29/50 [12:16<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:07] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            " 58%|█████▊    | 29/50 [12:21<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:12] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:01s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            " 58%|█████▊    | 29/50 [12:27<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            " 58%|█████▊    | 29/50 [12:32<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            " 58%|█████▊    | 29/50 [12:32<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 58%|█████▊    | 29/50 [12:37<08:56, 25.53s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 60%|██████    | 30/50 [12:42<08:31, 25.59s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            " 60%|██████    | 30/50 [12:47<08:31, 25.59s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 60%|██████    | 30/50 [12:48<08:31, 25.59s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 60%|██████    | 30/50 [12:53<08:31, 25.59s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 60%|██████    | 30/50 [12:57<08:31, 25.59s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "                                                                                  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s\n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:02s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:02s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:03s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 62%|██████▏   | 31/50 [13:06<07:56, 25.07s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:07:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:01s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 62%|██████▏   | 31/50 [13:11<07:56, 25.07s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:01] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 62%|██████▏   | 31/50 [13:15<07:56, 25.07s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 62%|██████▏   | 31/50 [13:19<07:56, 25.07s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            " 62%|██████▏   | 31/50 [13:24<07:56, 25.07s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 64%|██████▍   | 32/50 [13:28<07:16, 24.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 64%|██████▍   | 32/50 [13:33<07:16, 24.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 64%|██████▍   | 32/50 [13:38<07:16, 24.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 64%|██████▍   | 32/50 [13:42<07:16, 24.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 64%|██████▍   | 32/50 [13:46<07:16, 24.25s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            " 66%|██████▌   | 33/50 [13:51<06:43, 23.73s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 66%|██████▌   | 33/50 [13:56<06:43, 23.73s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 66%|██████▌   | 33/50 [14:00<06:43, 23.73s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 66%|██████▌   | 33/50 [14:04<06:43, 23.73s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            " 66%|██████▌   | 33/50 [14:09<06:43, 23.73s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:08:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 68%|██████▊   | 34/50 [14:14<06:14, 23.40s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 68%|██████▊   | 34/50 [14:18<06:14, 23.40s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 68%|██████▊   | 34/50 [14:23<06:14, 23.40s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 68%|██████▊   | 34/50 [14:27<06:14, 23.40s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:17] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            " 68%|██████▊   | 34/50 [14:32<06:14, 23.40s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 70%|███████   | 35/50 [14:36<05:46, 23.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 70%|███████   | 35/50 [14:40<05:46, 23.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:30] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 70%|███████   | 35/50 [14:45<05:46, 23.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:35] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:02s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 70%|███████   | 35/50 [14:49<05:46, 23.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            " 70%|███████   | 35/50 [14:54<05:46, 23.11s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 72%|███████▏  | 36/50 [14:58<05:20, 22.88s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 72%|███████▏  | 36/50 [15:03<05:20, 22.88s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 72%|███████▏  | 36/50 [15:07<05:20, 22.88s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:09:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 72%|███████▏  | 36/50 [15:12<05:20, 22.88s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:02] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 72%|███████▏  | 36/50 [15:16<05:20, 22.88s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:06] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 74%|███████▍  | 37/50 [15:21<04:56, 22.80s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 74%|███████▍  | 37/50 [15:26<04:56, 22.80s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 74%|███████▍  | 37/50 [15:30<04:56, 22.80s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 74%|███████▍  | 37/50 [15:34<04:56, 22.80s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:24] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 74%|███████▍  | 37/50 [15:39<04:56, 22.80s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 76%|███████▌  | 38/50 [15:43<04:32, 22.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 76%|███████▌  | 38/50 [15:48<04:32, 22.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:38] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 76%|███████▌  | 38/50 [15:52<04:32, 22.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 76%|███████▌  | 38/50 [15:57<04:32, 22.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 76%|███████▌  | 38/50 [16:01<04:32, 22.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 78%|███████▊  | 39/50 [16:06<04:09, 22.65s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:10:56] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 78%|███████▊  | 39/50 [16:10<04:09, 22.65s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 78%|███████▊  | 39/50 [16:15<04:09, 22.65s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 78%|███████▊  | 39/50 [16:20<04:09, 22.65s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:10] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 78%|███████▊  | 39/50 [16:24<04:09, 22.65s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:14] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 80%|████████  | 40/50 [16:29<03:46, 22.68s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:19] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 80%|████████  | 40/50 [16:33<03:46, 22.68s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 80%|████████  | 40/50 [16:38<03:46, 22.68s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:28] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 80%|████████  | 40/50 [16:42<03:46, 22.68s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            " 80%|████████  | 40/50 [16:47<03:46, 22.68s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:00s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            " 82%|████████▏ | 41/50 [16:51<03:23, 22.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 82%|████████▏ | 41/50 [16:55<03:23, 22.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:46] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 82%|████████▏ | 41/50 [17:00<03:23, 22.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 82%|████████▏ | 41/50 [17:05<03:23, 22.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:03s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:03s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:03s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 82%|████████▏ | 41/50 [17:09<03:23, 22.57s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:11:59] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:01s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:02s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:02s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 84%|████████▍ | 42/50 [17:14<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:04] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:03s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:04s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:04s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:04s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:05s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:05s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:05s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:05s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:05s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:05s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 84%|████████▍ | 42/50 [17:19<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:09] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:05s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:05s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:05s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:05s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            " 84%|████████▍ | 42/50 [17:25<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:15] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            " 84%|████████▍ | 42/50 [17:30<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:20] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 84%|████████▍ | 42/50 [17:31<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 84%|████████▍ | 42/50 [17:36<03:00, 22.58s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:26] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 86%|████████▌ | 43/50 [17:41<02:48, 24.05s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 86%|████████▌ | 43/50 [17:46<02:48, 24.05s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 86%|████████▌ | 43/50 [17:52<02:48, 24.05s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:42] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 86%|████████▌ | 43/50 [17:57<02:48, 24.05s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:47] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 86%|████████▌ | 43/50 [18:02<02:48, 24.05s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:52] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 88%|████████▊ | 44/50 [18:07<02:28, 24.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:12:57] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 88%|████████▊ | 44/50 [18:13<02:28, 24.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 88%|████████▊ | 44/50 [18:18<02:28, 24.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 88%|████████▊ | 44/50 [18:23<02:28, 24.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 88%|████████▊ | 44/50 [18:28<02:28, 24.72s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            " 90%|█████████ | 45/50 [18:33<02:05, 25.12s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:23] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 90%|█████████ | 45/50 [18:38<02:05, 25.12s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:29] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 90%|█████████ | 45/50 [18:44<02:05, 25.12s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:34] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 90%|█████████ | 45/50 [18:49<02:05, 25.12s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:39] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 90%|█████████ | 45/50 [18:54<02:05, 25.12s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:44] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            "                                                                                  "
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:49] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s\n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 92%|█████████▏| 46/50 [19:05<01:41, 25.37s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:13:55] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 92%|█████████▏| 46/50 [19:10<01:41, 25.37s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:00] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            " 92%|█████████▏| 46/50 [19:15<01:41, 25.37s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:05] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:01s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:02s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:03s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:03s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:04s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:04s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:05s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:05s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:05s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 92%|█████████▏| 46/50 [19:21<01:41, 25.37s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:11] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 94%|█████████▍| 47/50 [19:26<01:17, 25.74s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:16] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            " 94%|█████████▍| 47/50 [19:31<01:17, 25.74s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:21] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 94%|█████████▍| 47/50 [19:37<01:17, 25.74s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:01s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:01s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:02s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:02s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:02s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:02s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:02s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:03s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:03s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:03s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:03s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:03s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:04s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:04s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:05s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:05s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:05s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 94%|█████████▍| 47/50 [19:42<01:17, 25.74s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:32] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:05s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 94%|█████████▍| 47/50 [19:47<01:17, 25.74s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:37] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:05s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 96%|█████████▌| 48/50 [19:53<00:51, 25.98s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:43] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:05s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            " 96%|█████████▌| 48/50 [19:58<00:51, 25.98s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:48] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:01s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:04s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:04s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:04s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:04s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:04s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:05s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:05s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:05s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:05s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:05s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            " 96%|█████████▌| 48/50 [20:03<00:51, 25.98s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:53] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            " 96%|█████████▌| 48/50 [20:08<00:51, 25.98s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:14:58] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:01s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:02s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:02s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:03s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:03s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:04s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:04s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 96%|█████████▌| 48/50 [20:13<00:51, 25.98s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:03] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:01s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:02s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:02s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:02s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:03s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:03s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:03s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:03s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:04s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:04s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:04s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:04s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:04s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:04s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:04s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "epoch 0  | loss: 0.82129 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.69413 |  0:00:00s                                              \n",
            " 98%|█████████▊| 49/50 [20:18<00:25, 25.93s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:08] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.68169 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.67449 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.66989 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66785 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66801 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.6692  |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.67093 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66066 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66997 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.66499 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6652  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66063 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66369 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66042 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66606 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.6549  |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66593 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.66275 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.67011 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.66674 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.66347 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66219 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.66776 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.66323 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66517 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66195 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66681 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66132 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66383 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.66425 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65932 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.66514 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66176 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66962 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66413 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.66864 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66556 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.66459 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.66431 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66639 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66566 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.66461 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.66274 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66234 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66805 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66645 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6657  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6629  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65916 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66155 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66205 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66028 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66463 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65953 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66682 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.66321 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.66373 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66355 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.66078 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.66516 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.66152 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66482 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66327 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.66354 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66134 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66747 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65962 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.66135 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.66312 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66194 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.66819 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.66099 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.6632  |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.6629  |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.66266 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65956 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.6576  |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66103 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66449 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.6628  |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.66488 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.6673  |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.66309 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.66404 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66472 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.66572 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66462 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66543 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66423 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.66119 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.66243 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66079 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.66241 |  0:00:04s                                              \n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276                        \n",
            "epoch 0  | loss: 0.79829 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.66669 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.66416 |  0:00:00s                                              \n",
            " 98%|█████████▊| 49/50 [20:23<00:25, 25.93s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:13] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65561 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65932 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66116 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65419 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65517 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66334 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65327 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65971 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.659   |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66051 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65711 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.6592  |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65753 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66103 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66029 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65484 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65865 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65416 |  0:00:01s                                              \n",
            "epoch 22 | loss: 0.65564 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65826 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65388 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65951 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65773 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.6569  |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66149 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66262 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.66118 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65915 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65585 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65769 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.6549  |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65304 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66283 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66036 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65727 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65843 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65764 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65725 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66286 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66094 |  0:00:02s                                              \n",
            "epoch 44 | loss: 0.65994 |  0:00:02s                                              \n",
            "epoch 45 | loss: 0.65736 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66132 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65876 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.655   |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.6546  |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6517  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65076 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.66134 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65962 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65704 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65682 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65616 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66184 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.66032 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.6559  |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65696 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65936 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65319 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65951 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.6624  |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65389 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.65476 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.6564  |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.6578  |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65999 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66151 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65282 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65757 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65051 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65598 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65955 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65574 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66164 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65796 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65643 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65466 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65868 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66234 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65762 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65425 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65816 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65813 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.66011 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65716 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65569 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6534  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65997 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65685 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65627 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65819 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65828 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65309 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65704 |  0:00:04s                                              \n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566                        \n",
            "epoch 0  | loss: 0.74116 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.70513 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.69432 |  0:00:00s                                              \n",
            " 98%|█████████▊| 49/50 [20:28<00:25, 25.93s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:18] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.67477 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67408 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.66448 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66114 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.67036 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.66511 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66068 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66214 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.6624  |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66165 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66099 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66227 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.66577 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66433 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.6621  |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.6587  |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65557 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65922 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.66284 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65952 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.66066 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65914 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65793 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.66025 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.66212 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.66159 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65868 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.66358 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65803 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65629 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.658   |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.66108 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.66254 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.66275 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65842 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.66139 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65955 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65772 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.66374 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.66061 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.6623  |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65957 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.66121 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.66013 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.66283 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.66251 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65744 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.66202 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65722 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.66317 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65923 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65988 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.66293 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.66161 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65706 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.66115 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65831 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65972 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.66007 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65941 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65281 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65539 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.66362 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.66359 |  0:00:02s                                              \n",
            "epoch 68 | loss: 0.66279 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.66138 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.66039 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65481 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65951 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65521 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.66144 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.66008 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66101 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65731 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65836 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65784 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.66167 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.66068 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.66122 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.65947 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65888 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.65789 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65418 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.65631 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.66406 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65971 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.66105 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.66296 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.66138 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65509 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65947 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.66348 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65617 |  0:00:04s                                              \n",
            "Training completed for Fold_2 with AUC: 0.632787634900311                         \n",
            "epoch 0  | loss: 0.82813 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.67173 |  0:00:00s                                              \n",
            "epoch 2  | loss: 0.65725 |  0:00:00s                                              \n",
            " 98%|█████████▊| 49/50 [20:32<00:25, 25.93s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:22] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 3  | loss: 0.65813 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.65162 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.65938 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.65699 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65609 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65957 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.65455 |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.65117 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.64928 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.6484  |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.65202 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.65083 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65236 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.65407 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.65722 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.65453 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65006 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65286 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.64635 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.65601 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65498 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65045 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.64957 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.64914 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.64831 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65263 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65653 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.6524  |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.64977 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.6519  |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65507 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65584 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.64758 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.64753 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.6508  |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.65147 |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65669 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65063 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65125 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65242 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65139 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.64979 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65282 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.64992 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.65172 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65339 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65177 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.6483  |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65488 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65112 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.64882 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.65205 |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.65058 |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.65775 |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65229 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65413 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65194 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.65233 |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.65317 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65004 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65206 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65329 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.65854 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64967 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65222 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65413 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.64616 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65072 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65243 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65491 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.64967 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.65569 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.64423 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.64815 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.64981 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65015 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.64704 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.65555 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65336 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.64887 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.65124 |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.64734 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.64845 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65064 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.6533  |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.65552 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.64917 |  0:00:04s                                              \n",
            "epoch 90 | loss: 0.6497  |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.64727 |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65358 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65438 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.64729 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.65835 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65608 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.64505 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65118 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.6487  |  0:00:04s                                              \n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492                        \n",
            "epoch 0  | loss: 1.16024 |  0:00:00s                                              \n",
            "epoch 1  | loss: 0.71131 |  0:00:00s                                              \n",
            " 98%|█████████▊| 49/50 [20:37<00:25, 25.93s/trial, best loss: -0.6385634246774857]"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:27] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 2  | loss: 0.70045 |  0:00:00s                                              \n",
            "epoch 3  | loss: 0.69166 |  0:00:00s                                              \n",
            "epoch 4  | loss: 0.67844 |  0:00:00s                                              \n",
            "epoch 5  | loss: 0.67201 |  0:00:00s                                              \n",
            "epoch 6  | loss: 0.66742 |  0:00:00s                                              \n",
            "epoch 7  | loss: 0.65627 |  0:00:00s                                              \n",
            "epoch 8  | loss: 0.65846 |  0:00:00s                                              \n",
            "epoch 9  | loss: 0.66    |  0:00:00s                                              \n",
            "epoch 10 | loss: 0.66268 |  0:00:00s                                              \n",
            "epoch 11 | loss: 0.65486 |  0:00:00s                                              \n",
            "epoch 12 | loss: 0.66073 |  0:00:00s                                              \n",
            "epoch 13 | loss: 0.66184 |  0:00:00s                                              \n",
            "epoch 14 | loss: 0.66056 |  0:00:00s                                              \n",
            "epoch 15 | loss: 0.65274 |  0:00:00s                                              \n",
            "epoch 16 | loss: 0.66015 |  0:00:00s                                              \n",
            "epoch 17 | loss: 0.66601 |  0:00:00s                                              \n",
            "epoch 18 | loss: 0.66265 |  0:00:00s                                              \n",
            "epoch 19 | loss: 0.65902 |  0:00:00s                                              \n",
            "epoch 20 | loss: 0.65729 |  0:00:00s                                              \n",
            "epoch 21 | loss: 0.65535 |  0:00:00s                                              \n",
            "epoch 22 | loss: 0.64845 |  0:00:01s                                              \n",
            "epoch 23 | loss: 0.65655 |  0:00:01s                                              \n",
            "epoch 24 | loss: 0.65627 |  0:00:01s                                              \n",
            "epoch 25 | loss: 0.65279 |  0:00:01s                                              \n",
            "epoch 26 | loss: 0.65325 |  0:00:01s                                              \n",
            "epoch 27 | loss: 0.65968 |  0:00:01s                                              \n",
            "epoch 28 | loss: 0.65152 |  0:00:01s                                              \n",
            "epoch 29 | loss: 0.65759 |  0:00:01s                                              \n",
            "epoch 30 | loss: 0.65344 |  0:00:01s                                              \n",
            "epoch 31 | loss: 0.65118 |  0:00:01s                                              \n",
            "epoch 32 | loss: 0.65427 |  0:00:01s                                              \n",
            "epoch 33 | loss: 0.65642 |  0:00:01s                                              \n",
            "epoch 34 | loss: 0.65423 |  0:00:01s                                              \n",
            "epoch 35 | loss: 0.65555 |  0:00:01s                                              \n",
            "epoch 36 | loss: 0.65816 |  0:00:01s                                              \n",
            "epoch 37 | loss: 0.65569 |  0:00:01s                                              \n",
            "epoch 38 | loss: 0.6559  |  0:00:01s                                              \n",
            "epoch 39 | loss: 0.65315 |  0:00:01s                                              \n",
            "epoch 40 | loss: 0.65212 |  0:00:01s                                              \n",
            "epoch 41 | loss: 0.65438 |  0:00:01s                                              \n",
            "epoch 42 | loss: 0.65336 |  0:00:01s                                              \n",
            "epoch 43 | loss: 0.65686 |  0:00:01s                                              \n",
            "epoch 44 | loss: 0.65681 |  0:00:01s                                              \n",
            "epoch 45 | loss: 0.65714 |  0:00:02s                                              \n",
            "epoch 46 | loss: 0.65902 |  0:00:02s                                              \n",
            "epoch 47 | loss: 0.64796 |  0:00:02s                                              \n",
            "epoch 48 | loss: 0.65183 |  0:00:02s                                              \n",
            "epoch 49 | loss: 0.65885 |  0:00:02s                                              \n",
            "epoch 50 | loss: 0.65517 |  0:00:02s                                              \n",
            "epoch 51 | loss: 0.65815 |  0:00:02s                                              \n",
            "epoch 52 | loss: 0.65451 |  0:00:02s                                              \n",
            "epoch 53 | loss: 0.65089 |  0:00:02s                                              \n",
            "epoch 54 | loss: 0.6525  |  0:00:02s                                              \n",
            "epoch 55 | loss: 0.6526  |  0:00:02s                                              \n",
            "epoch 56 | loss: 0.6611  |  0:00:02s                                              \n",
            "epoch 57 | loss: 0.65559 |  0:00:02s                                              \n",
            "epoch 58 | loss: 0.65667 |  0:00:02s                                              \n",
            "epoch 59 | loss: 0.65166 |  0:00:02s                                              \n",
            "epoch 60 | loss: 0.6541  |  0:00:02s                                              \n",
            "epoch 61 | loss: 0.64848 |  0:00:02s                                              \n",
            "epoch 62 | loss: 0.65481 |  0:00:02s                                              \n",
            "epoch 63 | loss: 0.65333 |  0:00:02s                                              \n",
            "epoch 64 | loss: 0.65386 |  0:00:02s                                              \n",
            "epoch 65 | loss: 0.64945 |  0:00:02s                                              \n",
            "epoch 66 | loss: 0.64804 |  0:00:02s                                              \n",
            "epoch 67 | loss: 0.65154 |  0:00:03s                                              \n",
            "epoch 68 | loss: 0.65439 |  0:00:03s                                              \n",
            "epoch 69 | loss: 0.65606 |  0:00:03s                                              \n",
            "epoch 70 | loss: 0.65102 |  0:00:03s                                              \n",
            "epoch 71 | loss: 0.65272 |  0:00:03s                                              \n",
            "epoch 72 | loss: 0.65625 |  0:00:03s                                              \n",
            "epoch 73 | loss: 0.65476 |  0:00:03s                                              \n",
            "epoch 74 | loss: 0.64848 |  0:00:03s                                              \n",
            "epoch 75 | loss: 0.65245 |  0:00:03s                                              \n",
            "epoch 76 | loss: 0.65572 |  0:00:03s                                              \n",
            "epoch 77 | loss: 0.66178 |  0:00:03s                                              \n",
            "epoch 78 | loss: 0.65548 |  0:00:03s                                              \n",
            "epoch 79 | loss: 0.65366 |  0:00:03s                                              \n",
            "epoch 80 | loss: 0.66175 |  0:00:03s                                              \n",
            "epoch 81 | loss: 0.65087 |  0:00:03s                                              \n",
            "epoch 82 | loss: 0.65403 |  0:00:03s                                              \n",
            "epoch 83 | loss: 0.654   |  0:00:03s                                              \n",
            "epoch 84 | loss: 0.65244 |  0:00:03s                                              \n",
            "epoch 85 | loss: 0.66005 |  0:00:03s                                              \n",
            "epoch 86 | loss: 0.65493 |  0:00:03s                                              \n",
            "epoch 87 | loss: 0.64949 |  0:00:03s                                              \n",
            "epoch 88 | loss: 0.64943 |  0:00:03s                                              \n",
            "epoch 89 | loss: 0.65977 |  0:00:03s                                              \n",
            "epoch 90 | loss: 0.64923 |  0:00:04s                                              \n",
            "epoch 91 | loss: 0.6526  |  0:00:04s                                              \n",
            "epoch 92 | loss: 0.65189 |  0:00:04s                                              \n",
            "epoch 93 | loss: 0.65097 |  0:00:04s                                              \n",
            "epoch 94 | loss: 0.65219 |  0:00:04s                                              \n",
            "epoch 95 | loss: 0.64573 |  0:00:04s                                              \n",
            "epoch 96 | loss: 0.65529 |  0:00:04s                                              \n",
            "epoch 97 | loss: 0.65213 |  0:00:04s                                              \n",
            "epoch 98 | loss: 0.65517 |  0:00:04s                                              \n",
            "epoch 99 | loss: 0.65794 |  0:00:04s                                              \n",
            "Training completed for Fold_4 with AUC: 0.634548889031184                         \n",
            "100%|██████████| 50/50 [20:41<00:00, 24.83s/trial, best loss: -0.6385634246774857]\n",
            "Best hyperparameters for TabNet: {'cat_emb_dim': 4.0, 'gamma': 1.9625566327022135, 'lr': 0.028004677059855897, 'n_a': 12.0, 'n_d': 13.0, 'n_steps': 3.0}\n",
            "epoch 0  | loss: 0.82129 |  0:00:00s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:31] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.69413 |  0:00:00s\n",
            "epoch 2  | loss: 0.68169 |  0:00:00s\n",
            "epoch 3  | loss: 0.67449 |  0:00:00s\n",
            "epoch 4  | loss: 0.66989 |  0:00:00s\n",
            "epoch 5  | loss: 0.66785 |  0:00:00s\n",
            "epoch 6  | loss: 0.66801 |  0:00:00s\n",
            "epoch 7  | loss: 0.6692  |  0:00:00s\n",
            "epoch 8  | loss: 0.67093 |  0:00:00s\n",
            "epoch 9  | loss: 0.66066 |  0:00:00s\n",
            "epoch 10 | loss: 0.66997 |  0:00:00s\n",
            "epoch 11 | loss: 0.66499 |  0:00:00s\n",
            "epoch 12 | loss: 0.6652  |  0:00:00s\n",
            "epoch 13 | loss: 0.66063 |  0:00:00s\n",
            "epoch 14 | loss: 0.66369 |  0:00:00s\n",
            "epoch 15 | loss: 0.66042 |  0:00:00s\n",
            "epoch 16 | loss: 0.66606 |  0:00:00s\n",
            "epoch 17 | loss: 0.6549  |  0:00:01s\n",
            "epoch 18 | loss: 0.66593 |  0:00:01s\n",
            "epoch 19 | loss: 0.66275 |  0:00:01s\n",
            "epoch 20 | loss: 0.66116 |  0:00:01s\n",
            "epoch 21 | loss: 0.67011 |  0:00:01s\n",
            "epoch 22 | loss: 0.66674 |  0:00:01s\n",
            "epoch 23 | loss: 0.66347 |  0:00:01s\n",
            "epoch 24 | loss: 0.66219 |  0:00:01s\n",
            "epoch 25 | loss: 0.66776 |  0:00:01s\n",
            "epoch 26 | loss: 0.66323 |  0:00:01s\n",
            "epoch 27 | loss: 0.66517 |  0:00:01s\n",
            "epoch 28 | loss: 0.66195 |  0:00:01s\n",
            "epoch 29 | loss: 0.66681 |  0:00:01s\n",
            "epoch 30 | loss: 0.66132 |  0:00:01s\n",
            "epoch 31 | loss: 0.66383 |  0:00:01s\n",
            "epoch 32 | loss: 0.66425 |  0:00:01s\n",
            "epoch 33 | loss: 0.65932 |  0:00:01s\n",
            "epoch 34 | loss: 0.66514 |  0:00:01s\n",
            "epoch 35 | loss: 0.66176 |  0:00:01s\n",
            "epoch 36 | loss: 0.66962 |  0:00:01s\n",
            "epoch 37 | loss: 0.66413 |  0:00:01s\n",
            "epoch 38 | loss: 0.66864 |  0:00:01s\n",
            "epoch 39 | loss: 0.66556 |  0:00:01s\n",
            "epoch 40 | loss: 0.66459 |  0:00:01s\n",
            "epoch 41 | loss: 0.66431 |  0:00:02s\n",
            "epoch 42 | loss: 0.66639 |  0:00:02s\n",
            "epoch 43 | loss: 0.66566 |  0:00:02s\n",
            "epoch 44 | loss: 0.66461 |  0:00:02s\n",
            "epoch 45 | loss: 0.66274 |  0:00:02s\n",
            "epoch 46 | loss: 0.66234 |  0:00:02s\n",
            "epoch 47 | loss: 0.66805 |  0:00:02s\n",
            "epoch 48 | loss: 0.66645 |  0:00:02s\n",
            "epoch 49 | loss: 0.6657  |  0:00:02s\n",
            "epoch 50 | loss: 0.6629  |  0:00:02s\n",
            "epoch 51 | loss: 0.65916 |  0:00:02s\n",
            "epoch 52 | loss: 0.66155 |  0:00:02s\n",
            "epoch 53 | loss: 0.66205 |  0:00:02s\n",
            "epoch 54 | loss: 0.65815 |  0:00:02s\n",
            "epoch 55 | loss: 0.66152 |  0:00:02s\n",
            "epoch 56 | loss: 0.66028 |  0:00:02s\n",
            "epoch 57 | loss: 0.66463 |  0:00:02s\n",
            "epoch 58 | loss: 0.65953 |  0:00:02s\n",
            "epoch 59 | loss: 0.66682 |  0:00:02s\n",
            "epoch 60 | loss: 0.66321 |  0:00:02s\n",
            "epoch 61 | loss: 0.66373 |  0:00:02s\n",
            "epoch 62 | loss: 0.66355 |  0:00:02s\n",
            "epoch 63 | loss: 0.66078 |  0:00:02s\n",
            "epoch 64 | loss: 0.66516 |  0:00:02s\n",
            "epoch 65 | loss: 0.66152 |  0:00:03s\n",
            "epoch 66 | loss: 0.66482 |  0:00:03s\n",
            "epoch 67 | loss: 0.66327 |  0:00:03s\n",
            "epoch 68 | loss: 0.66354 |  0:00:03s\n",
            "epoch 69 | loss: 0.66134 |  0:00:03s\n",
            "epoch 70 | loss: 0.66747 |  0:00:03s\n",
            "epoch 71 | loss: 0.65962 |  0:00:03s\n",
            "epoch 72 | loss: 0.66135 |  0:00:03s\n",
            "epoch 73 | loss: 0.66312 |  0:00:03s\n",
            "epoch 74 | loss: 0.66194 |  0:00:03s\n",
            "epoch 75 | loss: 0.66819 |  0:00:03s\n",
            "epoch 76 | loss: 0.66151 |  0:00:03s\n",
            "epoch 77 | loss: 0.65716 |  0:00:03s\n",
            "epoch 78 | loss: 0.66099 |  0:00:03s\n",
            "epoch 79 | loss: 0.6632  |  0:00:03s\n",
            "epoch 80 | loss: 0.6629  |  0:00:03s\n",
            "epoch 81 | loss: 0.66266 |  0:00:03s\n",
            "epoch 82 | loss: 0.65956 |  0:00:03s\n",
            "epoch 83 | loss: 0.6576  |  0:00:03s\n",
            "epoch 84 | loss: 0.66103 |  0:00:03s\n",
            "epoch 85 | loss: 0.66449 |  0:00:03s\n",
            "epoch 86 | loss: 0.6628  |  0:00:03s\n",
            "epoch 87 | loss: 0.66488 |  0:00:03s\n",
            "epoch 88 | loss: 0.6673  |  0:00:03s\n",
            "epoch 89 | loss: 0.66309 |  0:00:04s\n",
            "epoch 90 | loss: 0.66404 |  0:00:04s\n",
            "epoch 91 | loss: 0.66472 |  0:00:04s\n",
            "epoch 92 | loss: 0.66572 |  0:00:04s\n",
            "epoch 93 | loss: 0.66462 |  0:00:04s\n",
            "epoch 94 | loss: 0.66543 |  0:00:04s\n",
            "epoch 95 | loss: 0.66423 |  0:00:04s\n",
            "epoch 96 | loss: 0.66119 |  0:00:04s\n",
            "epoch 97 | loss: 0.66243 |  0:00:04s\n",
            "epoch 98 | loss: 0.66079 |  0:00:04s\n",
            "epoch 99 | loss: 0.66241 |  0:00:04s\n",
            "Training completed for Fold_0 with AUC: 0.6757534917912276\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:36] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.79829 |  0:00:00s\n",
            "epoch 1  | loss: 0.66669 |  0:00:00s\n",
            "epoch 2  | loss: 0.66416 |  0:00:00s\n",
            "epoch 3  | loss: 0.65561 |  0:00:00s\n",
            "epoch 4  | loss: 0.65932 |  0:00:00s\n",
            "epoch 5  | loss: 0.66116 |  0:00:00s\n",
            "epoch 6  | loss: 0.65419 |  0:00:00s\n",
            "epoch 7  | loss: 0.65517 |  0:00:00s\n",
            "epoch 8  | loss: 0.66334 |  0:00:00s\n",
            "epoch 9  | loss: 0.65327 |  0:00:00s\n",
            "epoch 10 | loss: 0.65971 |  0:00:00s\n",
            "epoch 11 | loss: 0.659   |  0:00:00s\n",
            "epoch 12 | loss: 0.66051 |  0:00:00s\n",
            "epoch 13 | loss: 0.65711 |  0:00:00s\n",
            "epoch 14 | loss: 0.6592  |  0:00:00s\n",
            "epoch 15 | loss: 0.65609 |  0:00:00s\n",
            "epoch 16 | loss: 0.65753 |  0:00:00s\n",
            "epoch 17 | loss: 0.66103 |  0:00:00s\n",
            "epoch 18 | loss: 0.66029 |  0:00:00s\n",
            "epoch 19 | loss: 0.65484 |  0:00:00s\n",
            "epoch 20 | loss: 0.65865 |  0:00:00s\n",
            "epoch 21 | loss: 0.65416 |  0:00:00s\n",
            "epoch 22 | loss: 0.65564 |  0:00:00s\n",
            "epoch 23 | loss: 0.65826 |  0:00:01s\n",
            "epoch 24 | loss: 0.65388 |  0:00:01s\n",
            "epoch 25 | loss: 0.65951 |  0:00:01s\n",
            "epoch 26 | loss: 0.65773 |  0:00:01s\n",
            "epoch 27 | loss: 0.6569  |  0:00:01s\n",
            "epoch 28 | loss: 0.66149 |  0:00:01s\n",
            "epoch 29 | loss: 0.66262 |  0:00:01s\n",
            "epoch 30 | loss: 0.66118 |  0:00:01s\n",
            "epoch 31 | loss: 0.65915 |  0:00:01s\n",
            "epoch 32 | loss: 0.65585 |  0:00:01s\n",
            "epoch 33 | loss: 0.65769 |  0:00:01s\n",
            "epoch 34 | loss: 0.6549  |  0:00:01s\n",
            "epoch 35 | loss: 0.65304 |  0:00:01s\n",
            "epoch 36 | loss: 0.66283 |  0:00:01s\n",
            "epoch 37 | loss: 0.66036 |  0:00:01s\n",
            "epoch 38 | loss: 0.65727 |  0:00:01s\n",
            "epoch 39 | loss: 0.65843 |  0:00:01s\n",
            "epoch 40 | loss: 0.65764 |  0:00:01s\n",
            "epoch 41 | loss: 0.65725 |  0:00:01s\n",
            "epoch 42 | loss: 0.66286 |  0:00:01s\n",
            "epoch 43 | loss: 0.66094 |  0:00:01s\n",
            "epoch 44 | loss: 0.65994 |  0:00:01s\n",
            "epoch 45 | loss: 0.65736 |  0:00:01s\n",
            "epoch 46 | loss: 0.66132 |  0:00:02s\n",
            "epoch 47 | loss: 0.65876 |  0:00:02s\n",
            "epoch 48 | loss: 0.655   |  0:00:02s\n",
            "epoch 49 | loss: 0.6546  |  0:00:02s\n",
            "epoch 50 | loss: 0.6517  |  0:00:02s\n",
            "epoch 51 | loss: 0.65076 |  0:00:02s\n",
            "epoch 52 | loss: 0.66134 |  0:00:02s\n",
            "epoch 53 | loss: 0.65962 |  0:00:02s\n",
            "epoch 54 | loss: 0.65704 |  0:00:02s\n",
            "epoch 55 | loss: 0.65682 |  0:00:02s\n",
            "epoch 56 | loss: 0.65616 |  0:00:02s\n",
            "epoch 57 | loss: 0.66184 |  0:00:02s\n",
            "epoch 58 | loss: 0.66032 |  0:00:02s\n",
            "epoch 59 | loss: 0.6559  |  0:00:02s\n",
            "epoch 60 | loss: 0.65696 |  0:00:02s\n",
            "epoch 61 | loss: 0.65936 |  0:00:02s\n",
            "epoch 62 | loss: 0.65319 |  0:00:02s\n",
            "epoch 63 | loss: 0.65951 |  0:00:02s\n",
            "epoch 64 | loss: 0.6624  |  0:00:02s\n",
            "epoch 65 | loss: 0.65389 |  0:00:02s\n",
            "epoch 66 | loss: 0.65476 |  0:00:02s\n",
            "epoch 67 | loss: 0.6564  |  0:00:02s\n",
            "epoch 68 | loss: 0.6578  |  0:00:03s\n",
            "epoch 69 | loss: 0.65999 |  0:00:03s\n",
            "epoch 70 | loss: 0.66151 |  0:00:03s\n",
            "epoch 71 | loss: 0.65282 |  0:00:03s\n",
            "epoch 72 | loss: 0.65757 |  0:00:03s\n",
            "epoch 73 | loss: 0.65051 |  0:00:03s\n",
            "epoch 74 | loss: 0.65598 |  0:00:03s\n",
            "epoch 75 | loss: 0.65955 |  0:00:03s\n",
            "epoch 76 | loss: 0.65574 |  0:00:03s\n",
            "epoch 77 | loss: 0.66164 |  0:00:03s\n",
            "epoch 78 | loss: 0.65796 |  0:00:03s\n",
            "epoch 79 | loss: 0.65643 |  0:00:03s\n",
            "epoch 80 | loss: 0.65572 |  0:00:03s\n",
            "epoch 81 | loss: 0.65466 |  0:00:03s\n",
            "epoch 82 | loss: 0.65868 |  0:00:03s\n",
            "epoch 83 | loss: 0.66234 |  0:00:03s\n",
            "epoch 84 | loss: 0.65762 |  0:00:03s\n",
            "epoch 85 | loss: 0.65425 |  0:00:03s\n",
            "epoch 86 | loss: 0.65816 |  0:00:03s\n",
            "epoch 87 | loss: 0.65813 |  0:00:03s\n",
            "epoch 88 | loss: 0.66011 |  0:00:03s\n",
            "epoch 89 | loss: 0.65716 |  0:00:03s\n",
            "epoch 90 | loss: 0.65569 |  0:00:04s\n",
            "epoch 91 | loss: 0.6534  |  0:00:04s\n",
            "epoch 92 | loss: 0.65997 |  0:00:04s\n",
            "epoch 93 | loss: 0.65685 |  0:00:04s\n",
            "epoch 94 | loss: 0.65627 |  0:00:04s\n",
            "epoch 95 | loss: 0.65819 |  0:00:04s\n",
            "epoch 96 | loss: 0.65828 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.65309 |  0:00:04s\n",
            "epoch 99 | loss: 0.65704 |  0:00:04s\n",
            "Training completed for Fold_1 with AUC: 0.6307752686873566\n",
            "epoch 0  | loss: 0.74116 |  0:00:00s\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:41] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 1  | loss: 0.70513 |  0:00:00s\n",
            "epoch 2  | loss: 0.69432 |  0:00:00s\n",
            "epoch 3  | loss: 0.67477 |  0:00:00s\n",
            "epoch 4  | loss: 0.67408 |  0:00:00s\n",
            "epoch 5  | loss: 0.66448 |  0:00:00s\n",
            "epoch 6  | loss: 0.66114 |  0:00:00s\n",
            "epoch 7  | loss: 0.67036 |  0:00:00s\n",
            "epoch 8  | loss: 0.66511 |  0:00:00s\n",
            "epoch 9  | loss: 0.66068 |  0:00:00s\n",
            "epoch 10 | loss: 0.66214 |  0:00:00s\n",
            "epoch 11 | loss: 0.6624  |  0:00:00s\n",
            "epoch 12 | loss: 0.66165 |  0:00:00s\n",
            "epoch 13 | loss: 0.66099 |  0:00:00s\n",
            "epoch 14 | loss: 0.66227 |  0:00:00s\n",
            "epoch 15 | loss: 0.66577 |  0:00:00s\n",
            "epoch 16 | loss: 0.6587  |  0:00:00s\n",
            "epoch 17 | loss: 0.66433 |  0:00:00s\n",
            "epoch 18 | loss: 0.6621  |  0:00:00s\n",
            "epoch 19 | loss: 0.6587  |  0:00:01s\n",
            "epoch 20 | loss: 0.65557 |  0:00:01s\n",
            "epoch 21 | loss: 0.65922 |  0:00:01s\n",
            "epoch 22 | loss: 0.66284 |  0:00:01s\n",
            "epoch 23 | loss: 0.65952 |  0:00:01s\n",
            "epoch 24 | loss: 0.66066 |  0:00:01s\n",
            "epoch 25 | loss: 0.65914 |  0:00:01s\n",
            "epoch 26 | loss: 0.65793 |  0:00:01s\n",
            "epoch 27 | loss: 0.66025 |  0:00:01s\n",
            "epoch 28 | loss: 0.66212 |  0:00:01s\n",
            "epoch 29 | loss: 0.66159 |  0:00:01s\n",
            "epoch 30 | loss: 0.65868 |  0:00:01s\n",
            "epoch 31 | loss: 0.66358 |  0:00:01s\n",
            "epoch 32 | loss: 0.65803 |  0:00:01s\n",
            "epoch 33 | loss: 0.65629 |  0:00:01s\n",
            "epoch 34 | loss: 0.658   |  0:00:01s\n",
            "epoch 35 | loss: 0.66108 |  0:00:01s\n",
            "epoch 36 | loss: 0.66254 |  0:00:01s\n",
            "epoch 37 | loss: 0.66275 |  0:00:01s\n",
            "epoch 38 | loss: 0.65842 |  0:00:01s\n",
            "epoch 39 | loss: 0.66139 |  0:00:01s\n",
            "epoch 40 | loss: 0.65955 |  0:00:01s\n",
            "epoch 41 | loss: 0.65772 |  0:00:02s\n",
            "epoch 42 | loss: 0.66374 |  0:00:02s\n",
            "epoch 43 | loss: 0.66061 |  0:00:02s\n",
            "epoch 44 | loss: 0.6623  |  0:00:02s\n",
            "epoch 45 | loss: 0.65957 |  0:00:02s\n",
            "epoch 46 | loss: 0.66121 |  0:00:02s\n",
            "epoch 47 | loss: 0.66013 |  0:00:02s\n",
            "epoch 48 | loss: 0.66283 |  0:00:02s\n",
            "epoch 49 | loss: 0.66251 |  0:00:02s\n",
            "epoch 50 | loss: 0.65744 |  0:00:02s\n",
            "epoch 51 | loss: 0.66202 |  0:00:02s\n",
            "epoch 52 | loss: 0.65722 |  0:00:02s\n",
            "epoch 53 | loss: 0.66317 |  0:00:02s\n",
            "epoch 54 | loss: 0.65923 |  0:00:02s\n",
            "epoch 55 | loss: 0.65988 |  0:00:02s\n",
            "epoch 56 | loss: 0.66293 |  0:00:02s\n",
            "epoch 57 | loss: 0.66161 |  0:00:02s\n",
            "epoch 58 | loss: 0.65706 |  0:00:02s\n",
            "epoch 59 | loss: 0.66115 |  0:00:02s\n",
            "epoch 60 | loss: 0.65831 |  0:00:02s\n",
            "epoch 61 | loss: 0.65972 |  0:00:02s\n",
            "epoch 62 | loss: 0.66007 |  0:00:02s\n",
            "epoch 63 | loss: 0.65941 |  0:00:02s\n",
            "epoch 64 | loss: 0.65281 |  0:00:03s\n",
            "epoch 65 | loss: 0.65539 |  0:00:03s\n",
            "epoch 66 | loss: 0.66362 |  0:00:03s\n",
            "epoch 67 | loss: 0.66359 |  0:00:03s\n",
            "epoch 68 | loss: 0.66279 |  0:00:03s\n",
            "epoch 69 | loss: 0.66138 |  0:00:03s\n",
            "epoch 70 | loss: 0.66039 |  0:00:03s\n",
            "epoch 71 | loss: 0.65481 |  0:00:03s\n",
            "epoch 72 | loss: 0.65951 |  0:00:03s\n",
            "epoch 73 | loss: 0.65521 |  0:00:03s\n",
            "epoch 74 | loss: 0.66144 |  0:00:03s\n",
            "epoch 75 | loss: 0.65836 |  0:00:03s\n",
            "epoch 76 | loss: 0.66008 |  0:00:03s\n",
            "epoch 77 | loss: 0.66101 |  0:00:03s\n",
            "epoch 78 | loss: 0.65836 |  0:00:03s\n",
            "epoch 79 | loss: 0.65731 |  0:00:03s\n",
            "epoch 80 | loss: 0.65836 |  0:00:03s\n",
            "epoch 81 | loss: 0.65784 |  0:00:03s\n",
            "epoch 82 | loss: 0.66167 |  0:00:03s\n",
            "epoch 83 | loss: 0.66068 |  0:00:03s\n",
            "epoch 84 | loss: 0.66122 |  0:00:03s\n",
            "epoch 85 | loss: 0.65947 |  0:00:03s\n",
            "epoch 86 | loss: 0.65888 |  0:00:03s\n",
            "epoch 87 | loss: 0.65789 |  0:00:03s\n",
            "epoch 88 | loss: 0.65418 |  0:00:04s\n",
            "epoch 89 | loss: 0.65439 |  0:00:04s\n",
            "epoch 90 | loss: 0.65631 |  0:00:04s\n",
            "epoch 91 | loss: 0.66406 |  0:00:04s\n",
            "epoch 92 | loss: 0.65971 |  0:00:04s\n",
            "epoch 93 | loss: 0.66105 |  0:00:04s\n",
            "epoch 94 | loss: 0.66296 |  0:00:04s\n",
            "epoch 95 | loss: 0.66138 |  0:00:04s\n",
            "epoch 96 | loss: 0.65509 |  0:00:04s\n",
            "epoch 97 | loss: 0.65947 |  0:00:04s\n",
            "epoch 98 | loss: 0.66348 |  0:00:04s\n",
            "epoch 99 | loss: 0.65617 |  0:00:04s\n",
            "Training completed for Fold_2 with AUC: 0.632787634900311\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:45] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 0.82813 |  0:00:00s\n",
            "epoch 1  | loss: 0.67173 |  0:00:00s\n",
            "epoch 2  | loss: 0.65725 |  0:00:00s\n",
            "epoch 3  | loss: 0.65813 |  0:00:00s\n",
            "epoch 4  | loss: 0.65162 |  0:00:00s\n",
            "epoch 5  | loss: 0.65938 |  0:00:00s\n",
            "epoch 6  | loss: 0.65699 |  0:00:00s\n",
            "epoch 7  | loss: 0.65609 |  0:00:00s\n",
            "epoch 8  | loss: 0.65957 |  0:00:00s\n",
            "epoch 9  | loss: 0.65455 |  0:00:00s\n",
            "epoch 10 | loss: 0.65117 |  0:00:00s\n",
            "epoch 11 | loss: 0.64928 |  0:00:00s\n",
            "epoch 12 | loss: 0.6484  |  0:00:00s\n",
            "epoch 13 | loss: 0.65202 |  0:00:00s\n",
            "epoch 14 | loss: 0.65083 |  0:00:00s\n",
            "epoch 15 | loss: 0.65236 |  0:00:00s\n",
            "epoch 16 | loss: 0.65407 |  0:00:00s\n",
            "epoch 17 | loss: 0.65722 |  0:00:00s\n",
            "epoch 18 | loss: 0.65453 |  0:00:00s\n",
            "epoch 19 | loss: 0.65006 |  0:00:00s\n",
            "epoch 20 | loss: 0.65286 |  0:00:00s\n",
            "epoch 21 | loss: 0.64635 |  0:00:00s\n",
            "epoch 22 | loss: 0.65601 |  0:00:00s\n",
            "epoch 23 | loss: 0.65498 |  0:00:01s\n",
            "epoch 24 | loss: 0.65045 |  0:00:01s\n",
            "epoch 25 | loss: 0.64957 |  0:00:01s\n",
            "epoch 26 | loss: 0.64914 |  0:00:01s\n",
            "epoch 27 | loss: 0.64831 |  0:00:01s\n",
            "epoch 28 | loss: 0.65263 |  0:00:01s\n",
            "epoch 29 | loss: 0.65653 |  0:00:01s\n",
            "epoch 30 | loss: 0.6524  |  0:00:01s\n",
            "epoch 31 | loss: 0.64977 |  0:00:01s\n",
            "epoch 32 | loss: 0.6519  |  0:00:01s\n",
            "epoch 33 | loss: 0.65507 |  0:00:01s\n",
            "epoch 34 | loss: 0.65584 |  0:00:01s\n",
            "epoch 35 | loss: 0.64758 |  0:00:01s\n",
            "epoch 36 | loss: 0.64753 |  0:00:01s\n",
            "epoch 37 | loss: 0.6508  |  0:00:01s\n",
            "epoch 38 | loss: 0.65147 |  0:00:01s\n",
            "epoch 39 | loss: 0.65669 |  0:00:01s\n",
            "epoch 40 | loss: 0.65063 |  0:00:01s\n",
            "epoch 41 | loss: 0.65125 |  0:00:01s\n",
            "epoch 42 | loss: 0.65242 |  0:00:01s\n",
            "epoch 43 | loss: 0.65139 |  0:00:01s\n",
            "epoch 44 | loss: 0.64979 |  0:00:01s\n",
            "epoch 45 | loss: 0.65282 |  0:00:01s\n",
            "epoch 46 | loss: 0.64992 |  0:00:01s\n",
            "epoch 47 | loss: 0.65172 |  0:00:02s\n",
            "epoch 48 | loss: 0.65339 |  0:00:02s\n",
            "epoch 49 | loss: 0.65177 |  0:00:02s\n",
            "epoch 50 | loss: 0.6483  |  0:00:02s\n",
            "epoch 51 | loss: 0.65488 |  0:00:02s\n",
            "epoch 52 | loss: 0.65112 |  0:00:02s\n",
            "epoch 53 | loss: 0.64882 |  0:00:02s\n",
            "epoch 54 | loss: 0.65205 |  0:00:02s\n",
            "epoch 55 | loss: 0.65058 |  0:00:02s\n",
            "epoch 56 | loss: 0.65775 |  0:00:02s\n",
            "epoch 57 | loss: 0.65229 |  0:00:02s\n",
            "epoch 58 | loss: 0.65413 |  0:00:02s\n",
            "epoch 59 | loss: 0.65194 |  0:00:02s\n",
            "epoch 60 | loss: 0.65233 |  0:00:02s\n",
            "epoch 61 | loss: 0.65317 |  0:00:02s\n",
            "epoch 62 | loss: 0.65004 |  0:00:02s\n",
            "epoch 63 | loss: 0.65206 |  0:00:02s\n",
            "epoch 64 | loss: 0.65329 |  0:00:02s\n",
            "epoch 65 | loss: 0.65854 |  0:00:02s\n",
            "epoch 66 | loss: 0.64967 |  0:00:02s\n",
            "epoch 67 | loss: 0.65222 |  0:00:02s\n",
            "epoch 68 | loss: 0.65413 |  0:00:02s\n",
            "epoch 69 | loss: 0.64616 |  0:00:02s\n",
            "epoch 70 | loss: 0.65072 |  0:00:02s\n",
            "epoch 71 | loss: 0.65243 |  0:00:02s\n",
            "epoch 72 | loss: 0.65491 |  0:00:03s\n",
            "epoch 73 | loss: 0.64967 |  0:00:03s\n",
            "epoch 74 | loss: 0.65569 |  0:00:03s\n",
            "epoch 75 | loss: 0.64423 |  0:00:03s\n",
            "epoch 76 | loss: 0.64815 |  0:00:03s\n",
            "epoch 77 | loss: 0.64981 |  0:00:03s\n",
            "epoch 78 | loss: 0.65015 |  0:00:03s\n",
            "epoch 79 | loss: 0.64704 |  0:00:03s\n",
            "epoch 80 | loss: 0.65555 |  0:00:03s\n",
            "epoch 81 | loss: 0.65336 |  0:00:03s\n",
            "epoch 82 | loss: 0.64887 |  0:00:03s\n",
            "epoch 83 | loss: 0.65124 |  0:00:03s\n",
            "epoch 84 | loss: 0.64734 |  0:00:03s\n",
            "epoch 85 | loss: 0.64845 |  0:00:03s\n",
            "epoch 86 | loss: 0.65064 |  0:00:03s\n",
            "epoch 87 | loss: 0.6533  |  0:00:03s\n",
            "epoch 88 | loss: 0.65552 |  0:00:03s\n",
            "epoch 89 | loss: 0.64917 |  0:00:03s\n",
            "epoch 90 | loss: 0.6497  |  0:00:03s\n",
            "epoch 91 | loss: 0.64727 |  0:00:03s\n",
            "epoch 92 | loss: 0.65358 |  0:00:03s\n",
            "epoch 93 | loss: 0.65438 |  0:00:03s\n",
            "epoch 94 | loss: 0.64729 |  0:00:04s\n",
            "epoch 95 | loss: 0.65835 |  0:00:04s\n",
            "epoch 96 | loss: 0.65608 |  0:00:04s\n",
            "epoch 97 | loss: 0.64505 |  0:00:04s\n",
            "epoch 98 | loss: 0.65118 |  0:00:04s\n",
            "epoch 99 | loss: 0.6487  |  0:00:04s\n",
            "Training completed for Fold_3 with AUC: 0.6189518389773492\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/xgboost/training.py:183: UserWarning: [17:15:50] WARNING: /workspace/src/learner.cc:738: \n",
            "Parameters: { \"use_label_encoder\" } are not used.\n",
            "\n",
            "  bst.update(dtrain, iteration=i, fobj=obj)\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:82: UserWarning: Device used : cpu\n",
            "  warnings.warn(f\"Device used : {self.device}\")\n",
            "/home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages/pytorch_tabnet/abstract_model.py:687: UserWarning: No early stopping will be performed, last training weights will be used.\n",
            "  warnings.warn(wrn_msg)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "epoch 0  | loss: 1.16024 |  0:00:00s\n",
            "epoch 1  | loss: 0.71131 |  0:00:00s\n",
            "epoch 2  | loss: 0.70045 |  0:00:00s\n",
            "epoch 3  | loss: 0.69166 |  0:00:00s\n",
            "epoch 4  | loss: 0.67844 |  0:00:00s\n",
            "epoch 5  | loss: 0.67201 |  0:00:00s\n",
            "epoch 6  | loss: 0.66742 |  0:00:00s\n",
            "epoch 7  | loss: 0.65627 |  0:00:00s\n",
            "epoch 8  | loss: 0.65846 |  0:00:00s\n",
            "epoch 9  | loss: 0.66    |  0:00:00s\n",
            "epoch 10 | loss: 0.66268 |  0:00:00s\n",
            "epoch 11 | loss: 0.65486 |  0:00:00s\n",
            "epoch 12 | loss: 0.66073 |  0:00:00s\n",
            "epoch 13 | loss: 0.66184 |  0:00:00s\n",
            "epoch 14 | loss: 0.66056 |  0:00:00s\n",
            "epoch 15 | loss: 0.65274 |  0:00:00s\n",
            "epoch 16 | loss: 0.66015 |  0:00:00s\n",
            "epoch 17 | loss: 0.66601 |  0:00:00s\n",
            "epoch 18 | loss: 0.66265 |  0:00:00s\n",
            "epoch 19 | loss: 0.65902 |  0:00:00s\n",
            "epoch 20 | loss: 0.65729 |  0:00:00s\n",
            "epoch 21 | loss: 0.65535 |  0:00:00s\n",
            "epoch 22 | loss: 0.64845 |  0:00:01s\n",
            "epoch 23 | loss: 0.65655 |  0:00:01s\n",
            "epoch 24 | loss: 0.65627 |  0:00:01s\n",
            "epoch 25 | loss: 0.65279 |  0:00:01s\n",
            "epoch 26 | loss: 0.65325 |  0:00:01s\n",
            "epoch 27 | loss: 0.65968 |  0:00:01s\n",
            "epoch 28 | loss: 0.65152 |  0:00:01s\n",
            "epoch 29 | loss: 0.65759 |  0:00:01s\n",
            "epoch 30 | loss: 0.65344 |  0:00:01s\n",
            "epoch 31 | loss: 0.65118 |  0:00:01s\n",
            "epoch 32 | loss: 0.65427 |  0:00:01s\n",
            "epoch 33 | loss: 0.65642 |  0:00:01s\n",
            "epoch 34 | loss: 0.65423 |  0:00:01s\n",
            "epoch 35 | loss: 0.65555 |  0:00:01s\n",
            "epoch 36 | loss: 0.65816 |  0:00:01s\n",
            "epoch 37 | loss: 0.65569 |  0:00:01s\n",
            "epoch 38 | loss: 0.6559  |  0:00:01s\n",
            "epoch 39 | loss: 0.65315 |  0:00:01s\n",
            "epoch 40 | loss: 0.65212 |  0:00:01s\n",
            "epoch 41 | loss: 0.65438 |  0:00:01s\n",
            "epoch 42 | loss: 0.65336 |  0:00:01s\n",
            "epoch 43 | loss: 0.65686 |  0:00:01s\n",
            "epoch 44 | loss: 0.65681 |  0:00:01s\n",
            "epoch 45 | loss: 0.65714 |  0:00:01s\n",
            "epoch 46 | loss: 0.65902 |  0:00:02s\n",
            "epoch 47 | loss: 0.64796 |  0:00:02s\n",
            "epoch 48 | loss: 0.65183 |  0:00:02s\n",
            "epoch 49 | loss: 0.65885 |  0:00:02s\n",
            "epoch 50 | loss: 0.65517 |  0:00:02s\n",
            "epoch 51 | loss: 0.65815 |  0:00:02s\n",
            "epoch 52 | loss: 0.65451 |  0:00:02s\n",
            "epoch 53 | loss: 0.65089 |  0:00:02s\n",
            "epoch 54 | loss: 0.6525  |  0:00:02s\n",
            "epoch 55 | loss: 0.6526  |  0:00:02s\n",
            "epoch 56 | loss: 0.6611  |  0:00:02s\n",
            "epoch 57 | loss: 0.65559 |  0:00:02s\n",
            "epoch 58 | loss: 0.65667 |  0:00:02s\n",
            "epoch 59 | loss: 0.65166 |  0:00:02s\n",
            "epoch 60 | loss: 0.6541  |  0:00:02s\n",
            "epoch 61 | loss: 0.64848 |  0:00:02s\n",
            "epoch 62 | loss: 0.65481 |  0:00:02s\n",
            "epoch 63 | loss: 0.65333 |  0:00:02s\n",
            "epoch 64 | loss: 0.65386 |  0:00:02s\n",
            "epoch 65 | loss: 0.64945 |  0:00:02s\n",
            "epoch 66 | loss: 0.64804 |  0:00:02s\n",
            "epoch 67 | loss: 0.65154 |  0:00:02s\n",
            "epoch 68 | loss: 0.65439 |  0:00:02s\n",
            "epoch 69 | loss: 0.65606 |  0:00:02s\n",
            "epoch 70 | loss: 0.65102 |  0:00:03s\n",
            "epoch 71 | loss: 0.65272 |  0:00:03s\n",
            "epoch 72 | loss: 0.65625 |  0:00:03s\n",
            "epoch 73 | loss: 0.65476 |  0:00:03s\n",
            "epoch 74 | loss: 0.64848 |  0:00:03s\n",
            "epoch 75 | loss: 0.65245 |  0:00:03s\n",
            "epoch 76 | loss: 0.65572 |  0:00:03s\n",
            "epoch 77 | loss: 0.66178 |  0:00:03s\n",
            "epoch 78 | loss: 0.65548 |  0:00:03s\n",
            "epoch 79 | loss: 0.65366 |  0:00:03s\n",
            "epoch 80 | loss: 0.66175 |  0:00:03s\n",
            "epoch 81 | loss: 0.65087 |  0:00:03s\n",
            "epoch 82 | loss: 0.65403 |  0:00:03s\n",
            "epoch 83 | loss: 0.654   |  0:00:03s\n",
            "epoch 84 | loss: 0.65244 |  0:00:03s\n",
            "epoch 85 | loss: 0.66005 |  0:00:03s\n",
            "epoch 86 | loss: 0.65493 |  0:00:03s\n",
            "epoch 87 | loss: 0.64949 |  0:00:03s\n",
            "epoch 88 | loss: 0.64943 |  0:00:03s\n",
            "epoch 89 | loss: 0.65977 |  0:00:03s\n",
            "epoch 90 | loss: 0.64923 |  0:00:03s\n",
            "epoch 91 | loss: 0.6526  |  0:00:03s\n",
            "epoch 92 | loss: 0.65189 |  0:00:03s\n",
            "epoch 93 | loss: 0.65097 |  0:00:03s\n",
            "epoch 94 | loss: 0.65219 |  0:00:04s\n",
            "epoch 95 | loss: 0.64573 |  0:00:04s\n",
            "epoch 96 | loss: 0.65529 |  0:00:04s\n",
            "epoch 97 | loss: 0.65213 |  0:00:04s\n",
            "epoch 98 | loss: 0.65517 |  0:00:04s\n",
            "epoch 99 | loss: 0.65794 |  0:00:04s\n",
            "Training completed for Fold_4 with AUC: 0.634548889031184\n",
            "Mean AUC after hyperparameter tuning: 0.6385634246774857\n"
          ]
        }
      ],
      "source": [
        "# hyperparameter tuning for TabNet\n",
        "from hyperopt import hp, fmin, tpe, Trials, STATUS_OK\n",
        "def objective_tabnet(params):\n",
        "    # Convert categorical feature names to indices\n",
        "    cat_idxs = [X.columns.get_loc(c) for c in cats]\n",
        "    cat_dims = [2] * len(cats)  # Assuming binary categorical features\n",
        "\n",
        "    # Initialize the TabNet model with hyperparameters\n",
        "    model = SklearnTabNet(\n",
        "        n_d=int(params['n_d']),\n",
        "        n_a=int(params['n_a']),\n",
        "        n_steps=int(params['n_steps']),\n",
        "        gamma=params['gamma'],\n",
        "        cat_idxs=cat_idxs,\n",
        "        cat_dims=cat_dims,\n",
        "        cat_emb_dim=int(params['cat_emb_dim']),\n",
        "        optimizer_fn=torch.optim.Adam,\n",
        "        optimizer_params=dict(lr=params['lr']),\n",
        "        scheduler_params=dict(step_size=50, gamma=0.9),\n",
        "        scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "        verbose=0\n",
        "    )\n",
        "\n",
        "    # Perform cross-validation and return the mean AUC\n",
        "    results = perform_cross_validation(X, y, groups, model, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "    auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "    mean_auc = np.mean(auc_values)\n",
        "\n",
        "    return {'loss': -mean_auc, 'status': STATUS_OK}\n",
        "# Define the hyperparameter search space\n",
        "space_tabnet = {\n",
        "    'n_d': hp.quniform('n_d', 4, 16, 1),  # Integer between 4 and 16\n",
        "    'n_a': hp.quniform('n_a', 4, 16, 1),  # Integer between 4 and 16\n",
        "    'n_steps': hp.quniform('n_steps', 1, 5, 1),  # Integer between 1 and 5\n",
        "    'gamma': hp.uniform('gamma', 1.0, 2.0),  # Float between 1.0 and 2.0\n",
        "    'cat_emb_dim': hp.quniform('cat_emb_dim', 1, 8, 1),  # Integer between 1 and 8\n",
        "    'lr': hp.loguniform('lr', -5, -2)  # Learning rate on a log scale (0.0067 to 0.01)\n",
        "}\n",
        "# Run hyperparameter optimization\n",
        "trials_tabnet = Trials()\n",
        "best_tabnet = fmin(\n",
        "    fn=objective_tabnet,\n",
        "    space=space_tabnet,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=50,\n",
        "    trials=trials_tabnet\n",
        ")\n",
        "print(\"Best hyperparameters for TabNet:\", best_tabnet)\n",
        "# Update the estimator with the best hyperparameters\n",
        "estimator = SklearnTabNet(\n",
        "    n_d=int(best_tabnet['n_d']),\n",
        "    n_a=int(best_tabnet['n_a']),\n",
        "    n_steps=int(best_tabnet['n_steps']),\n",
        "    gamma=best_tabnet['gamma'],\n",
        "    cat_idxs=[X.columns.get_loc(c) for c in cats],\n",
        "    cat_dims=[2] * len(cats),  # Assuming binary categorical features\n",
        "    cat_emb_dim=int(best_tabnet['cat_emb_dim']),\n",
        "    optimizer_fn=torch.optim.Adam,\n",
        "    optimizer_params=dict(lr=best_tabnet['lr']),\n",
        "    scheduler_params=dict(step_size=50, gamma=0.9),\n",
        "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "    verbose=0\n",
        ")\n",
        "# Perform cross-validation with the tuned TabNet model\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[xgb_selector], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(\"Mean AUC after hyperparameter tuning:\", mean_auc)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Best hyperparameters for TabNet: {'cat_emb_dim': 4.0, 'gamma': 1.9625566327022135, 'lr': 0.028004677059855897, 'n_a': 12.0, 'n_d': 13.0, 'n_steps': 3.0}\n",
        "#Mean AUC after hyperparameter tuning: 0.6385634246774857"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZdspTz_G2D2"
      },
      "source": [
        "## Assignment 5. Please try combining all the above methods to push the model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftBi462fHGQn"
      },
      "source": [
        "Hint: Methods other than the above methods are also okay to use to improve model performance.\n",
        "\n",
        "Please avoid data leakage when conducting hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9SI2OpwG8eE"
      },
      "outputs": [],
      "source": [
        "#######Your code for combing all above mentioned methods to push model performance########"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "intro-machine-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}

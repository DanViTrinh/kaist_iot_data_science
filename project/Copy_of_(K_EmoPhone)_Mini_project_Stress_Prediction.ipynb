{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cqjSsKeM3fhL"
      },
      "source": [
        "# CS565-DS522 IoT Data Science Mini Project for K-EmoPhone dataset\n",
        "*This material is a joint work of TAs from IC Lab at KAIST, including Panyu Zhang, Soowon Kang, and Woohyeok Choi. This work is licensed under CC BY-SA 4.0.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TiQ3qZRo52Wi"
      },
      "source": [
        "## Instruction\n",
        "In this mini-project, we will build a model to predict users' self-reported stress using extracted features from K-EmoPhone dataset. This material mainly refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) conducting indepedent reproducibility experiments on K-EmoPhone dataset. In order to save time, we provide the extracted features from the raw data instead of starting from scratch. Besides, traditional machine learning model is used considering limited number of labels and multimodality issue in the in-the-wild K-EmoPhone dataset.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vbgf8BGx5MsB"
      },
      "source": [
        "## Guidance\n",
        "\n",
        "1. Before running the code, please first download the extracted features from the following [link](https://drive.google.com/file/d/1HcyFvzWEzO21osyP5E8VpVmHROX1ew7q/view?usp=sharing).\n",
        "\n",
        "2. Please change your runtime type to T4-GPU or other runtime types with GPU available since later we may use GPU for\n",
        "xgboost execution"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YH4izzUnJ6hp"
      },
      "source": [
        "Install latest version of xgboost > 2.0.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PQDlc8HW-Kt4",
        "outputId": "c6d197a5-7b27-4c57-f371-dc2f6c053009"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting xgboost\n",
            "  Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl.metadata (2.1 kB)\n",
            "Requirement already satisfied: numpy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.26.4)\n",
            "Requirement already satisfied: nvidia-nccl-cu12 in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (2.21.5)\n",
            "Requirement already satisfied: scipy in /home/dan/miniconda3/envs/intro-machine-learning/lib/python3.12/site-packages (from xgboost) (1.14.0)\n",
            "Downloading xgboost-3.0.1-py3-none-manylinux_2_28_x86_64.whl (253.9 MB)\n",
            "\u001b[2K   \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m253.9/253.9 MB\u001b[0m \u001b[31m4.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m[36m0:00:02\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: xgboost\n",
            "Successfully installed xgboost-3.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install xgboost"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "XnKkV_aTVZ_Z"
      },
      "outputs": [],
      "source": [
        "import pytz\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import scipy.stats as st\n",
        "import cloudpickle\n",
        "from datetime import datetime\n",
        "from contextlib import contextmanager\n",
        "import warnings\n",
        "import time\n",
        "from typing import Optional\n",
        "from contextlib import contextmanager\n",
        "\n",
        "DEFAULT_TZ = pytz.FixedOffset(540)  # GMT+09:00; Asia/Seoul\n",
        "\n",
        "RANDOM_STATE =42\n",
        "\n",
        "\n",
        "def log(msg: any):\n",
        "    print('[{}] {}'.format(datetime.now().strftime('%y-%m-%d %H:%M:%S'), msg))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0aF5NVMMUzBb"
      },
      "source": [
        "## 1.Preparation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oG5n57OnHIOM"
      },
      "source": [
        "### 1.1. Mount to Your Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NZ9KU0Kq4WZs",
        "outputId": "a9ea32a9-c6b4-4be2-c90f-df5028d7fe7e"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "\"\\nfrom google.colab import drive\\n\\ndrive.mount('/content/drive')\\n\""
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# not relevant for local execution\n",
        "'''\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "'''"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q9u_nJzcIKXF"
      },
      "source": [
        "### 1.2. Load Extracted Features"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eg5JSHftIP_f",
        "outputId": "a770ce09-541d-4a64-f0d8-f368a7ce0ba1"
      },
      "outputs": [],
      "source": [
        "import pickle\n",
        "import numpy as np\n",
        "\n",
        "#PATH = '/content/drive/MyDrive/IoT_Data_Science/Project/Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "PATH = './Datasets/features_stress_fixed_K-EmoPhone.pkl'\n",
        "\n",
        "X, y, groups, t, datetimes = pickle.load(open(PATH, mode='rb'))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ffYa1CuFIx7Q"
      },
      "source": [
        "X is the extracted features and the feature extraction process refers to the public [repository](https://github.com/SteinPanyu/IndependentReproducibility) and the immediate past time window is set as 15 minutes. y is the array of labels while groups is the user ids.\n",
        "\n",
        "Please note that here y is binarized using theoretical threshold (if ESM stress > 0, binarize as 1, else 0, ESM label scale [-3, 3])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tc0jkopjUl2h"
      },
      "source": [
        "Since features are already extracted, we do not need to work on preprocessing and feature extraction again."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AGLIYf29UYES"
      },
      "source": [
        "## 2.Feature Preparation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xOgjC9HXo5cP"
      },
      "source": [
        "There exist multiple types of features. Please try different combinations of features to see if there is any model performance improvement."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PE57GSJPVOWi"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'f' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[8], line 49\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m     45\u001b[0m \n\u001b[1;32m     46\u001b[0m \u001b[38;5;66;03m#Prepare the final feature set\u001b[39;00m\n\u001b[1;32m     47\u001b[0m feat_baseline \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m feat_final \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mconcat([feat_baseline, feat_current_sensor,\u001b[43mf\u001b[49m ],axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     52\u001b[0m \u001b[38;5;66;03m################################################################################\u001b[39;00m\n\u001b[1;32m     53\u001b[0m X \u001b[38;5;241m=\u001b[39m feat_final\n",
            "\u001b[0;31mNameError\u001b[0m: name 'f' is not defined"
          ]
        }
      ],
      "source": [
        "\n",
        "#The following code is designed for reordering the data\n",
        "#################################################\n",
        "# Create a DataFrame with user_id and datetime\n",
        "\n",
        "df = pd.DataFrame({'user_id': groups, 'datetime': datetimes, 'label': y})\n",
        "\n",
        "# df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "df_merged = pd.merge(df, X, left_index=True, right_index=True)\n",
        "\n",
        "# Sort the DataFrame by datetime\n",
        "df_merged = df_merged.sort_values(by=['user_id', 'datetime'])\n",
        "\n",
        "# Update groups and datetimes\n",
        "groups = df_merged['user_id'].to_numpy()\n",
        "datetimes = df_merged['datetime'].to_numpy()\n",
        "y = df_merged['label'].to_numpy()\n",
        "X = df_merged.drop(columns=['user_id', 'datetime', 'label'])\n",
        "\n",
        "\n",
        "\n",
        "#Divide the features into different categories\n",
        "feat_current = X.loc[:,[('#VAL' in str(x)) or ('ESM#LastLabel' in str(x)) for x in X.keys()]]\n",
        "feat_dsc = X.loc[:,[('#DSC' in str(x))  for x in X.keys()]]\n",
        "feat_yesterday = X.loc[:,[('Yesterday' in str(x))  for x in X.keys()]]\n",
        "feat_today = X.loc[:,[('Today' in str(x))  for x in X.keys()]]\n",
        "\n",
        "feat_ImmediatePast = X.loc[:,[('ImmediatePast_15' in str(x))  for x in X.keys()]]\n",
        "\n",
        "#################################################################################\n",
        "#Below are the available features\n",
        "#Divide the time window features into sensor/ESM self-report features\n",
        "feat_current_sensor = X.loc[:,[('#VAL' in str(x))  for x in X.keys()]] #Current sensor features (value right before label)\n",
        "feat_current_ESM = X.loc[:,[('ESM#LastLabel' in str(x)) for x in X.keys()]] #Current ESM features (value right before label)\n",
        "feat_ImmediatePast_sensor = feat_ImmediatePast.loc[:,[('ESM' not in str(x)) for x in feat_ImmediatePast.keys()]] #Immediate past sensor features (in past 15 minutes before label)\n",
        "feat_ImmediatePast_ESM = feat_ImmediatePast.loc[:,[('ESM'  in str(x)) for x in feat_ImmediatePast.keys()]]  #Immediate past ESM features\n",
        "feat_today_sensor = feat_today.loc[:,[('ESM' not in str(x))  for x in feat_today.keys()]] #Today epoch sensor features\n",
        "feat_today_ESM = feat_today.loc[:,[('ESM'  in str(x)) for x in feat_today.keys()]] #Today epoch ESM features\n",
        "feat_yesterday_sensor = feat_yesterday.loc[:,[('ESM' not in str(x)) for x in feat_yesterday.keys()]] #Yesterday sensor features\n",
        "feat_yesterday_ESM = feat_yesterday.loc[:,[('ESM'  in str(x)) for x in feat_yesterday.keys()]] #Yesterday ESM features\n",
        "\n",
        "feat_sleep = X.loc[:,[('Sleep' in str(x))  for x in X.keys()]]\n",
        "feat_time = X.loc[:,[('Time' in str(x))  for x in X.keys()]]\n",
        "feat_pif = X.loc[:,[('PIF' in str(x))  for x in X.keys()]]\n",
        "################################################################################\n",
        "\n",
        "#Prepare the final feature set\n",
        "feat_baseline = pd.concat([ feat_time,feat_dsc,feat_current_sensor, feat_ImmediatePast_sensor],axis=1)\n",
        "\n",
        "feat_final = pd.concat([feat_baseline  ],axis=1)\n",
        "\n",
        "\n",
        "################################################################################\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 424
        },
        "id": "VIqbJ_A8JGUS",
        "outputId": "59928fd1-9982-4580-c4a1-25f4afbb070c"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"feat_current_ESM\",\n  \"rows\": 2619,\n  \"fields\": [\n    {\n      \"column\": \"ESM#LastLabel\",\n      \"properties\": {\n        \"dtype\": \"float32\",\n        \"num_unique_values\": 2,\n        \"samples\": [\n          1.0,\n          0.0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe",
              "variable_name": "feat_current_ESM"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-2b21e6e2-250a-4667-ab4f-deb6d3b86d8d\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>ESM#LastLabel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>...</th>\n",
              "      <td>...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2614</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2615</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2616</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2617</th>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2618</th>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2619 rows × 1 columns</p>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2b21e6e2-250a-4667-ab4f-deb6d3b86d8d')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-2b21e6e2-250a-4667-ab4f-deb6d3b86d8d button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-2b21e6e2-250a-4667-ab4f-deb6d3b86d8d');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-80227cc2-fb52-4539-8694-a5dbbb628de4\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-80227cc2-fb52-4539-8694-a5dbbb628de4')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-80227cc2-fb52-4539-8694-a5dbbb628de4 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "  <div id=\"id_93ecf685-aa60-4199-a832-134659a87f2d\">\n",
              "    <style>\n",
              "      .colab-df-generate {\n",
              "        background-color: #E8F0FE;\n",
              "        border: none;\n",
              "        border-radius: 50%;\n",
              "        cursor: pointer;\n",
              "        display: none;\n",
              "        fill: #1967D2;\n",
              "        height: 32px;\n",
              "        padding: 0 0 0 0;\n",
              "        width: 32px;\n",
              "      }\n",
              "\n",
              "      .colab-df-generate:hover {\n",
              "        background-color: #E2EBFA;\n",
              "        box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "        fill: #174EA6;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate {\n",
              "        background-color: #3B4455;\n",
              "        fill: #D2E3FC;\n",
              "      }\n",
              "\n",
              "      [theme=dark] .colab-df-generate:hover {\n",
              "        background-color: #434B5C;\n",
              "        box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "        filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "        fill: #FFFFFF;\n",
              "      }\n",
              "    </style>\n",
              "    <button class=\"colab-df-generate\" onclick=\"generateWithVariable('feat_current_ESM')\"\n",
              "            title=\"Generate code using this dataframe.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M7,19H8.4L18.45,9,17,7.55,7,17.6ZM5,21V16.75L18.45,3.32a2,2,0,0,1,2.83,0l1.4,1.43a1.91,1.91,0,0,1,.58,1.4,1.91,1.91,0,0,1-.58,1.4L9.25,21ZM18.45,9,17,7.55Zm-12,3A5.31,5.31,0,0,0,4.9,8.1,5.31,5.31,0,0,0,1,6.5,5.31,5.31,0,0,0,4.9,4.9,5.31,5.31,0,0,0,6.5,1,5.31,5.31,0,0,0,8.1,4.9,5.31,5.31,0,0,0,12,6.5,5.46,5.46,0,0,0,6.5,12Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "    <script>\n",
              "      (() => {\n",
              "      const buttonEl =\n",
              "        document.querySelector('#id_93ecf685-aa60-4199-a832-134659a87f2d button.colab-df-generate');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      buttonEl.onclick = () => {\n",
              "        google.colab.notebook.generateWithVariable('feat_current_ESM');\n",
              "      }\n",
              "      })();\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "      ESM#LastLabel\n",
              "0               0.0\n",
              "1               1.0\n",
              "2               1.0\n",
              "3               0.0\n",
              "4               0.0\n",
              "...             ...\n",
              "2614            0.0\n",
              "2615            0.0\n",
              "2616            0.0\n",
              "2617            1.0\n",
              "2618            0.0\n",
              "\n",
              "[2619 rows x 1 columns]"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "feat_current_ESM"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kPIZll5fXQld"
      },
      "source": [
        "## 3.Model Training & Evaluation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2sE6PaldpCNU"
      },
      "source": [
        "Here is the revised XGBoost Classifier. We will use random eval_size percent of training set data as evaluation set for early stoppping."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "cxqMVtSVXTfH"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from xgboost import XGBClassifier, DMatrix\n",
        "from sklearn.base import BaseEstimator\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, train_test_split\n",
        "from typing import Union\n",
        "\n",
        "#Function for revised xgboost classifier\n",
        "class EvXGBClassifier(BaseEstimator):\n",
        "    \"\"\"\n",
        "    Enhanced XGBClassifier with built-in validation set approach for early stopping.\n",
        "    \"\"\"\n",
        "    def __init__(\n",
        "        self,\n",
        "        eval_size=None,\n",
        "        eval_metric='logloss',\n",
        "        early_stopping_rounds=10,\n",
        "        random_state=None,\n",
        "        **kwargs\n",
        "        ):\n",
        "        \"\"\"\n",
        "        Initializes the custom XGBoost Classifier.\n",
        "\n",
        "        Args:\n",
        "            eval_size (float): The proportion of the dataset to include in the evaluation split.\n",
        "            eval_metric (str): The evaluation metric used for model training.\n",
        "            early_stopping_rounds (int): The number of rounds to stop training if hold-out metric doesn't improve.\n",
        "            random_state (int): Seed for the random number generator for reproducibility.\n",
        "            **kwargs: Additional arguments to be passed to the underlying XGBClassifier.\n",
        "        \"\"\"\n",
        "        self.random_state = random_state\n",
        "        self.eval_size = eval_size\n",
        "        self.eval_metric = eval_metric\n",
        "        self.early_stopping_rounds = early_stopping_rounds\n",
        "        # Initialize the XGBClassifier with specified arguments and GPU acceleration.\n",
        "        self.model = XGBClassifier(\n",
        "            random_state=self.random_state,\n",
        "            eval_metric=self.eval_metric,\n",
        "            early_stopping_rounds=self.early_stopping_rounds,\n",
        "            tree_method = \"hist\", device = \"cuda\", #Use gpu for acceleration\n",
        "            **kwargs\n",
        "        )\n",
        "\n",
        "    @property\n",
        "    def feature_importances_(self):\n",
        "        \"\"\" Returns the feature importances from the fitted model. \"\"\"\n",
        "        return self.model.feature_importances_\n",
        "\n",
        "    @property\n",
        "    def feature_names_in_(self):\n",
        "        \"\"\" Returns the feature names from the input dataset used for fitting. \"\"\"\n",
        "        return self.model.feature_names_in_\n",
        "\n",
        "    def fit(self, X: Union[pd.DataFrame, np.ndarray], y: np.ndarray):\n",
        "        \"\"\"\n",
        "        Fit the XGBoost model with optional early stopping using a validation set.\n",
        "\n",
        "        Args:\n",
        "            X (Union[pd.DataFrame, np.ndarray]): Training features.\n",
        "            y (np.ndarray): Target values.\n",
        "        \"\"\"\n",
        "        if self.eval_size:\n",
        "            # Split data for early stopping evaluation if eval_size is specified.\n",
        "            X_train_sub, X_val, y_train_sub, y_val = train_test_split(\n",
        "                X, y, test_size=self.eval_size, random_state=self.random_state)\n",
        "            # Fit the model with early stopping.\n",
        "            self.model.fit(\n",
        "                X_train_sub, y_train_sub,\n",
        "                eval_set=[(X_val, y_val)],\n",
        "                verbose=False\n",
        "            )\n",
        "        else:\n",
        "            # Fit the model without early stopping.\n",
        "            self.model.fit(X, y, verbose=False)\n",
        "\n",
        "        # Store the best iteration number for predictions.\n",
        "        self.best_iteration_ = self.model.get_booster().best_iteration\n",
        "        return self\n",
        "\n",
        "    def predict(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the classes for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict(X, iteration_range=(0, self.best_iteration_ + 1))\n",
        "\n",
        "    def predict_proba(self, X: pd.DataFrame):\n",
        "        \"\"\"\n",
        "        Predict the class probabilities for the given features.\n",
        "\n",
        "        Args:\n",
        "            X (pd.DataFrame): Input features.\n",
        "        \"\"\"\n",
        "        return self.model.predict_proba(X, iteration_range=(0, self.best_iteration_ + 1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8h_yate5pYRg"
      },
      "source": [
        "The following is defined functions for model training and model evaluation (cross-validation)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "id": "AoHTuB3qpsfe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import time\n",
        "import traceback\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.base import clone\n",
        "from sklearn.model_selection import StratifiedShuffleSplit, StratifiedKFold, LeaveOneGroupOut, StratifiedGroupKFold\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from dataclasses import dataclass\n",
        "\n",
        "@dataclass\n",
        "class FoldResult:\n",
        "    name: str\n",
        "    metrics: dict\n",
        "    duration: float\n",
        "\n",
        "def log(message: str):\n",
        "    print(message)  # Simple logging to stdout or enhance as needed\n",
        "\n",
        "def train_fold(dir_result: str, fold_name: str, X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate the model for a single fold.\n",
        "    Args:\n",
        "        dir_result (str): Directory to store results.\n",
        "        fold_name (str): Name of the fold for identification.\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        C_cat, C_num (array): Lists of categorical and numeric feature names.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "        if normalize:\n",
        "            X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "            X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "            # Standard scaler only applied to numeric data\n",
        "            scaler = StandardScaler().fit(X_train_N)\n",
        "            X_train_N = scaler.transform(X_train_N)\n",
        "            X_test_N = scaler.transform(X_test_N)\n",
        "\n",
        "            X_train = pd.DataFrame(\n",
        "                np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "            X_test = pd.DataFrame(\n",
        "                np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                columns=np.concatenate((C_cat, C_num))\n",
        "            )\n",
        "\n",
        "        #Applying the LASSO feature selection method\n",
        "        if select:\n",
        "\n",
        "            if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "            for i, s in enumerate(select):\n",
        "                C = np.asarray(X_train.columns)\n",
        "                M = s.fit(X=X_train.values, y=y_train).get_support()\n",
        "                C_sel = C[M]\n",
        "                C_cat = C_cat[np.isin(C_cat, C_sel)]\n",
        "                C_num = C_num[np.isin(C_num, C_sel)]\n",
        "\n",
        "                X_train_N, X_test_N = X_train[C_num].values, X_test[C_num].values\n",
        "                X_train_C, X_test_C = X_train[C_cat].values, X_test[C_cat].values\n",
        "\n",
        "\n",
        "                X_train = pd.DataFrame(\n",
        "                    np.concatenate((X_train_C, X_train_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "                X_test = pd.DataFrame(\n",
        "                    np.concatenate((X_test_C, X_test_N), axis=1),\n",
        "                    columns=np.concatenate((C_cat, C_num))\n",
        "                )\n",
        "\n",
        "        if oversample:\n",
        "            #If there is any categorical data, apply SMOTE-NC, otherwise just SMOTE\n",
        "            if len(C_cat) > 0:\n",
        "                sampler = SMOTENC(categorical_features=[X_train.columns.get_loc(c) for c in C_cat], random_state=random_state)\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "            X_train, y_train = sampler.fit_resample(X_train, y_train)\n",
        "\n",
        "        estimator = clone(estimator).fit(X_train, y_train)\n",
        "        y_pred = estimator.predict_proba(X_test)[:, 1]\n",
        "        #Deafult average method for roc_auc_score is macro\n",
        "        auc_score = roc_auc_score(y_test, y_pred, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "def perform_cross_validation(X, y, groups, estimator, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        estimator (estimator instance): The model to be trained.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle =True, random_state = 42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X.columns[~X.columns.isin(C_cat)]))\n",
        "\n",
        "        job = train_fold('path_to_results', f'Fold_{idx}', X_train, y_train, X_test, y_test, C_cat, C_num, estimator, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "\n",
        "    return futures"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZOD1KJTup7il"
      },
      "source": [
        "Here, we define the feature selection method and classifier and execute the code. AUC-ROC is calculated as mean of macro AUC-ROC for all folds/users."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nqvkNsznrEDe",
        "outputId": "6d0b3b54-c64d-4d54-838c-e3dd6b6bdcb9"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/xgboost/core.py:158: UserWarning: [13:07:56] WARNING: /workspace/src/common/error_msg.cc:58: Falling back to prediction using DMatrix due to mismatched devices. This might lead to higher memory usage and slower performance. XGBoost is running on: cuda:0, while the input data is on: cpu.\n",
            "Potential solutions:\n",
            "- Use a data structure that matches the device ordinal in the booster.\n",
            "- Set the device for booster before call to inplace_predict.\n",
            "\n",
            "This warning will only be shown once.\n",
            "\n",
            "  warnings.warn(smsg, UserWarning)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Training completed for Fold_0 with AUC: 0.5760597892673365\n",
            "Training completed for Fold_1 with AUC: 0.5194843617920542\n",
            "Training completed for Fold_2 with AUC: 0.605396012438266\n",
            "Training completed for Fold_3 with AUC: 0.5232254989908052\n",
            "Training completed for Fold_4 with AUC: 0.5972818082858423\n",
            "0.5642894941548608\n"
          ]
        }
      ],
      "source": [
        "#Featur Selection, you may want to change the feature selection methods\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005\n",
        ")\n",
        "#Classifier\n",
        "#There could exist more parameters. Please search in your defined parameter\n",
        "#space for model performance improvement\n",
        "estimator = EvXGBClassifier(\n",
        "    random_state=RANDOM_STATE,\n",
        "    eval_metric='logloss',\n",
        "    eval_size=0.2,\n",
        "    early_stopping_rounds=10,\n",
        "    objective='binary:logistic', #Prediction instead of regression\n",
        "    verbosity=0,\n",
        "    learning_rate=0.01,\n",
        ")\n",
        "\n",
        "#Perform cross validation including model training and evaluation\n",
        "results = perform_cross_validation(X, y, groups, estimator, normalize=True, select=[SELECT_LASSO], oversample=True, random_state=42)\n",
        "auc_values = [results[i].metrics['AUC'] for i in range(len(results))]\n",
        "mean_auc = np.mean(auc_values)\n",
        "print(mean_auc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O-EZcuiA56Ls"
      },
      "source": [
        "# Assignment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fuOqHMwFzWHv"
      },
      "source": [
        "## Assignment 1. Improve the model performance using different types of feature combinations. (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6kR-jSPN0i52"
      },
      "source": [
        " Hint: Currently we are only using feat_baseline. You may want to try other feature combinations."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "id": "wD6HOgC_zuiq"
      },
      "outputs": [],
      "source": [
        "#######You may need to go back to the feature preparation code and check#########\n",
        "\n",
        "feat_final = pd.concat([feat_baseline, feat_current_sensor,feat_ImmediatePast_sensor,feat_today_sensor,feat_yesterday_sensor,feat_sleep,feat_pif],axis=1)\n",
        "X = feat_final\n",
        "cats = X.columns[X.dtypes == bool]\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMEhtgwJz9IP"
      },
      "source": [
        "## Assignment 2. Please try different feature selection methods (20pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8FCRh8eF0zud"
      },
      "source": [
        "Hint: Currently, we are using LASSO filter for feature selection. Please consider using embedded method as well(same model for both feature selection and model training). Besides, the threshold for LASSO filter may also affect the performance. **Sepcifically, there is a method called 'mean' which is using mean of feature importances of all features as threshold.** Please try both different feature selection methods and different thresholds for filtering features to improve model performance."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "Bwna8wpv3Dhj"
      },
      "outputs": [],
      "source": [
        "#######You may need to go back to the Model Training & Evaluation part and revise feature selection code########\n",
        "#Featur Selection, you may want to change the feature selection methods or change feature selection threshold\n",
        "SELECT_LASSO = SelectFromModel(\n",
        "    #\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    # This threshold may impact the model performance as well\n",
        "    threshold = 0.005 #Change to other thresholds or trying 'mean'\n",
        ")\n",
        "\n",
        "SELECT_LASSO_MEAN = SelectFromModel(\n",
        "        estimator=LogisticRegression(\n",
        "        penalty='l1'\n",
        "        ,solver='liblinear'\n",
        "        , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "    ),\n",
        "    threshold = 'mean'\n",
        ")\n",
        "\n",
        "from xgboost import XGBClassifier\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "xgb_selector = SelectFromModel(\n",
        "    estimator=XGBClassifier(random_state=RANDOM_STATE, use_label_encoder=False, eval_metric='logloss'),\n",
        "    threshold='mean' # Or a different threshold\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Q_FJZurk3ulY"
      },
      "source": [
        "## Assignment 3. Please try using hyperopt for model hyperparameter tuning (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0gQcs-te34PK"
      },
      "source": [
        "Hint: Please be aware that for revised xgboost classifier EvXGBClassifier, there exist other parameters other than default XGBClassifier parameters such as eval_size.\n",
        "\n",
        "For hyperparameter tuning, we will use 20% of training set as validation set to avoid data leakage.\n",
        "\n",
        "If it is too timeconsuming to run the code in colab, please run the code locally and consider using [ray tune](https://docs.ray.io/en/latest/tune/index.html) if needed."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-8PyEHuv4R-L",
        "outputId": "ee08935d-a307-409f-ba56-901261230535"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "  2%|▏         | 2/100 [02:39<2:11:08, 80.29s/trial, best loss: -0.6406176718563223]"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "from hyperopt import STATUS_OK, Trials, hp, fmin, tpe\n",
        "from sklearn.model_selection import StratifiedGroupKFold, train_test_split\n",
        "from sklearn.metrics import roc_auc_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from imblearn.over_sampling import SMOTE, SMOTENC\n",
        "from sklearn.feature_selection import SelectFromModel\n",
        "\n",
        "# define your outer CV\n",
        "OUTER_CV = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "def objective(params):\n",
        "    val_scores = []\n",
        "\n",
        "    # outer loop: split into train_full / test (we will only use train_full for tuning)\n",
        "    for train_full_idx, _ in OUTER_CV.split(X, y, groups):\n",
        "        X_train_full = X.iloc[train_full_idx]\n",
        "        y_train_full = y[train_full_idx]\n",
        "\n",
        "        # split 20% of the *training fold* into a validation set\n",
        "        X_train, X_val, y_train, y_val = train_test_split(\n",
        "            X_train_full, y_train_full,\n",
        "            test_size=0.20,\n",
        "            stratify=y_train_full,\n",
        "            random_state=42\n",
        "        )\n",
        "\n",
        "        # 1) Normalize\n",
        "        scaler = StandardScaler()\n",
        "        X_train_scaled = scaler.fit_transform(X_train)\n",
        "        X_val_scaled   = scaler.transform(X_val)\n",
        "\n",
        "        # 2) (Optional) Oversample on *training only*\n",
        "        if np.any(X_train_scaled[:, -1] < 1):\n",
        "            smote = SMOTENC(\n",
        "                categorical_features=[X_train_scaled.shape[1]-1],\n",
        "                random_state=int(params['random_state'])\n",
        "            )\n",
        "        else:\n",
        "            smote = SMOTE(random_state=int(params['random_state']))\n",
        "        X_train_os, y_train_os = smote.fit_resample(X_train_scaled, y_train)\n",
        "\n",
        "        # 3) Feature selection on *training only*\n",
        "        selector = SelectFromModel(\n",
        "            LogisticRegression(penalty='l1', solver='liblinear',\n",
        "                               random_state=int(params['random_state'])),\n",
        "            threshold='mean'\n",
        "        )\n",
        "        X_train_sel = selector.fit_transform(X_train_os, y_train_os)\n",
        "        X_val_sel   = selector.transform(X_val_scaled)\n",
        "\n",
        "        # 4) Train & score on *validation only*\n",
        "        clf = LogisticRegression(\n",
        "            random_state=int(params['random_state']),\n",
        "            max_iter=1000\n",
        "        )\n",
        "        clf.fit(X_train_sel, y_train_os)\n",
        "        y_val_prob = clf.predict_proba(X_val_sel)[:, 1]\n",
        "        val_scores.append(roc_auc_score(y_val, y_val_prob))\n",
        "\n",
        "    # Hyperopt minimizes “loss”, so negate AUC\n",
        "    return {'loss': -np.mean(val_scores), 'status': STATUS_OK}\n",
        "\n",
        "\n",
        "# define your search space (fill in any missing parameters e.g. max_depth)\n",
        "space = {\n",
        "    'max_depth':          hp.quniform('max_depth', 3, 10, 1),  # Integer between 3 and 10\n",
        "    'min_child_weight':   hp.quniform('min_child_weight', 1, 10, 1), # Integer between 1 and 10\n",
        "    'subsample':          hp.uniform('subsample', 0.6, 1.0),  # Float between 0.6 and 1.0\n",
        "    'colsample_bytree':   hp.uniform('colsample_bytree', 0.6, 1.0), # Float between 0.6 and 1.0\n",
        "    'gamma':              hp.uniform('gamma', 0, 0.5),      # Float between 0 and 0.5\n",
        "    'learning_rate':      hp.loguniform('learning_rate', -5, 0), # Float on a log scale (0.0067 to 1)\n",
        "    'n_estimators':       hp.quniform('n_estimators', 100, 1000, 50), # Integer between 100 and 1000, steps of 50\n",
        "    'reg_lambda':         hp.uniform('reg_lambda', 0, 1),     # Float between 0 and 1 (L2 regularization)\n",
        "    'reg_alpha':          hp.uniform('reg_alpha', 0, 0.5),    # Float between 0 and 0.5 (L1 regularization)\n",
        "    'random_state':       42 # Keeping random_state fixed\n",
        "}\n",
        "\n",
        "# run hyperopt\n",
        "trials = Trials()\n",
        "best = fmin(\n",
        "    fn=objective,\n",
        "    space=space,\n",
        "    algo=tpe.suggest,\n",
        "    max_evals=100,\n",
        "    trials=trials\n",
        ")\n",
        "\n",
        "print(\"Best hyperparameters:\", best)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EPz966iD6yoH"
      },
      "source": [
        "## Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for **tabular data** to improve model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oIfbskLs7qmc"
      },
      "source": [
        "Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for **tabular data** and see if there is performance improvement."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CbE_mod6TUIn"
      },
      "source": [
        "You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DDEeP0pkBsn6"
      },
      "source": [
        "Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CORRv9Ph4Icj"
      },
      "outputs": [],
      "source": [
        "# prompt: write the code for this: Assignment 4. Please consider replacing the previous traditional machine learning model with deep learning models designed for tabular data to improve model performance. (20 pts)\n",
        "# Hint: Since features are already extracted manually, it is impossible to use end-to-end deep learning models. Instead, try replacing xgboost with deep learning models designed for tabular data and see if there is performance improvement.\n",
        "# You may need to change runtime to TPU first to use torch or other packages you may want to use.\n",
        "# Please compare it with your previous XGBoost model performance and think about why it is higher or lower than XGBoost.\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "!pip install pytorch-tabnet\n",
        "\n",
        "import torch\n",
        "from pytorch_tabnet.tab_model import TabNetClassifier\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Convert pandas DataFrames to numpy arrays for TabNet\n",
        "X_np = X.values\n",
        "y_np = y\n",
        "\n",
        "# Encode categorical features. TabNet handles categorical features automatically if specified.\n",
        "# Identify categorical feature indices\n",
        "categorical_feature_indices = [i for i, col in enumerate(X.columns) if col in cats]\n",
        "# Since we already One-Hot Encoded the boolean columns earlier, let's treat them as numerical for now\n",
        "# if you decide to use the boolean columns as categorical, you'll need to handle that differently\n",
        "# For now, assuming all features in X are treated as numerical inputs for TabNet after potential scaling.\n",
        "\n",
        "\n",
        "class TabularDataset(Dataset):\n",
        "    def __init__(self, X, y):\n",
        "        self.X = torch.tensor(X, dtype=torch.float32)\n",
        "        self.y = torch.tensor(y, dtype=torch.long)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.X)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return self.X[idx], self.y[idx]\n",
        "\n",
        "def train_tabnet_fold(X_train, y_train, X_test, y_test, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to train and evaluate TabNet model for a single fold.\n",
        "    Args:\n",
        "        X_train, y_train (DataFrame, Series): Training data.\n",
        "        X_test, y_test (DataFrame, Series): Testing data.\n",
        "        normalize (bool): Flag to apply normalization.\n",
        "        select (SelectFromModel instance): Feature selection method.\n",
        "        oversample (bool): Flag to apply oversampling.\n",
        "        random_state (int): Random state for reproducibility.\n",
        "    Returns:\n",
        "        FoldResult: Object containing metrics and duration of the training.\n",
        "    \"\"\"\n",
        "    try:\n",
        "        start_time = time.time()\n",
        "\n",
        "        X_train_processed = X_train.copy()\n",
        "        X_test_processed = X_test.copy()\n",
        "        y_train_processed = y_train.copy()\n",
        "\n",
        "        # Separate categorical and numerical columns for processing\n",
        "        C_cat = np.asarray(sorted(cats))\n",
        "        C_num = np.asarray(sorted(X_train.columns[~X_train.columns.isin(C_cat)]))\n",
        "\n",
        "        # Normalize numerical features\n",
        "        if normalize:\n",
        "            scaler = StandardScaler().fit(X_train_processed[C_num])\n",
        "            X_train_processed[C_num] = scaler.transform(X_train_processed[C_num])\n",
        "            X_test_processed[C_num] = scaler.transform(X_test_processed[C_num])\n",
        "\n",
        "        # Apply feature selection\n",
        "        if select:\n",
        "             if isinstance(select, SelectFromModel):\n",
        "                select = [select]\n",
        "\n",
        "             for i, s in enumerate(select):\n",
        "                # Fit selector on training data\n",
        "                s.fit(X_train_processed, y_train_processed)\n",
        "                # Transform both train and test data\n",
        "                X_train_processed = pd.DataFrame(s.transform(X_train_processed), columns=X_train_processed.columns[s.get_support()])\n",
        "                X_test_processed = pd.DataFrame(s.transform(X_test_processed), columns=X_test_processed.columns[s.get_support()])\n",
        "\n",
        "             # Update categorical and numerical column lists based on selected features\n",
        "             C_cat = np.asarray([col for col in C_cat if col in X_train_processed.columns])\n",
        "             C_num = np.asarray([col for col in C_num if col in X_train_processed.columns])\n",
        "\n",
        "\n",
        "        # Apply oversampling (on training data only)\n",
        "        if oversample:\n",
        "            # If there is any categorical data, apply SMOTE-NC, otherwise just SMOTE\n",
        "            if len(C_cat) > 0:\n",
        "                 # Need to determine categorical feature indices in the *processed* dataframe\n",
        "                 processed_cat_indices = [X_train_processed.columns.get_loc(c) for c in C_cat]\n",
        "                 sampler = SMOTENC(categorical_features=processed_cat_indices, random_state=random_state)\n",
        "            else:\n",
        "                sampler = SMOTE(random_state=random_state)\n",
        "\n",
        "            X_train_processed, y_train_processed = sampler.fit_resample(X_train_processed, y_train_processed)\n",
        "\n",
        "\n",
        "        # Convert to numpy arrays before passing to TabNet\n",
        "        X_train_np = X_train_processed.values\n",
        "        y_train_np = y_train_processed.values\n",
        "        X_test_np = X_test_processed.values\n",
        "        y_test_np = y_test\n",
        "\n",
        "        # Initialize TabNet model\n",
        "        # Adjust parameters based on your data and hyperparameter tuning\n",
        "        clf = TabNetClassifier(\n",
        "            optimizer_fn=torch.optim.Adam,\n",
        "            optimizer_params=dict(lr=2e-2),\n",
        "            scheduler_params={\"step_size\":50, # how many steps before decaying the learning rate\n",
        "                              \"gamma\":0.9},\n",
        "            scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
        "            mask_type='entmax', # \"sparsemax\", \"entmax\"\n",
        "            seed=random_state,\n",
        "            verbose=0 # Set to 1 for detailed training logs per epoch\n",
        "        )\n",
        "\n",
        "        # Train the model\n",
        "        # Use evaluation set for early stopping (TabNet has its own early stopping mechanism)\n",
        "        # Split processed training data for early stopping\n",
        "        X_train_sub_np, X_val_np, y_train_sub_np, y_val_np = train_test_split(\n",
        "            X_train_np, y_train_np, test_size=0.2, random_state=random_state, stratify=y_train_np\n",
        "        )\n",
        "\n",
        "        clf.fit(\n",
        "            X_train=X_train_sub_np, y_train=y_train_sub_np,\n",
        "            eval_set=[(X_val_np, y_val_np)],\n",
        "            eval_name=['valid'],\n",
        "            max_epochs=100, # You might need to tune this\n",
        "            patience=10, # You might need to tune this\n",
        "            batch_size=1024, # You might need to tune this\n",
        "            virtual_batch_size=128, # You might need to tune this\n",
        "            num_workers=0,\n",
        "            weights=1, # Set to 1 for automatic class weighting\n",
        "            drop_last=False,\n",
        "        )\n",
        "\n",
        "\n",
        "        # Predict probabilities on the test set\n",
        "        y_pred_proba = clf.predict_proba(X_test_np)[:, 1]\n",
        "\n",
        "        # Calculate AUC\n",
        "        auc_score = roc_auc_score(y_test_np, y_pred_proba, average=None)\n",
        "\n",
        "        result = FoldResult(\n",
        "            name=fold_name,\n",
        "            metrics={'AUC': auc_score},\n",
        "            duration=time.time() - start_time\n",
        "        )\n",
        "        log(f'Training completed for {fold_name} with AUC: {auc_score}')\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        log(f'Error in {fold_name}: {traceback.format_exc()}')\n",
        "        return None\n",
        "\n",
        "\n",
        "def perform_tabnet_cross_validation(X, y, groups, normalize=False, select=None, oversample=False, random_state=None):\n",
        "    \"\"\"\n",
        "    Function to perform cross-validation using StratifiedGroupKFold with TabNet.\n",
        "    Args:\n",
        "        X, y (DataFrame, Series): The entire dataset.\n",
        "        groups (array): Array indicating the group for each instance in X.\n",
        "        normalize, select, oversample (bool): Preprocessing options.\n",
        "        random_state (int): Seed for reproducibility.\n",
        "    Returns:\n",
        "        list: A list containing FoldResult for each fold.\n",
        "    \"\"\"\n",
        "    futures = []\n",
        "    # Group-k cross validation\n",
        "    splitter = StratifiedGroupKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "    # Loop over all the LOSO splits\n",
        "    for idx, (train_idx, test_idx) in enumerate(splitter.split(X, y, groups)):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y[train_idx], y[test_idx]\n",
        "\n",
        "        job = train_tabnet_fold(X_train, y_train, X_test, y_test, normalize, select, oversample, random_state)\n",
        "        futures.append(job)\n",
        "\n",
        "    return futures\n",
        "\n",
        "# Define feature selection method if needed for TabNet (Optional - TabNet can handle raw features)\n",
        "# You can still use LASSO or XGBoost based selection before passing to TabNet if you want to reduce dimensionality.\n",
        "# For this example, let's try without explicit feature selection first to see TabNet's inherent capabilities.\n",
        "# If you want to use feature selection, uncomment and configure SELECT_LASSO or xgb_selector here.\n",
        "\n",
        "# SELECT_LASSO = SelectFromModel(\n",
        "#         estimator=LogisticRegression(\n",
        "#         penalty='l1'\n",
        "#         ,solver='liblinear'\n",
        "#         , C=1, random_state=RANDOM_STATE, max_iter=4000\n",
        "#     ),\n",
        "#     threshold = 'mean'\n",
        "# )\n",
        "\n",
        "# Perform cross validation with TabNet\n",
        "# Set select=None to use all features after normalization and oversampling\n",
        "results_tabnet = perform_tabnet_cross_validation(X, y, groups, normalize=True, select=None, oversample=True, random_state=42)\n",
        "\n",
        "# Calculate and print the mean AUC for TabNet\n",
        "auc_values_tabnet = [result.metrics['AUC'] for result in results_tabnet if result is not None]\n",
        "if auc_values_tabnet:\n",
        "    mean_auc_tabnet = np.mean(auc_values_tabnet)\n",
        "    print(f\"Mean AUC for TabNet: {mean_auc_tabnet}\")\n",
        "else:\n",
        "    print(\"TabNet training failed for all folds.\")\n",
        "\n",
        "# Compare with previous XGBoost result\n",
        "# You would have run the XGBoost code previously to get `mean_auc`\n",
        "print(f\"Mean AUC for XGBoost (previous run): {mean_auc}\")\n",
        "\n",
        "# Reflection on why performance is higher or lower\n",
        "# TabNet is a deep learning model designed specifically for tabular data. It uses a sequential attention mechanism\n",
        "# to select relevant features at each decision step, which can capture complex interactions between features.\n",
        "# XGBoost is a powerful gradient boosting model that is also very effective on tabular data, especially\n",
        "# with well-engineered features.\n",
        "\n",
        "# Possible reasons for performance difference:\n",
        "# 1. Complex Feature Interactions: TabNet might be better at capturing non-linear relationships and interactions\n",
        "#    between features than XGBoost, especially if these interactions are not explicitly engineered.\n",
        "# 2. Handling of Categorical Features: TabNet has built-in mechanisms for handling categorical features,\n",
        "#    which might be more effective than one-hot encoding or other methods used with traditional models.\n",
        "#    (Note: In this specific code, we assumed the boolean features were handled as numerical, so this point\n",
        "#    might be less relevant unless you modify the code to treat them as categorical).\n",
        "# 3. Hyperparameter Tuning: The performance of both models is highly dependent on hyperparameters.\n",
        "#    The provided TabNet hyperparameters are defaults or basic settings; further tuning for TabNet\n",
        "#    (e.g., network architecture, learning rate scheduling, batch sizes, mask type) might lead to significant\n",
        "#    improvement. The hyperopt code provided was for Logistic Regression, not XGBoost or TabNet. You would\n",
        "#    need to adapt hyperparameter tuning for TabNet.\n",
        "# 4. Data Size: Deep learning models often require more data than traditional models to perform well.\n",
        "#    If the dataset is relatively small, XGBoost might have an advantage.\n",
        "# 5. Regularization: TabNet has various regularization mechanisms (e.g., in the attention masks) that can help\n",
        "#    prevent overfitting, which might be beneficial.\n",
        "# 6. Feature Scaling: TabNet, like many neural networks, is sensitive to feature scaling. The `StandardScaler`\n",
        "#    applied to numerical features is important.\n",
        "# 7. Stochasticity: Deep learning models like TabNet have more stochasticity in training (due to initialization,\n",
        "#    batching, etc.) compared to tree-based models. Running multiple times or with different seeds might\n",
        "#    give slightly different results.\n",
        "\n",
        "# To get a more definitive comparison, you would ideally:\n",
        "# - Perform comprehensive hyperparameter tuning for BOTH XGBoost and TabNet.\n",
        "# - Ensure fair comparison of preprocessing steps (normalization, feature selection, oversampling) applied to both models.\n",
        "# - Evaluate on the same cross-validation folds.\n",
        "\n",
        "# In summary, if TabNet performs better, it could be due to its ability to learn complex feature representations and interactions. If XGBoost performs better, it might indicate that the engineered features are already capturing most of the necessary information, or that the dataset size or the specific characteristics of the data favor gradient boosting.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MZk2A2-I7nPd"
      },
      "outputs": [],
      "source": [
        "#######Your code for deep learning model#########"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aZdspTz_G2D2"
      },
      "source": [
        "## Assignment 5. Please try combining all the above methods to push the model performance. (20 pts)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ftBi462fHGQn"
      },
      "source": [
        "Hint: Methods other than the above methods are also okay to use to improve model performance.\n",
        "\n",
        "Please avoid data leakage when conducting hyperparameter tuning.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "T9SI2OpwG8eE"
      },
      "outputs": [],
      "source": [
        "#######Your code for combing all above mentioned methods to push model performance########"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "intro-machine-learning",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
